{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYtkoluuDJRSdfprFlujVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Interference_models_using_external_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference using pretrained models**"
      ],
      "metadata": {
        "id": "7bnfu3qWA2lX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF6RLIRqARlg",
        "outputId": "06297132-03fc-4011-88b5-9e472caee5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.13.1+cu116 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#GDriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/for_validation\"\n",
        "\n",
        "mobileNet_model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/MobileNetV3_aug2.pth\"\n",
        "YOLOv5_model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "laxJknZhCVBG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the image paths\n",
        "\n",
        "# 番号順に並べる\n",
        "# https://dlrecord.hatenablog.com/entry/2020/07/30/230234\n",
        "\n",
        "\n",
        "import re\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "def natural_keys(text):\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
        "\n",
        "grav_list = sorted([i for i in glob.glob(f\"{dataset_folder_path}/treated_640px/*\")], key=natural_keys)\n",
        "cont_list = sorted([i for i in glob.glob(f\"{dataset_folder_path}/control_640px/*\")], key=natural_keys)\n",
        "image_list = grav_list + cont_list\n",
        "label_list = [1]*len(grav_list) + [0]*len(cont_list)\n",
        "\n",
        "image_list\n"
      ],
      "metadata": {
        "id": "y4-J8tuNA8RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show sample images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "root = f\"{dataset_folder_path}/treated_640px\" #画像があるフォルダ。適宜変えてください\n",
        "lsdir = os.listdir(root)\n",
        "\n",
        "imgs = []\n",
        "for l in lsdir:\n",
        "    target = os.path.join(root,l)\n",
        "    img = cv2.imread(target)\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) #pyplotで表示するために色変換\n",
        "    imgs.append(img)\n",
        "\n",
        "shownumber = 6 #画像を並べる数\n",
        "showaxis = 1\n",
        "\n",
        "while(showaxis*showaxis < shownumber):\n",
        "    showaxis += 1\n",
        "\n",
        "cnt = 0\n",
        "while(1):\n",
        "    limit = 6\n",
        "    if cnt >= limit:\n",
        "       break\n",
        "    fig,axs = plt.subplots(showaxis,showaxis, figsize=(16.0, 12.0))\n",
        "    ar = axs.ravel()\n",
        "    for i in range(showaxis*showaxis):\n",
        "        ar[i].axis('off')\n",
        "        if i < shownumber:\n",
        "            ar[i].imshow(imgs[cnt])\n",
        "            cnt += 1\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iACUWQq356Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**MobileNetV3 interference**"
      ],
      "metadata": {
        "id": "t4OiMurgHhRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model \n",
        "##########################\n",
        "!pip install --quiet timm\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_ft.load_state_dict(torch.load(mobileNet_model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0s6sfExHeeZ",
        "outputId": "437c5b2a-2f03-43d0-fd7b-1dfb467d260c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\" to /root/.cache/torch/hub/checkpoints/mobilenetv3_large_100_ra-f55367f5.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "from PIL import *\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "def image_loader(image_path):\n",
        "    \"\"\"load image, returns cuda tensor\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    image = img_transforms(image).float()\n",
        "    image = image.unsqueeze(0) \n",
        "    return image.to(device)\n",
        "\n",
        "def my_round(val, digit=0):\n",
        "    p = 10 ** digit\n",
        "    return (val * p * 2 + 1) // 2 / p\n",
        "\n",
        "def interference(image_list):\n",
        "    image_tensor = image_loader(path)\n",
        "\n",
        "    model_ft.to(device)\n",
        "    model_ft.eval()\n",
        "    output = model_ft(image_tensor)\n",
        "    _, pred = torch.max(output, 1) \n",
        "    pred = pred[0].to('cpu').detach().numpy().copy().tolist() \n",
        "\n",
        "    prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "    prob = prob[0][1].cpu().detach().numpy().copy().tolist() #probalility of being positive\n",
        "    prob = my_round(prob, 3)\n",
        "\n",
        "    return pred, prob, image_tensor"
      ],
      "metadata": {
        "id": "vYhubvrbRid7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple image eval\n",
        "path_list, pred_list, prob_list = [], [], []\n",
        "for idx, path in enumerate(image_list, 1):\n",
        "    pred, prob, _ = interference(path)\n",
        "    path_list.append(os.path.basename(path)) \n",
        "    pred_list.append(pred)\n",
        "    prob_list.append(prob)\n",
        "print(label_list)\n",
        "print(pred_list)\n",
        "print(prob_list)\n",
        "\n",
        "df = pd.DataFrame(index=[], columns=[])\n",
        "df[\"path\"] = path_list\n",
        "df[\"label\"] = label_list\n",
        "df[\"pred_MobileNet\"] = pred_list\n",
        "df[\"prob_MobileNet\"] = prob_list\n",
        "df.to_csv('/content/interference.csv', header=True, index=False, encoding = \"utf-8\")\n",
        "\n",
        "pd.set_option('display.max_rows', 400)\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "kmELszx_XJlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "df"
      ],
      "metadata": {
        "id": "9MpED4MQXKV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single image eval\n",
        "num = 5\n",
        "path = image_list[num]\n",
        "label = label_list[num]\n",
        "print(f\"path: {path}\")\n",
        "\n",
        "pred, prob, tensor = interference(path) \n",
        "print(f\"label: {label}, pred: {pred}, prob: {prob}\")\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "i4ChqsnIXgfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**YOLOv5n interference**"
      ],
      "metadata": {
        "id": "CupAqAPuTS5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNvH_R8sTbKg",
        "outputId": "7aca2c7b-c5bf-4206-e010-1313b36a8f80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 26.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inference (folder内全部)\n",
        "# !python detect.py --weights $YOLOv5_model_path --img 640 --conf 0.25 --source {dataset_folder_path}/treated_640px\n"
      ],
      "metadata": {
        "id": "bniMvUF3TbMR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "Aooz-_mLaLR3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_path = image_list\n",
        "# img = image_path[102]\n",
        "\n",
        "parent_dir = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/for_validation/treated_640px\"\n",
        "img = f\"{parent_dir}/970.JPG\"\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, YOLOv5_model_path)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "\n",
        "# 横幅が640pxになるようにリサイズ\n",
        "height, width, _ = img_cv2.shape\n",
        "resize_width = 640\n",
        "resize_height = int((height / width) * resize_width)\n",
        "resize_size = (resize_width, resize_height)\n",
        "img_cv2 = cv2.resize(img_cv2, resize_size)\n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "9twS6nUkTbOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list, prob_list = [], []\n",
        "\n",
        "for img in image_list:\n",
        "    pred = interference(img, YOLOv5_model_path)\n",
        "    # output result\n",
        "    # x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # class\n",
        "    class_num = int(pred[0][0][5].item())\n",
        "    pred_list.append(class_num)\n",
        "\n",
        "    # prob\n",
        "    prob = pred[0][0][4].item()\n",
        "    if class_num == 0: #cont\n",
        "        prob = 1-prob\n",
        "    elif class_num == 1: #grav\n",
        "        prob = prob\n",
        "    prob_list.append(prob)\n",
        "\n",
        "df[\"pred_YOLOv5\"] = pred_list\n",
        "df[\"prob_YOLOv5\"] = prob_list\n",
        "\n",
        "df.to_csv('/content/interference.csv', header=True, index=False, encoding = \"utf-8\")\n",
        "\n",
        "pd.set_option('display.max_rows', 400)\n",
        "df"
      ],
      "metadata": {
        "id": "dEq6-EfVjDXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ROC curve**"
      ],
      "metadata": {
        "id": "zBb8ButdWWE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "label = df[\"label\"]\n",
        "# prob = df[\"prob_MobileNet\"]\n",
        "# pred = df[\"pred_MobileNet\"]\n",
        "prob = df[\"prob_YOLOv5\"]\n",
        "pred = df[\"pred_YOLOv5\"]\n",
        "\n",
        "cm = confusion_matrix(label, pred)\n",
        "accuracy = accuracy_score(label, pred)\n",
        "sensitivity = recall_score(label, pred)\n",
        "specificity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
        "ppv = precision_score(label, pred)\n",
        "f1 = f1_score(label, pred)\n",
        "\n",
        "print('Confusion Matrix:\\n', cm) # 混同行列\n",
        "print('Accuracy:', accuracy) # 正解率\n",
        "print('Sensitivity:', sensitivity) # 感度\n",
        "print('Specificity:', specificity) # 特異度\n",
        "print('Positive Predictive Value:', ppv) # 陽性的中率\n",
        "print('F1-Score:', f1) # F1スコア\n",
        "\n",
        "\n",
        "\n",
        "# 真陽性率（TPR）、偽陽性率（FPR）、しきい値（thresholds）を計算する\n",
        "fpr, tpr, thresholds = roc_curve(label, prob)\n",
        "\n",
        "# AUCスコアを計算する\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# ROC曲線をプロットする\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5P0_yZzxWZH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "deYJXnANWZJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Output CoreML**"
      ],
      "metadata": {
        "id": "kGUYCKO6pe-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML_example\n",
        "###########################\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "import numpy as np\n",
        "!pip install --quiet timm\n",
        "\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "\n",
        "base_model = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            base_model,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "import urllib\n",
        "label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = class_labels[1:] # remove the first class which is background\n",
        "assert len(class_labels) == 1000\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "image_input = ct.ImageType(name=\"input_1\",\n",
        "                           shape=example_input.shape,\n",
        "                           scale=scale,\n",
        "                           bias=[red_bias, green_bias, blue_bias])\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[image_input], \n",
        "    classifier_config = ct.ClassifierConfig(class_labels), \n",
        "    compute_units=ct.ComputeUnit.CPU_ONLY,\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(\"MobileNetV3_pytorch.mlmodel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN85pcwkeznp",
        "outputId": "0c982481-601f-41b3-9217-bcf08c6ad63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 317 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 327 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 337 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 378 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 409 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 419 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 430 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 440 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 450 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 460 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 481 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 491 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 501 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 512 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 522 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 532 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 542 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 552 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 573 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 583 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 593 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 604 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 614 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 624 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 634 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 645 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 655 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 665 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 675 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 686 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 696 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 716 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 727 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 737 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 747 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 757 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 768 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 778 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 788 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 808 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 819 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 829 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 839 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 849 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 860 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 870 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 880 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 890 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 901 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 911 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 921 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 931 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 942 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 952 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 962 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 972 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 983 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 993 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 6.8 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TensorFlow version 2.9.2 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.8.0 is the most recent version that has been tested.\n",
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 1244.78 ops/s]\n",
            "Running MIL Common passes:   0%|          | 0/38 [00:00<?, ? passes/s]/usr/local/lib/python3.7/dist-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '879', of the source model, has been renamed to 'var_879' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL Common passes: 100%|██████████| 38/38 [00:00<00:00, 59.40 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 84.66 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1466.69 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzxTtZprVcAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML 飛ばして下さい\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "image_input = ct.ImageType(name=\"input_1\",\n",
        "                           shape=example_input.shape,\n",
        "                           scale=scale,\n",
        "                           bias=[red_bias, green_bias, blue_bias])\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[image_input], \n",
        "    classifier_config = ct.ClassifierConfig(class_labels), \n",
        "    compute_units=ct.ComputeUnit.CPU_ONLY,\n",
        ")\n",
        "\n",
        "\n",
        "# # Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(\"/content/gravcont_mobilenetv3.mlmodel\")\n"
      ],
      "metadata": {
        "id": "nggE6m1ah8G0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1da4890-761a-48ae-b83f-df7c1e9e8727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2482.53 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 38/38 [00:00<00:00, 60.42 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 83.79 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1744.69 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "### Output as CoreML (Tensor type) ###\n",
        "################################\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.TensorType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "    classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(\"/content/gravcont_mobilenetv3.mlmodel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndvCOs4aGQ18",
        "outputId": "a4ef679d-adf7-4799-9993-ecf8bfff7f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2451.94 ops/s]\n",
            "Running MIL Common passes:   0%|          | 0/38 [00:00<?, ? passes/s]/usr/local/lib/python3.7/dist-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '879', of the source model, has been renamed to 'var_879' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL Common passes: 100%|██████████| 38/38 [00:00<00:00, 60.45 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 85.48 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1717.72 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference on CoreML model**"
      ],
      "metadata": {
        "id": "a8hqka6ery11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This script can be used in Mac only\n",
        "\n",
        "##https://gist.github.com/ozgurshn/85cf74558d82c831827e12f015f752a1\n",
        "##https://github.com/apple/coremltools/blob/master/examples/APIExamples.md\n",
        "import coremltools\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "# load a model whose input type is \"Image\"\n",
        "model = coremltools.models.MLModel('/content/gravcont_mobilenetv3.mlmodel')\n",
        "\n",
        "Height = 224  # use the correct input image height\n",
        "Width = 224  # use the correct input image width\n",
        "\n",
        "\n",
        "# Scenario 1: load an image from disk\n",
        "def load_image(path, resize_to=None):\n",
        "    # resize_to: (Width, Height)\n",
        "    img = PIL.Image.open(path)\n",
        "    if resize_to is not None:\n",
        "        img = img.resize(resize_to, PIL.Image.ANTIALIAS)\n",
        "    img_np = np.array(img).astype(np.float32)\n",
        "    return img_np, img\n",
        "\n",
        "\n",
        "# load the image and resize using PIL utilities\n",
        "_, img = load_image('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_newPatient_250px/スライド1.jpeg', resize_to=(Width, Height))\n",
        "out_dict = model.predict({'image': img})\n",
        "\n",
        "# Scenario 2: load an image from a numpy array\n",
        "shape = (Height, Width, 3)  # height x width x RGB\n",
        "data = np.zeros(shape, dtype=np.uint8)\n",
        "# manipulate numpy data\n",
        "pil_img = PIL.Image.fromarray(data)\n",
        "out_dict = model.predict({'image': pil_img})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "mlesVk5aQln-",
        "outputId": "2fe8cb94-f9c7-457d-e587-080f18fac2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c0ccb8862f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# load the image and resize using PIL utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_newPatient_250px/スライド1.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mout_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Scenario 2: load an image from a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/coremltools/models/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_macos_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 raise Exception(\n\u001b[0;32m--> 524\u001b[0;31m                     \u001b[0;34m\"Model prediction is only supported on macOS version 10.13 or later.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                 )\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Model prediction is only supported on macOS version 10.13 or later."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Interference without Mac**"
      ],
      "metadata": {
        "id": "UMIWSpNBr8A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://tvm.apache.org/docs/how_to/compile_models/from_coreml.html\n",
        "!pip install --quiet apache-tvm\n",
        "!pip install --quiet coremltools\n",
        "import tvm\n",
        "from tvm import te\n",
        "import tvm.relay as relay\n",
        "from tvm.contrib.download import download_testdata\n",
        "import coremltools as cm\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK8mOh84o3Ih",
        "outputId": "aee9507e-41fc-4c37-828d-96b2ee08006c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 42.8 MB 1.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load the model**"
      ],
      "metadata": {
        "id": "5S-9HAe7qjC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://docs-assets.developer.apple.com/coreml/models/MobileNet.mlmodel\"\n",
        "model_file = \"mobilenet.mlmodel\"\n",
        "model_path = download_testdata(model_url, model_file, module=\"coreml\")\n",
        "# Now you have mobilenet.mlmodel on disk\n",
        "mlmodel = cm.models.MLModel(model_path)"
      ],
      "metadata": {
        "id": "1k6jpavuqFHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load the test image**"
      ],
      "metadata": {
        "id": "hEI_hbRbqJdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\n",
        "img_path = download_testdata(img_url, \"cat.png\", module=\"data\")\n",
        "img = Image.open(img_path).resize((224, 224))\n",
        "# Mobilenet.mlmodel's input is BGR format\n",
        "img_bgr = np.array(img)[:, :, ::-1]\n",
        "x = np.transpose(img_bgr, (2, 0, 1))[np.newaxis, :]"
      ],
      "metadata": {
        "id": "kCriDoPpqfxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Compile model on relay**"
      ],
      "metadata": {
        "id": "_uTYH_-prJ5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"llvm\"\n",
        "shape_dict = {\"image\": x.shape}\n",
        "\n",
        "# Parse CoreML model and convert into Relay computation graph\n",
        "mod, params = relay.frontend.from_coreml(mlmodel, shape_dict)\n",
        "\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target, params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD584_4lqPGX",
        "outputId": "385a912a-3d2b-44c8-85e7-692141e2d62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tvm/driver/build_module.py:268: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
            "  \"target_host parameter is going to be deprecated. \"\n",
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute on TVM (これはサンプルの通り)\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "dev = tvm.cpu(0)\n",
        "dtype = \"float32\"\n",
        "m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "# set inputs\n",
        "m.set_input(\"image\", tvm.nd.array(x.astype(dtype)))\n",
        "# execute\n",
        "m.run()\n",
        "# get outputs\n",
        "tvm_output = m.get_output(0)\n",
        "top1 = np.argmax(tvm_output.numpy()[0])"
      ],
      "metadata": {
        "id": "eEVnsfV8rU5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Look up system name**"
      ],
      "metadata": {
        "id": "10OvE25urm0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synset_url = \"\".join(\n",
        "    [\n",
        "        \"https://gist.githubusercontent.com/zhreshold/\",\n",
        "        \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n",
        "        \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n",
        "        \"imagenet1000_clsid_to_human.txt\",\n",
        "    ]\n",
        ")\n",
        "synset_name = \"imagenet1000_clsid_to_human.txt\"\n",
        "synset_path = download_testdata(synset_url, synset_name, module=\"data\")\n",
        "with open(synset_path) as f:\n",
        "    synset = eval(f.read())\n",
        "# You should see the following result: Top-1 id 282 class name tiger cat\n",
        "print(\"Top-1 id\", top1, \"class name\", synset[top1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNZsGci-rhPY",
        "outputId": "e1ac8a1c-d2bd-497c-a63b-c5d7e2cac521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 id 282 class name tiger cat\n"
          ]
        }
      ]
    }
  ]
}