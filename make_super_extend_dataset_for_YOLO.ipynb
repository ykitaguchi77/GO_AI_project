{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJnVUOfua8E9HOXZhTFzBG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/make_super_extend_dataset_for_YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create extended dataset for YOLOv5 implementation**\n",
        "\n",
        "#Olympia\n",
        "・Olympia dataset\n",
        "\n",
        "Treated 640px: 561枚。テストに50枚を残す（test_olympia）、残りの511枚をtrain/valに回す\n",
        "\n",
        "Control 640px: 169枚。テストに50枚を残す(test_control)、残りの119枚をtrain/valに回す\n",
        "\n",
        "#Handai\n",
        "・GO_extended_dataset\n",
        "\n",
        "control: 1886枚。テストに50枚を残す(test_handai)、残りの1836枚をtrain/valに回す\n",
        "\n",
        "treatable: 1879枚。テストに50枚を残す(test_handai)、残りの1829枚をtrain/valに回す\n",
        "\n",
        "\n",
        "#合計：\n",
        "treatable: 1829 + 119\n",
        "\n",
        "control: 1836 + 511\n",
        "\n",
        "testset: Olympia treated 50, control 50, Handai treated 50, control 50\n",
        "\n",
        "##既存のYOLOでbounding boxを作成 --> classは既知のもので。Glaucomaのrepositoryを参考にしてYOLO datasetを作成。"
      ],
      "metadata": {
        "id": "BW6BkO088P6u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ds7rvBFs8LcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d4d277-ea95-4495-aa2f-2cbdc7b812b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Google Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイル数確認用\n",
        "import os\n",
        "\n",
        "def countfile(name, path):\n",
        "    print(f\"path: {path}\")\n",
        "    print(f\"number: {len(os.listdir(path))}\")\n",
        "    print(\"\")\n",
        "\n",
        "if not os.path.exists(\"/content/GO_extended_dataset\"):\n",
        "    !unzip /content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip -d /content/\n",
        "\n",
        "\n",
        "olympia_cont_dir = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/control_640px\"\n",
        "olympia_grav_dir = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "handai_cont_dir = \"/content/GO_extended_dataset/Control_photo_1886mai\"\n",
        "handai_grav_dir = \"/content/GO_extended_dataset/treatable\"\n",
        "\n",
        "\n",
        "countfile(\"control_640px_olympia\", olympia_cont_dir)\n",
        "countfile(\"treated_640px_olympia\", olympia_grav_dir)\n",
        "\n",
        "countfile(\"control_handai\", handai_cont_dir)\n",
        "countfile(\"treated_handai\", handai_grav_dir)\n"
      ],
      "metadata": {
        "id": "JFEWXrZrz_CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Handai dataset, Olympia dataset --> super_extend_datase_for_YOLO**"
      ],
      "metadata": {
        "id": "6pYCYtrN79vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dst_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO\"\n",
        "\n",
        "if os.path.exists(dst_dir):\n",
        "    shutil.rmtree(dst_dir)\n",
        "os.makedirs(dst_dir)\n",
        "os.makedirs(f\"{dst_dir}/train_val\")\n",
        "os.makedirs(f\"{dst_dir}/train_val/grav\")\n",
        "os.makedirs(f\"{dst_dir}/train_val/cont\")\n",
        "os.makedirs(f\"{dst_dir}/test\")\n",
        "os.makedirs(f\"{dst_dir}/test/handai/grav\")\n",
        "os.makedirs(f\"{dst_dir}/test/handai/cont\")\n",
        "os.makedirs(f\"{dst_dir}/test/olympia/grav\")\n",
        "os.makedirs(f\"{dst_dir}/test/olympia/cont\")"
      ],
      "metadata": {
        "id": "FlAqRIZbCDRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "random_state = 1\n",
        "\n",
        "# Olympia Control images\n",
        "src_cont = olympia_cont_dir\n",
        "test_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "trainval_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/cont\"\n",
        "\n",
        "cont_images = os.listdir(src_cont)\n",
        "random.Random(random_state).shuffle(cont_images)\n",
        "\n",
        "test_cont_images = cont_images[:50]\n",
        "trainval_cont_images = cont_images[50:]\n",
        "\n",
        "for image in tqdm(test_cont_images, desc=\"Copying test cont images\"):\n",
        "    src_path = os.path.join(src_cont, image)\n",
        "    dst_path = os.path.join(test_cont, image)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "for image in tqdm(trainval_cont_images, desc=\"Copying trainval cont images\"):\n",
        "    src_path = os.path.join(src_cont, image)\n",
        "    dst_path = os.path.join(trainval_cont, image)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "\n",
        "# Olympia Grav images\n",
        "src_grav = olympia_grav_dir\n",
        "test_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "trainval_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/grav\"\n",
        "\n",
        "grav_images = os.listdir(src_grav)\n",
        "random.Random(random_state).shuffle(grav_images)\n",
        "\n",
        "test_grav_images = grav_images[:50]\n",
        "trainval_grav_images = grav_images[50:]\n",
        "\n",
        "for image in tqdm(test_grav_images, desc=\"Copying test grav images\"):\n",
        "    src_path = os.path.join(src_grav, image)\n",
        "    dst_path = os.path.join(test_grav, image)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "for image in tqdm(trainval_grav_images, desc=\"Copying trainval grav images\"):\n",
        "    src_path = os.path.join(src_grav, image)\n",
        "    dst_path = os.path.join(trainval_grav, image)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sta5Z-LLC01f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "grav_dir = olympia_grav_dir\n",
        "cont_dir = olympia_cont_dir\n",
        "test_grav_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "test_cont_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "train_val_grav_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/grav\"\n",
        "train_val_cont_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/cont\"\n",
        "\n",
        "# Check if files in grav_dir are in test_grav_dir\n",
        "for filename in tqdm(os.listdir(grav_dir)):\n",
        "    basename = os.path.basename(filename)\n",
        "    if os.path.exists(os.path.join(test_grav_dir, basename)):\n",
        "        print(\"coooool!!!\")\n",
        "        continue\n",
        "    else:\n",
        "        copyfile(os.path.join(grav_dir, filename), os.path.join(train_val_grav_dir, filename))\n",
        "\n",
        "# Check if files in cont_dir are in test_cont_dir\n",
        "for filename in tqdm(os.listdir(cont_dir)):\n",
        "    basename = os.path.basename(filename)\n",
        "    if os.path.exists(os.path.join(test_cont_dir, basename)):\n",
        "        print(\"hotttttt!!!\")\n",
        "        continue\n",
        "    else:\n",
        "        copyfile(os.path.join(cont_dir, filename), os.path.join(train_val_cont_dir, filename))\n"
      ],
      "metadata": {
        "id": "-Ll-LhlRKSQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# rename\n",
        "#指定されたフォルダ内にあるファイル名が\"olympia_\"から始まらない場合に、\"olympia_\"を先頭につけたファイル名にリネームする処理を行う。\n",
        "#####################\n",
        "\n",
        "import os\n",
        "\n",
        "# フォルダのパスを指定\n",
        "test_grav_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "test_cont_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "train_val_grav_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/grav\"\n",
        "train_val_cont_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/cont\"\n",
        "\n",
        "# ファイル名が \"olympia_\" から始まらない場合、\"olympia_\" を先頭につけたファイル名にリネームする\n",
        "def rename_file(path):\n",
        "    for filename in os.listdir(path):\n",
        "        if not filename.startswith(\"oly_\"):\n",
        "            new_filename = \"oly_\" + filename\n",
        "            os.rename(os.path.join(path, filename), os.path.join(path, new_filename))\n",
        "\n",
        "# リネーム処理を実行\n",
        "rename_file(test_grav_dir)\n",
        "rename_file(test_cont_dir)\n",
        "rename_file(train_val_grav_dir)\n",
        "rename_file(train_val_cont_dir)\n"
      ],
      "metadata": {
        "id": "AleU8QFERy6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/grav\")))\n",
        "print(len(os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/cont\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ3tV45TLy95",
        "outputId": "16637498-4e0d-4216-8abf-7f9a790bed0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "############################\n",
        "# Handai GO_extended_dataset\n",
        "#\n",
        "# /treatable → /train_val/grav\n",
        "# /control → /train_val/Control_photo_1886mai\n",
        "#\n",
        "############################\n",
        "\n",
        "if not os.path.exists(\"/content/GO_extended_dataset\"):\n",
        "    !unzip /content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip -d /content/\n",
        "\n",
        "#file_name, class, idxのりストを作成\n",
        "def get_file_info(src_dir, class_id):\n",
        "    file_list = os.listdir(src_dir)\n",
        "    ids_list = [int(file_name.split('.')[0].split('-')[0]) for file_name in file_list]\n",
        "    class_list = [class_id] * len(file_list)\n",
        "    return file_list, ids_list, class_list\n",
        "\n",
        "# Contフォルダの情報\n",
        "cont_dir = '/content/GO_extended_dataset/Control_photo_1886mai'\n",
        "file_list_cont, ids_list_cont, class_list_cont = get_file_info(cont_dir, 0)\n",
        "\n",
        "# Gravフォルダの情報\n",
        "grav_dir = '/content/GO_extended_dataset/treatable'\n",
        "file_list_grav, ids_list_grav, class_list_grav = get_file_info(grav_dir, 1)\n",
        "\n",
        "# 結合\n",
        "file_list = file_list_cont + file_list_grav\n",
        "ids_list = ids_list_cont + ids_list_grav\n",
        "class_list = class_list_cont + class_list_grav\n",
        "\n",
        "print(file_list)\n",
        "print(ids_list)\n",
        "print(class_list)\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# file_listに画像のパスが、ids_listに患者のidが、class_listにgravかcontか(0 or 1)が格納されていると仮定する\n",
        "# 例: file_list = ['/path/to/image1.jpg', '/path/to/image2.jpg', ...]\n",
        "#     ids_list = ['patient1', 'patient1', 'patient2', 'patient3', ...]\n",
        "#     class_list = [0, 1, 0, 1, ...]\n",
        "\n",
        "# ランダムシードを固定する\n",
        "random.seed(1234)\n",
        "\n",
        "# idごとに画像のリストをまとめる\n",
        "id_to_files = {}\n",
        "for i, id in enumerate(ids_list):\n",
        "    if id not in id_to_files:\n",
        "        id_to_files[id] = []\n",
        "    id_to_files[id].append((file_list[i], class_list[i]))\n",
        "\n",
        "# idごとに画像をtestかtrainに振り分ける\n",
        "test_files = []\n",
        "train_files = []\n",
        "for id in id_to_files:\n",
        "    files = id_to_files[id]\n",
        "    random.shuffle(files)  # 同じid内の画像の順番をランダムに並べ替える\n",
        "    for file, cls in files:\n",
        "        if len(test_files) < 50 and cls == 0:\n",
        "            test_files.append((file, cls))\n",
        "        else:\n",
        "            train_files.append((file, cls))\n",
        "\n",
        "# 確認\n",
        "print(len(test_files), len(train_files))  # 50 以上の場合は再実行してください。"
      ],
      "metadata": {
        "id": "DjeQtXC5F_7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "############################\n",
        "# Handai GO_extended_dataset\n",
        "#\n",
        "# /treatable → /train_val/grav\n",
        "# /control → /train_val/Control_photo_1886mai\n",
        "#\n",
        "############################\n",
        "\n",
        "if not os.path.exists(\"/content/GO_extended_dataset\"):\n",
        "    !unzip /content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip -d /content/\n",
        "\n",
        "# Contフォルダのパス\n",
        "src_dir = '/content/GO_extended_dataset/Control_photo_1886mai'\n",
        "train_val_dir = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/cont'\n",
        "test_dir = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/cont'\n",
        "\n",
        "# ファイルリストを取得し、IDを抽出してリストに格納\n",
        "file_list = os.listdir(src_dir)\n",
        "ids = [int(file_name.split('.')[0].split('-', 1)[0]) for file_name in file_list]\n",
        "\n",
        "# IDごとにグループを作成して、group-stratifiedに分割\n",
        "splitter = GroupShuffleSplit(n_splits=1, test_size=50, random_state=2)\n",
        "train_val_indices, test_indices = next(splitter.split(file_list, groups=ids))\n",
        "\n",
        "# 学習用とテスト用のフォルダにファイルをコピー\n",
        "for i, file_name in tqdm(enumerate(file_list), total=len(file_list)):\n",
        "    src_path = os.path.join(src_dir, file_name)\n",
        "    dst_dir = test_dir if i in test_indices else train_val_dir\n",
        "    dst_path = os.path.join(dst_dir, file_name)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "# Gravフォルダのパス\n",
        "src_dir = '/content/GO_extended_dataset/treatable'\n",
        "train_val_dir = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/grav'\n",
        "test_dir = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/grav'\n",
        "\n",
        "# ファイルリストを取得し、IDを抽出してリストに格納\n",
        "file_list = os.listdir(src_dir)\n",
        "ids = [int(file_name.split('.')[0].split('-', 1)[0]) for file_name in file_list]\n",
        "\n",
        "# IDごとにグループを作成して、group-stratifiedに分割\n",
        "splitter = GroupShuffleSplit(n_splits=1, test_size=15, random_state=4)\n",
        "train_val_indices, test_indices = next(splitter.split(file_list, groups=ids))\n",
        "print(len(test_indices))\n",
        "\n",
        "# 学習用とテスト用のフォルダにファイルをコピー\n",
        "for i, file_name in tqdm(enumerate(file_list), total=len(file_list)):\n",
        "    src_path = os.path.join(src_dir, file_name)\n",
        "    dst_dir = test_dir if i in test_indices else train_val_dir\n",
        "    dst_path = os.path.join(dst_dir, file_name)\n",
        "    shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy5IinNPpPgO",
        "outputId": "ff41944c-0e82-4626-a441-25bfb0dbbe2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1886/1886 [00:45<00:00, 41.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1879/1879 [00:37<00:00, 50.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #（GroupShuffleSplitがgroupごとに分割するので）総数が50になるようにrandom_stateとtest_sizeを調節\n",
        "# # 結果の格納用リスト\n",
        "# # Gravフォルダのパス\n",
        "# src_dir = '/content/GO_extended_dataset/treatable'\n",
        "# train_val_dir = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/grav'\n",
        "# test_dir = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/grav'\n",
        "\n",
        "# # ファイルリストを取得し、IDを抽出してリストに格納\n",
        "# file_list = os.listdir(src_dir)\n",
        "# ids = [int(file_name.split('.')[0].split('-', 1)[0]) for file_name in file_list]\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for random_state in range(10):\n",
        "#     for test_size in range(1, 51):\n",
        "#         splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "#         train_val_indices, test_indices = next(splitter.split(file_list, groups=ids))\n",
        "#         test_set_size = len(test_indices)\n",
        "#         print(f\"random_state: {random_state}, test_size: {test_size}, test_set_size: {test_set_size}\")\n",
        "#         if test_set_size == 50:\n",
        "#             print(\"Coooool!!\")\n",
        "#             results.append((random_state, test_size))\n",
        "\n",
        "# # 結果の表示\n",
        "# for result in results:\n",
        "#     print(f\"random_state: {result[0]}, test_size: {result[1]}\")\n"
      ],
      "metadata": {
        "id": "MMk9fpa9zkZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ファイル数の確認**"
      ],
      "metadata": {
        "id": "jQkGRfUO9omP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO\"\n",
        "print(len(os.listdir(f\"{dir}/train_val/grav\")))\n",
        "print(len(os.listdir(f\"{dir}/train_val/cont\")))\n",
        "print(len(os.listdir(f\"{dir}/test/olympia/grav\")))\n",
        "print(len(os.listdir(f\"{dir}/test/olympia/cont\")))\n",
        "print(len(os.listdir(f\"{dir}/test/handai/grav\")))\n",
        "print(len(os.listdir(f\"{dir}/test/handai/cont\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGGR7_-P96mS",
        "outputId": "4624aede-b57d-4afb-a2bb-87da77b99557"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2340\n",
            "1933\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**既存のYOLOv5モデルを用いてバウンディングボックスを設定**"
      ],
      "metadata": {
        "id": "R6FT8pBYNvF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dst_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val/labels\"\n",
        "if os.path.exists(dst_dir):\n",
        "    shutil.rmtree(dst_dir)\n",
        "os.makedirs(f\"{dst_dir}/grav\")\n",
        "os.makedirs(f\"{dst_dir}/cont\")"
      ],
      "metadata": {
        "id": "-wggPJzAOBYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gF6whZ7P5ax",
        "outputId": "199e92f5-42c4-4b82-e5e6-879686d42334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.10.11 torch-2.0.1+cu118 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 34.7/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "Yo05tVYVaD1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interference_thres(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.15, iou_thres=0.25, classes=None,  max_det=1000)\n",
        "\n",
        "    #print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "P-HTdamj-mfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YOLOv5_model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5n_130epch.pt\"\n",
        "parent_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val\"\n",
        "\n",
        "for class_name in [\"grav\", \"cont\"]:\n",
        "    if class_name == \"cont\":\n",
        "        class_num = 0\n",
        "    else:\n",
        "        class_num = 1\n",
        "    for image_path in glob.glob(f\"{parent_dir}/{class_name}/*\"):\n",
        "        name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "        print(f\"image_name: {name}\")\n",
        "\n",
        "        #YOLOv5によりバウンディングボックスを抜き出す\n",
        "        try:\n",
        "            pred = interference(image_path, YOLOv5_model_path)[0].tolist()[0]\n",
        "        except: #通常の閾値設定で検出されない場合\n",
        "            pred = interference_thres(image_path, YOLOv5_model_path)[0].tolist()[0]\n",
        "        pred = [max(num, 0) for num in pred] #0より小さい座標は0に修正する\n",
        "        pred = [min(num, 640) for num in pred] #640より小さい座標は640に修正する\n",
        "        xcenter = (pred[0]+pred[2])/2\n",
        "        ycenter = (pred[1]+pred[3])/2\n",
        "        xwidth = pred[2]-pred[0]\n",
        "        ywidth = pred[3]-pred[1]\n",
        "        \n",
        "        # ファイル名と作成する内容を指定\n",
        "        txt_name = f\"{parent_dir}/labels/{class_name}/{name}.txt\"\n",
        "        content = f\"{class_num} {round(xcenter)} {round(ycenter)} {round(xwidth)} {round(ywidth)}\"\n",
        "        with open(txt_name, \"w\") as file:\n",
        "            # テキストをファイルに書き込む\n",
        "            file.write(content)\n",
        "            print(f\"text file successfully created🙆 {content}\")\n",
        "            print(\"\")\n",
        "            print(\"\")\n",
        "        "
      ],
      "metadata": {
        "id": "wlGbrrAWRtHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val\"\n",
        "print(len(os.listdir(f\"{dir}/grav\")))\n",
        "print(len(os.listdir(f\"{dir}/cont\")))\n",
        "print(len(os.listdir(f\"{dir}/labels/grav\")))\n",
        "print(len(os.listdir(f\"{dir}/labels/cont\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzlaSDfE_Zk1",
        "outputId": "b5774067-eedf-4309-8e81-d219980a27e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2340\n",
            "1933\n",
            "2340\n",
            "1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5用トレーニング用フォルダ作成**\n",
        "\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Extend_dataset_YOLOv5%EF%BC%A0.ipynb\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/"
      ],
      "metadata": {
        "id": "9XI1oisiBK1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "---------------super_extend_for_YOLO_training --------train------labels\n",
        "                                              |           |--images\n",
        "                                              |            \n",
        "                                              |------valid ----labels\n",
        "                                              |           |--images\n",
        "                                              |\n",
        "                                              |--dataset.yaml\n",
        "                                              |--yolov5                                    \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nt3WfWu_Bb_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dst_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training\"\n",
        "orig_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val\"\n"
      ],
      "metadata": {
        "id": "ZbyOJj3g_8Fj"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(dst_path):\n",
        "    shutil.rmtree(dst_path)\n",
        "os.makedirs(dst_path)\n",
        "os.makedirs(f\"{dst_path}/train/images\")\n",
        "os.makedirs(f\"{dst_path}/train/labels\")\n",
        "os.makedirs(f\"{dst_path}/valid/images\")\n",
        "os.makedirs(f\"{dst_path}/valid/labels\")"
      ],
      "metadata": {
        "id": "DtXJAAtsCjP3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVip7xMUEDgf",
        "outputId": "cf84c02d-1f9a-4967-dd8a-b55121ab6abb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMwlnFiqCpxI",
        "outputId": "6e63e566-9cbb-4a98-a324-d8e86eb7bd38"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset_list\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dataset_dir =  \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train_val\"\n",
        "\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "    \n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    print(f\"image_list: {image_list}\")\n",
        "    print(f\"label_list: {label_list}\")\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(orig_path, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(orig_path, \"cont\")\n",
        "\n",
        "print(\"image\")\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")\n",
        "print(\"label\")\n",
        "print(f\"grav: {len(grav_label_list)}\")\n",
        "print(f\"cont: {len(cont_label_list)}\")"
      ],
      "metadata": {
        "id": "Lb7vg1IfHSCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset using GroupShuffleSplit\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "############################\n",
        "# 以下をGroupShuffleSplitで分ける\n",
        "# super_extend_dataset_for_YOLO/train_val/grav\n",
        "# → /super_extend_for_YOLO_training/train/images \n",
        "# → /super_extend_for_YOLO_training/vallid/images\n",
        "# \n",
        "# super_extend_dataset_for_YOLO/train_val/cont\n",
        "# → /super_extend_for_YOLO_training/train/images \n",
        "# → /super_extend_for_YOLO_training/vallid/images\n",
        "# \n",
        "# 以下はbasenameが同じものを移動するのみとする\n",
        "# super_extend_dataset_for_YOLO/train_val/label/grav\n",
        "# → /super_extend_for_YOLO_training/train/labels \n",
        "# → /super_extend_for_YOLO_training/vallid/labels\n",
        "#\n",
        "# super_extend_dataset_for_YOLO/train_val/label/grav\n",
        "# → /super_extend_for_YOLO_training/train/labels \n",
        "# → /super_extend_for_YOLO_training/vallid/labels\n",
        "############################\n",
        "\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training\"\n",
        "orig_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/train_val\"\n",
        "\n",
        "for class_name in [\"cont\", \"grav\"]:\n",
        "    # フォルダのパス\n",
        "    src_dir = f\"{orig_folder}/{class_name}\"\n",
        "    train_dir = f\"{dst_folder}/train/images\"\n",
        "    valid_dir = f\"{dst_folder}/valid/images\"\n",
        "\n",
        "    # ファイルリストを取得し、IDを抽出してリストに格納\n",
        "    file_list = os.listdir(src_dir)\n",
        "    ids = [str(file_name.split('.')[0].split('-', 1)[0]) for file_name in file_list]\n",
        "\n",
        "    # IDごとにグループを作成して、group-stratifiedに分割\n",
        "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=2)\n",
        "    train_indices, valid_indices = next(splitter.split(file_list, groups=ids))\n",
        "\n",
        "    # 学習用とテスト用のフォルダにファイルをコピー\n",
        "    for i, file_name in tqdm(enumerate(file_list), total=len(file_list)):\n",
        "        src_path = os.path.join(src_dir, file_name)        \n",
        "        dst_dir = valid_dir if i in valid_indices else train_dir\n",
        "        dst_path = os.path.join(dst_dir, file_name)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "        label_file_name = file_name.rsplit(\".\", 1)[0] + \".txt\" #同じ名前のラベルファイルも移動\n",
        "        print(f\"label_file_name: {label_file_name}\")\n",
        "        label_dst_dir = dst_path.replace(\"images\", \"labels\")\n",
        "        label_src_path = os.path.join(orig_folder, \"labels\", class_name, label_file_name)\n",
        "        label_dst_path = os.path.join(label_dst_dir, label_file_name)\n",
        "        #shutil.copy(label_src_path, label_dst_path)"
      ],
      "metadata": {
        "id": "f8hUG0YRLnUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4814a82-5124-47bf-b16d-e102876e89a4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 6/1933 [00:00<00:34, 55.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 5252.txt\n",
            "label_file_name: 7618.txt\n",
            "label_file_name: 5150.txt\n",
            "label_file_name: 6118.txt\n",
            "label_file_name: 3588.txt\n",
            "label_file_name: 6193.txt\n",
            "label_file_name: 3651.txt\n",
            "label_file_name: 3878.txt\n",
            "label_file_name: 4904.txt\n",
            "label_file_name: 6049.txt\n",
            "label_file_name: 1023.txt\n",
            "label_file_name: 5369.txt\n",
            "label_file_name: 4560.txt\n",
            "label_file_name: 2663.txt\n",
            "label_file_name: 9716-20210126-75-091511_e0129e7d6f1d8520d8e8bdc6486e150af79014900975d90b110edb1cd73eb0a0.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 16/1933 [00:00<00:24, 77.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 9117-20200909-49-100502_c3a6822858249faf8f2abc3cc93507027e3de2daca680f602410e91dd8c9e5e3.txt\n",
            "label_file_name: 4545.txt\n",
            "label_file_name: 4441.txt\n",
            "label_file_name: 4111.txt\n",
            "label_file_name: 760.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 32/1933 [00:05<05:34,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 4383.txt\n",
            "label_file_name: 4193.txt\n",
            "label_file_name: 6521.txt\n",
            "label_file_name: 3728.txt\n",
            "label_file_name: 7015.txt\n",
            "label_file_name: 719.txt\n",
            "label_file_name: 1102.txt\n",
            "label_file_name: 3691.txt\n",
            "label_file_name: 3500.txt\n",
            "label_file_name: 6127.txt\n",
            "label_file_name: 7134.txt\n",
            "label_file_name: 1900.txt\n",
            "label_file_name: 4201.txt\n",
            "label_file_name: 9274-20191225-10-100928_376573b69f03e21f6cf87814e512ec5a7d0bc87a706d6b0de01aacb48c83103a.txt\n",
            "label_file_name: 5091.txt\n",
            "label_file_name: 3165.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 48/1933 [00:05<02:39, 11.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 1980.txt\n",
            "label_file_name: 1451.txt\n",
            "label_file_name: 5300.txt\n",
            "label_file_name: 3547.txt\n",
            "label_file_name: 562.txt\n",
            "label_file_name: 1882.txt\n",
            "label_file_name: 4380.txt\n",
            "label_file_name: 1166.txt\n",
            "label_file_name: 9714-20201209-66-084635_e3bee3cc2b60164bf11eb445d43d9febcc24f7513f4b8cb94c4a6be78f337836.txt\n",
            "label_file_name: 4786.txt\n",
            "label_file_name: 3599.txt\n",
            "label_file_name: 913.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 63/1933 [00:05<01:28, 21.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 5130.txt\n",
            "label_file_name: 7338.txt\n",
            "label_file_name: 8008.txt\n",
            "label_file_name: 1795.txt\n",
            "label_file_name: 4928.txt\n",
            "label_file_name: 8145.txt\n",
            "label_file_name: 1428.txt\n",
            "label_file_name: 6264.txt\n",
            "label_file_name: 6220.txt\n",
            "label_file_name: 5450.txt\n",
            "label_file_name: 5447.txt\n",
            "label_file_name: 1459.txt\n",
            "label_file_name: 4737.txt\n",
            "label_file_name: 4672.txt\n",
            "label_file_name: 7767.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 80/1933 [00:05<00:51, 35.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 9744-20200227-34-092640_c31bf53608f210e2b1f2d4a9ff05514e385dfa2ab5c92450cdcf717509fc9ad4.txt\n",
            "label_file_name: 3307.txt\n",
            "label_file_name: 976.txt\n",
            "label_file_name: 4517.txt\n",
            "label_file_name: 9830-20191030-77-124411_5c3a498f8fae9ffd2a451832f5346d885154eb17626a329b3febb49198bc23de.txt\n",
            "label_file_name: 2767.txt\n",
            "label_file_name: 2323.txt\n",
            "label_file_name: 118.txt\n",
            "label_file_name: 4222.txt\n",
            "label_file_name: 4508.txt\n",
            "label_file_name: 4038.txt\n",
            "label_file_name: 9222-20200819-16-101307_c8721e23affbd5e09a2fdc4ab8c1fe7c62e90d754d8ad7ca0921a2d9920b9223.txt\n",
            "label_file_name: 1276.txt\n",
            "label_file_name: 6879.txt\n",
            "label_file_name: 4065.txt\n",
            "label_file_name: 3906.txt\n",
            "label_file_name: 9545-20210414-73-111436_0ad95f678bc6273ed159422304707f885d55cf4b4020305ff237bdc009935b19.txt\n",
            "label_file_name: 6758.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 98/1933 [00:06<00:35, 52.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 4916.txt\n",
            "label_file_name: 2098.txt\n",
            "label_file_name: 2969.txt\n",
            "label_file_name: 5247.txt\n",
            "label_file_name: 4952.txt\n",
            "label_file_name: 3771.txt\n",
            "label_file_name: 5232.txt\n",
            "label_file_name: 2583.txt\n",
            "label_file_name: 2731.txt\n",
            "label_file_name: 6171.txt\n",
            "label_file_name: 3109.txt\n",
            "label_file_name: 5332.txt\n",
            "label_file_name: 8138.txt\n",
            "label_file_name: 5521.txt\n",
            "label_file_name: 6531.txt\n",
            "label_file_name: 2971.txt\n",
            "label_file_name: 2452.txt\n",
            "label_file_name: 5195.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 116/1933 [00:06<00:27, 65.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 3021.txt\n",
            "label_file_name: 5714.txt\n",
            "label_file_name: 2110.txt\n",
            "label_file_name: 2308.txt\n",
            "label_file_name: 3987.txt\n",
            "label_file_name: 9310-20200930-4-121022_ad117f7aba42b14b441d331792384abd5b3b804c7a2526a0f5569af0eb83cbbb.txt\n",
            "label_file_name: 9428-20200709-73-112643_38bebd9e863ec286c3a270059c9f5232649d62ef6ba873a3a24f6dcef1ea992a.txt\n",
            "label_file_name: 932.txt\n",
            "label_file_name: 8134.txt\n",
            "label_file_name: 4464.txt\n",
            "label_file_name: 4396.txt\n",
            "label_file_name: 1837.txt\n",
            "label_file_name: 4372.txt\n",
            "label_file_name: 9257-20191121-51-121321_c77e422200a8e43dd49051cce9e5b28bce4a8b43576fa612b1ba19c73fdfa582.txt\n",
            "label_file_name: 802.txt\n",
            "label_file_name: 1531.txt\n",
            "label_file_name: 9512-20190918-53-084402_2b957945ff94af4ed1ba088aff4fef508d18c1124ec6a71837d4c5a4e7b39404.txt\n",
            "label_file_name: 5340.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 125/1933 [00:06<00:28, 62.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 9377-20210203-44-111819_58701e7c75925f02441028b1ba1e4ad99cc93f80052c299d39b7dce9de35ef9e.txt\n",
            "label_file_name: 2555.txt\n",
            "label_file_name: 9382-20200402-15-144452_e585a944c82b3f3a9511c4e77dc69147f9fd30e7bc68994c77db3b426df68f1f.txt\n",
            "label_file_name: 1034.txt\n",
            "label_file_name: 1820.txt\n",
            "label_file_name: 4633.txt\n",
            "label_file_name: 4647.txt\n",
            "label_file_name: 9700-20200728-53-163045_63a671f65d4440a29529cf4584e7e60524c4907f3542f7bfb38086fac6c70fe5.txt\n",
            "label_file_name: 8117.txt\n",
            "label_file_name: 9655-20210115-72-125405_49ee954cadf2e261cdf50b42626545a8ef391142d1a16b7b413603f6d66f285b.txt\n",
            "label_file_name: 6815.txt\n",
            "label_file_name: 6154.txt\n",
            "label_file_name: 5106.txt\n",
            "label_file_name: 3220.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 142/1933 [00:06<00:25, 70.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 5061.txt\n",
            "label_file_name: 2113.txt\n",
            "label_file_name: 4513.txt\n",
            "label_file_name: 4862.txt\n",
            "label_file_name: 168.txt\n",
            "label_file_name: 2805.txt\n",
            "label_file_name: 2698.txt\n",
            "label_file_name: 9447-20200819-80-121147_3807cc8b8e8859893ca6258c3fc49828facef5dd170ed7a670ca9756644717ef.txt\n",
            "label_file_name: 9653-20210113-85-085113_cf6fe7e2b594c8fc2e289bd3819e9529b1832e6d9a7e962d74a996d0f95d67a1.txt\n",
            "label_file_name: 956.txt\n",
            "label_file_name: 3090.txt\n",
            "label_file_name: 4762.txt\n",
            "label_file_name: 3579.txt\n",
            "label_file_name: 4833.txt\n",
            "label_file_name: 9654-20200311-84-095556_e0d0f3f3dedfed97d54bbd2e9d0cca32ab879db76b60501a0897c4fb632c71b4.txt\n",
            "label_file_name: 5177.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 150/1933 [00:06<00:26, 66.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 9205-20210121-67-103344_45f86458b9368297a3a6a8b4cc08e76d5f1b4ea78a9ff04430dffbbf4167cbed.txt\n",
            "label_file_name: 1948.txt\n",
            "label_file_name: 5997.txt\n",
            "label_file_name: 3384.txt\n",
            "label_file_name: 567.txt\n",
            "label_file_name: 477.txt\n",
            "label_file_name: 4139.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▊         | 167/1933 [00:07<00:38, 45.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 1772.txt\n",
            "label_file_name: 1424.txt\n",
            "label_file_name: 3674.txt\n",
            "label_file_name: 4763.txt\n",
            "label_file_name: 775.txt\n",
            "label_file_name: 6105.txt\n",
            "label_file_name: 3804.txt\n",
            "label_file_name: 7251.txt\n",
            "label_file_name: 9593-20210415-62-105459_c305d7ef60d51b88b8ef26274c105a817a2a648dbd8f87c15bac2bbae77b5495.txt\n",
            "label_file_name: 5501.txt\n",
            "label_file_name: 7258.txt\n",
            "label_file_name: 6773.txt\n",
            "label_file_name: 9400-20201112-25-142818_d3ebb0bfe120ff9f4d601564c3d0b3325f0b70642585b3f41fcb6c0fad2dd9f5.txt\n",
            "label_file_name: 2585.txt\n",
            "label_file_name: 66.txt\n",
            "label_file_name: 7140.txt\n",
            "label_file_name: 3761.txt\n",
            "label_file_name: 3560.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 184/1933 [00:07<01:12, 24.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file_name: 172.txt\n",
            "label_file_name: 9523-20210428-10-140328_a71d5119ca5662e721e0dbf820a9cb5e691267dedaae35e118c7499248697186.txt\n",
            "label_file_name: 6980.txt\n",
            "label_file_name: 821.txt\n",
            "label_file_name: 2196.txt\n",
            "label_file_name: 3247.txt\n",
            "label_file_name: 9350-20201111-7-112536_ad51a647fd99e1255e95509b0854d2ddef7baaec11c083e300d676b9f3b0a19a.txt\n",
            "label_file_name: 7544.txt\n",
            "label_file_name: 3578.txt\n",
            "label_file_name: 1181.txt\n",
            "label_file_name: 6319.txt\n",
            "label_file_name: 1568.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2c74505b37d3>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mdst_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dir\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mlabel_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m \u001b[0;31m#同じ名前のラベルファイルも移動\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0m_fastcopy_fcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COPYFILE_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_images: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images'))}\")\n",
        "print(f\"val_images: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images'))}\")\n",
        "print(f\"train_labels: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels'))}\")\n",
        "print(f\"val_labels: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels'))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S33XiPEhieCc",
        "outputId": "d99fd307-1437-4781-dac0-29bcab747e20"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images: 3480\n",
            "val_images: 792\n",
            "train_labels: 0\n",
            "val_labels: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dst_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nkm96dMkicsq",
        "outputId": "086dfd59-20d6-445d-e199-58032f15059c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMpdg5EhfVfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}