{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1zm0mqq9Qb/MyTlWjyxFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Export%20YOLOv5%20into%20CoreML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Export YOLOv5 into CoreML**\n",
        "\n",
        "https://github.com/junmcenroe/YOLOv5-CoreML-Export-with-NMS/blob/main/export-coreml-nms.py"
      ],
      "metadata": {
        "id": "FFvTNR0mQBMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5PbxXfcDzUs",
        "outputId": "58ded182-4cfd-4380-d8f3-532f1db4f3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "import shutil\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "LTYPIiE7D1sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip install coremltools==6.2\n",
        "!git clone https://github.com/junmcenroe/YOLOv5-CoreML-Export-with-NMS.git\n",
        "shutil.copy(\"/content/YOLOv5-CoreML-Export-with-NMS/export-coreml-nms.py\", \"/content/yolov5/export-coreml-nms.py\")\n",
        "\n",
        "###‚Üì‚Üì git clone„Åå„ÅÜ„Åæ„Åè„ÅÑ„Åã„Å™„Åè„Å™„Å£„ÅüÊôÇ„ÅÆ‰∫àÂÇô"
      ],
      "metadata": {
        "id": "nWLnY5QIJV5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%% writefile /content/yolov5/export-coreml-nms.py\n",
        "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit\n",
        "\n",
        "Format                      | `export.py --include`         | Model\n",
        "---                         | ---                           | ---\n",
        "PyTorch                     | -                             | yolov5s.pt\n",
        "TorchScript                 | `torchscript`                 | yolov5s.torchscript\n",
        "ONNX                        | `onnx`                        | yolov5s.onnx\n",
        "OpenVINO                    | `openvino`                    | yolov5s_openvino_model/\n",
        "TensorRT                    | `engine`                      | yolov5s.engine\n",
        "CoreML                      | `coreml`                      | yolov5s.mlmodel\n",
        "TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\n",
        "TensorFlow GraphDef         | `pb`                          | yolov5s.pb\n",
        "TensorFlow Lite             | `tflite`                      | yolov5s.tflite\n",
        "TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\n",
        "TensorFlow.js               | `tfjs`                        | yolov5s_web_model/\n",
        "PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/\n",
        "\n",
        "Requirements:\n",
        "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU\n",
        "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU\n",
        "\n",
        "Usage:\n",
        "    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...\n",
        "\n",
        "Inference:\n",
        "    $ python detect.py --weights yolov5s.pt                 # PyTorch\n",
        "                                 yolov5s.torchscript        # TorchScript\n",
        "                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
        "                                 yolov5s_openvino_model     # OpenVINO\n",
        "                                 yolov5s.engine             # TensorRT\n",
        "                                 yolov5s.mlmodel            # CoreML (macOS-only)\n",
        "                                 yolov5s_saved_model        # TensorFlow SavedModel\n",
        "                                 yolov5s.pb                 # TensorFlow GraphDef\n",
        "                                 yolov5s.tflite             # TensorFlow Lite\n",
        "                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
        "                                 yolov5s_paddle_model       # PaddlePaddle\n",
        "\n",
        "TensorFlow.js:\n",
        "    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example\n",
        "    $ npm install\n",
        "    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model\n",
        "    $ npm start\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import contextlib\n",
        "import json\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "if platform.system() != 'Windows':\n",
        "    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\n",
        "from utils.dataloaders import LoadImages\n",
        "from utils.general import (LOGGER, Profile, check_dataset, check_img_size, check_requirements, check_version,\n",
        "                           check_yaml, colorstr, file_size, get_default_args, print_args, url2file, yaml_save)\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "MACOS = platform.system() == 'Darwin'  # macOS environment\n",
        "\n",
        "\n",
        "def export_formats():\n",
        "    # YOLOv5 export formats\n",
        "    x = [\n",
        "        ['PyTorch', '-', '.pt', True, True],\n",
        "        ['TorchScript', 'torchscript', '.torchscript', True, True],\n",
        "        ['ONNX', 'onnx', '.onnx', True, True],\n",
        "        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n",
        "        ['TensorRT', 'engine', '.engine', False, True],\n",
        "        ['CoreML', 'coreml', '.mlmodel', True, False],\n",
        "        ['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],\n",
        "        ['TensorFlow GraphDef', 'pb', '.pb', True, True],\n",
        "        ['TensorFlow Lite', 'tflite', '.tflite', True, False],\n",
        "        ['TensorFlow Edge TPU', 'edgetpu', '_edgetpu.tflite', False, False],\n",
        "        ['TensorFlow.js', 'tfjs', '_web_model', False, False],\n",
        "        ['PaddlePaddle', 'paddle', '_paddle_model', True, True],]\n",
        "    return pd.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n",
        "\n",
        "\n",
        "def try_export(inner_func):\n",
        "    # YOLOv5 export decorator, i..e @try_export\n",
        "    inner_args = get_default_args(inner_func)\n",
        "\n",
        "    def outer_func(*args, **kwargs):\n",
        "        prefix = inner_args['prefix']\n",
        "        try:\n",
        "            with Profile() as dt:\n",
        "                f, model = inner_func(*args, **kwargs)\n",
        "            LOGGER.info(f'{prefix} export success ‚úÖ {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')\n",
        "            return f, model\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'{prefix} export failure ‚ùå {dt.t:.1f}s: {e}')\n",
        "            return None, None\n",
        "\n",
        "    return outer_func\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n",
        "    # YOLOv5 TorchScript model export\n",
        "    LOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n",
        "    f = file.with_suffix('.torchscript')\n",
        "\n",
        "    ts = torch.jit.trace(model, im, strict=False)\n",
        "    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n",
        "    extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n",
        "    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n",
        "        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n",
        "    else:\n",
        "        ts.save(str(f), _extra_files=extra_files)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n",
        "    # YOLOv5 ONNX export\n",
        "    check_requirements('onnx')\n",
        "    import onnx\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\n",
        "    f = file.with_suffix('.onnx')\n",
        "\n",
        "    output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']\n",
        "    if dynamic:\n",
        "        dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\n",
        "        if isinstance(model, SegmentationModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "            dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\n",
        "        elif isinstance(model, DetectionModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n",
        "        im.cpu() if dynamic else im,\n",
        "        f,\n",
        "        verbose=False,\n",
        "        opset_version=opset,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['images'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes=dynamic or None)\n",
        "\n",
        "    # Checks\n",
        "    model_onnx = onnx.load(f)  # load onnx model\n",
        "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
        "\n",
        "    # Metadata\n",
        "    d = {'stride': int(max(model.stride)), 'names': model.names}\n",
        "    for k, v in d.items():\n",
        "        meta = model_onnx.metadata_props.add()\n",
        "        meta.key, meta.value = k, str(v)\n",
        "    onnx.save(model_onnx, f)\n",
        "\n",
        "    # Simplify\n",
        "    if simplify:\n",
        "        try:\n",
        "            cuda = torch.cuda.is_available()\n",
        "            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))\n",
        "            import onnxsim\n",
        "\n",
        "            LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')\n",
        "            model_onnx, check = onnxsim.simplify(model_onnx)\n",
        "            assert check, 'assert check failed'\n",
        "            onnx.save(model_onnx, f)\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'{prefix} simplifier failure: {e}')\n",
        "    return f, model_onnx\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_openvino(file, metadata, half, prefix=colorstr('OpenVINO:')):\n",
        "    # YOLOv5 OpenVINO export\n",
        "    check_requirements('openvino-dev')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n",
        "    import openvino.inference_engine as ie\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with openvino {ie.__version__}...')\n",
        "    f = str(file).replace('.pt', f'_openvino_model{os.sep}')\n",
        "\n",
        "    cmd = f\"mo --input_model {file.with_suffix('.onnx')} --output_dir {f} --data_type {'FP16' if half else 'FP32'}\"\n",
        "    subprocess.run(cmd.split(), check=True, env=os.environ)  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_paddle(model, im, file, metadata, prefix=colorstr('PaddlePaddle:')):\n",
        "    # YOLOv5 Paddle export\n",
        "    check_requirements(('paddlepaddle', 'x2paddle'))\n",
        "    import x2paddle\n",
        "    from x2paddle.convert import pytorch2paddle\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n",
        "    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n",
        "\n",
        "    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[im])  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "# @try_export\n",
        "# def export_coreml(model, im, file, int8, half, prefix=colorstr('CoreML:')):\n",
        "#     # YOLOv5 CoreML export\n",
        "#     check_requirements('coremltools')\n",
        "#     import coremltools as ct\n",
        "\n",
        "#     LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
        "#     f = file.with_suffix('.mlmodel')\n",
        "\n",
        "#     ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n",
        "#     ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "#     bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "#     if bits < 32:\n",
        "#         if MACOS:  # quantization only supported on macOS\n",
        "#             with warnings.catch_warnings():\n",
        "#                 warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "#                 ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
        "#         else:\n",
        "#             print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "#     ct_model.save(f)\n",
        "#     return f, ct_model\n",
        "\n",
        "class CoreMLExportModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, img_size):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)[0]\n",
        "        x = x.squeeze(0)\n",
        "        # Convert box coords to normalized coordinates [0 ... 1]\n",
        "        w = self.img_size[0]\n",
        "        h = self.img_size[1]\n",
        "        objectness = x[:, 4:5]\n",
        "        class_probs = x[:, 5:] * objectness\n",
        "        boxes = x[:, :4] * torch.tensor([1. / w, 1. / h, 1. / w, 1. / h])\n",
        "        return class_probs, boxes\n",
        "\n",
        "@try_export\n",
        "#def export_coreml(model, im, file, num_boxes, num_classes, labels, conf_thres, iou_thres, prefix=colorstr('CoreML:')):\n",
        "def export_coreml(model, im, file, int8, half, num_boxes, num_classes, labels, conf_thres, iou_thres, prefix=colorstr('CoreML:')):\n",
        "    # YOLOv5 CoreML export\n",
        "    check_requirements(('coremltools',))\n",
        "    import coremltools as ct\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
        "    f = file.with_suffix('.mlmodel')\n",
        "\n",
        "    export_model = CoreMLExportModel(model, img_size=opt.imgsz)\n",
        "\n",
        "    ts = torch.jit.trace(export_model.eval(), im, strict=False)  # TorchScript model\n",
        "\n",
        "    orig_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "\n",
        "    # quantize\n",
        "    bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "    if bits < 32:\n",
        "        if MACOS:  # quantization only supported on macOS\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "                orig_model = ct.models.neural_network.quantization_utils.quantize_weights(orig_model, bits, mode)\n",
        "        else:\n",
        "            print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "\n",
        "    spec = orig_model.get_spec()\n",
        "    old_box_output_name = spec.description.output[1].name\n",
        "    old_scores_output_name = spec.description.output[0].name\n",
        "    ct.utils.rename_feature(spec, old_scores_output_name, \"raw_confidence\")\n",
        "    ct.utils.rename_feature(spec, old_box_output_name, \"raw_coordinates\")\n",
        "    spec.description.output[0].type.multiArrayType.shape.extend([num_boxes, num_classes])\n",
        "    spec.description.output[1].type.multiArrayType.shape.extend([num_boxes, 4])\n",
        "    spec.description.output[0].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "    spec.description.output[1].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "\n",
        "    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\n",
        "\n",
        "    # Model from spec\n",
        "    yolo_model = ct.models.MLModel(spec)\n",
        "\n",
        "    # 3. Create NMS protobuf\n",
        "    # Build Non Maximum Suppression model\n",
        "    nms_spec = ct.proto.Model_pb2.Model()\n",
        "    # nms_spec.specificationVersion = 3\n",
        "    nms_spec.specificationVersion = 5\n",
        "\n",
        "    for i in range(2):\n",
        "        decoder_output = spec.description.output[i].SerializeToString()\n",
        "        nms_spec.description.input.add()\n",
        "        nms_spec.description.input[i].ParseFromString(decoder_output)\n",
        "        nms_spec.description.output.add()\n",
        "        nms_spec.description.output[i].ParseFromString(decoder_output)\n",
        "\n",
        "    nms_spec.description.output[0].name = \"confidence\"\n",
        "    nms_spec.description.output[1].name = \"coordinates\"\n",
        "\n",
        "    output_sizes = [num_classes, 4]\n",
        "    for i in range(2):\n",
        "        ma_type = nms_spec.description.output[i].type.multiArrayType\n",
        "        ma_type.shapeRange.sizeRanges.add()\n",
        "        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n",
        "        ma_type.shapeRange.sizeRanges[0].upperBound = -1\n",
        "        ma_type.shapeRange.sizeRanges.add()\n",
        "        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n",
        "        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n",
        "        del ma_type.shape[:]\n",
        "\n",
        "    nms = nms_spec.nonMaximumSuppression\n",
        "    nms.confidenceInputFeatureName = \"raw_confidence\"\n",
        "    nms.coordinatesInputFeatureName = \"raw_coordinates\"\n",
        "    nms.confidenceOutputFeatureName = \"confidence\"\n",
        "    nms.coordinatesOutputFeatureName = \"coordinates\"\n",
        "    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n",
        "    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n",
        "    nms.iouThreshold = iou_thres\n",
        "    nms.confidenceThreshold = conf_thres\n",
        "    # nms.pickTop.perClass = False\n",
        "    nms.pickTop.perClass = True\n",
        "    nms.stringClassLabels.vector.extend(labels.values())\n",
        "    nms_model = ct.models.MLModel(nms_spec)\n",
        "\n",
        "    # 4. Pipeline models together\n",
        "    # Assembling a pipeline model from the two models\n",
        "    # input_features = [(\"image\", ct.models.datatypes.Array(3, 300, 300)),\n",
        "    input_features = [(\"image\", ct.models.datatypes.Array(3, ny, nx)),\n",
        "                        (\"iouThreshold\", ct.models.datatypes.Double()),\n",
        "                        (\"confidenceThreshold\", ct.models.datatypes.Double())]\n",
        "    output_features = [\"confidence\", \"coordinates\"]\n",
        "    pipeline = ct.models.pipeline.Pipeline(input_features, output_features)\n",
        "    pipeline.add_model(yolo_model)\n",
        "    pipeline.add_model(nms_model)\n",
        "\n",
        "    # Correct datatypes\n",
        "    # The \"image\" input should really be an image, not a multi-array\n",
        "    pipeline.spec.description.input[0].ParseFromString(spec.description.input[0].SerializeToString())\n",
        "    # Copy the declarations of the \"confidence\" and \"coordinates\" outputs\n",
        "    # The Pipeline makes these strings by default\n",
        "    pipeline.spec.description.output[0].ParseFromString(nms_spec.description.output[0].SerializeToString())\n",
        "    pipeline.spec.description.output[1].ParseFromString(nms_spec.description.output[1].SerializeToString())\n",
        "\n",
        "    # Add descriptions to the inputs and outputs\n",
        "    pipeline.spec.description.input[1].shortDescription = \"(optional) IOU Threshold override\"\n",
        "    pipeline.spec.description.input[2].shortDescription = \"(optional) Confidence Threshold override\"\n",
        "    pipeline.spec.description.output[0].shortDescription = \"Boxes Class confidence\"\n",
        "    pipeline.spec.description.output[1].shortDescription = \"Boxes [x, y, width, height] (normalized to [0...1])\"\n",
        "\n",
        "    # Add metadata to the model\n",
        "    pipeline.spec.description.metadata.shortDescription = \"YOLOv5 object detector\"\n",
        "    pipeline.spec.description.metadata.author = \"Ultralytics\"\n",
        "\n",
        "    # Add the default threshold values and list of class labels\n",
        "    user_defined_metadata = {\n",
        "        \"iou_threshold\": str(iou_thres),\n",
        "        \"confidence_threshold\": str(conf_thres),\n",
        "        \"classes\": \", \".join(labels.values())}\n",
        "    pipeline.spec.description.metadata.userDefined.update(user_defined_metadata)\n",
        "\n",
        "    # Don't forget this or Core ML might attempt to run the model on an unsupported operating system version!\n",
        "    pipeline.spec.specificationVersion = 5\n",
        "\n",
        "    ct_model = ct.models.MLModel(pipeline.spec)\n",
        "\n",
        "    f = str(file).replace('.pt', '.mlmodel')\n",
        "    ct_model.save(f)\n",
        "\n",
        "    LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
        "    return  f, ct_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n",
        "    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n",
        "    assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n",
        "    try:\n",
        "        import tensorrt as trt\n",
        "    except Exception:\n",
        "        if platform.system() == 'Linux':\n",
        "            check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\n",
        "        import tensorrt as trt\n",
        "\n",
        "    if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n",
        "        grid = model.model[-1].anchor_grid\n",
        "        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "        model.model[-1].anchor_grid = grid\n",
        "    else:  # TensorRT >= 8\n",
        "        check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt>=8.0.0\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "    onnx = file.with_suffix('.onnx')\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\n",
        "    assert onnx.exists(), f'failed to export ONNX file: {onnx}'\n",
        "    f = file.with_suffix('.engine')  # TensorRT engine file\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    if verbose:\n",
        "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
        "\n",
        "    builder = trt.Builder(logger)\n",
        "    config = builder.create_builder_config()\n",
        "    config.max_workspace_size = workspace * 1 << 30\n",
        "    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)  # fix TRT 8.4 deprecation notice\n",
        "\n",
        "    flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "    network = builder.create_network(flag)\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "    if not parser.parse_from_file(str(onnx)):\n",
        "        raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
        "\n",
        "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "    for inp in inputs:\n",
        "        LOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n",
        "    for out in outputs:\n",
        "        LOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n",
        "\n",
        "    if dynamic:\n",
        "        if im.shape[0] <= 1:\n",
        "            LOGGER.warning(f\"{prefix} WARNING ‚ö†Ô∏è --dynamic model requires maximum --batch-size argument\")\n",
        "        profile = builder.create_optimization_profile()\n",
        "        for inp in inputs:\n",
        "            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)\n",
        "        config.add_optimization_profile(profile)\n",
        "\n",
        "    LOGGER.info(f'{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}')\n",
        "    if builder.platform_has_fast_fp16 and half:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "    with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n",
        "        t.write(engine.serialize())\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_saved_model(model,\n",
        "                       im,\n",
        "                       file,\n",
        "                       dynamic,\n",
        "                       tf_nms=False,\n",
        "                       agnostic_nms=False,\n",
        "                       topk_per_class=100,\n",
        "                       topk_all=100,\n",
        "                       iou_thres=0.45,\n",
        "                       conf_thres=0.25,\n",
        "                       keras=False,\n",
        "                       prefix=colorstr('TensorFlow SavedModel:')):\n",
        "    # YOLOv5 TensorFlow SavedModel export\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except Exception:\n",
        "        check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\n",
        "        import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    from models.tf import TFModel\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    f = str(file).replace('.pt', '_saved_model')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "\n",
        "    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n",
        "    im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n",
        "    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)\n",
        "    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    keras_model.trainable = False\n",
        "    keras_model.summary()\n",
        "    if keras:\n",
        "        keras_model.save(f, save_format='tf')\n",
        "    else:\n",
        "        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n",
        "        m = tf.function(lambda x: keras_model(x))  # full model\n",
        "        m = m.get_concrete_function(spec)\n",
        "        frozen_func = convert_variables_to_constants_v2(m)\n",
        "        tfm = tf.Module()\n",
        "        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])\n",
        "        tfm.__call__(im)\n",
        "        tf.saved_model.save(tfm,\n",
        "                            f,\n",
        "                            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False) if check_version(\n",
        "                                tf.__version__, '2.6') else tf.saved_model.SaveOptions())\n",
        "    return f, keras_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_pb(keras_model, file, prefix=colorstr('TensorFlow GraphDef:')):\n",
        "    # YOLOv5 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    f = file.with_suffix('.pb')\n",
        "\n",
        "    m = tf.function(lambda x: keras_model(x))  # full model\n",
        "    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n",
        "    frozen_func = convert_variables_to_constants_v2(m)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tflite(keras_model, im, file, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n",
        "    # YOLOv5 TensorFlow Lite export\n",
        "    import tensorflow as tf\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "    f = str(file).replace('.pt', '-fp16.tflite')\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    if int8:\n",
        "        from models.tf import representative_dataset_gen\n",
        "        dataset = LoadImages(check_dataset(check_yaml(data))['train'], img_size=imgsz, auto=False)\n",
        "        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.target_spec.supported_types = []\n",
        "        converter.inference_input_type = tf.uint8  # or tf.int8\n",
        "        converter.inference_output_type = tf.uint8  # or tf.int8\n",
        "        converter.experimental_new_quantizer = True\n",
        "        f = str(file).replace('.pt', '-int8.tflite')\n",
        "    if nms or agnostic_nms:\n",
        "        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "    open(f, \"wb\").write(tflite_model)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_edgetpu(file, prefix=colorstr('Edge TPU:')):\n",
        "    # YOLOv5 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n",
        "    cmd = 'edgetpu_compiler --version'\n",
        "    help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n",
        "    assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n",
        "    if subprocess.run(f'{cmd} >/dev/null', shell=True).returncode != 0:\n",
        "        LOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\n",
        "        sudo = subprocess.run('sudo --version >/dev/null', shell=True).returncode == 0  # sudo installed on system\n",
        "        for c in (\n",
        "                'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n",
        "                'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n",
        "                'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):\n",
        "            subprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\n",
        "    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\n",
        "    f = str(file).replace('.pt', '-int8_edgetpu.tflite')  # Edge TPU model\n",
        "    f_tfl = str(file).replace('.pt', '-int8.tflite')  # TFLite model\n",
        "\n",
        "    cmd = f\"edgetpu_compiler -s -d -k 10 --out_dir {file.parent} {f_tfl}\"\n",
        "    subprocess.run(cmd.split(), check=True)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tfjs(file, prefix=colorstr('TensorFlow.js:')):\n",
        "    # YOLOv5 TensorFlow.js export\n",
        "    check_requirements('tensorflowjs')\n",
        "    import tensorflowjs as tfjs\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\n",
        "    f = str(file).replace('.pt', '_web_model')  # js dir\n",
        "    f_pb = file.with_suffix('.pb')  # *.pb path\n",
        "    f_json = f'{f}/model.json'  # *.json path\n",
        "\n",
        "    cmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n",
        "          f'--output_node_names=Identity,Identity_1,Identity_2,Identity_3 {f_pb} {f}'\n",
        "    subprocess.run(cmd.split())\n",
        "\n",
        "    json = Path(f_json).read_text()\n",
        "    with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n",
        "        subst = re.sub(\n",
        "            r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}', r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n",
        "            r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n",
        "            r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n",
        "            r'\"Identity_3\": {\"name\": \"Identity_3\"}}}', json)\n",
        "        j.write(subst)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "def add_tflite_metadata(file, metadata, num_outputs):\n",
        "    # Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\n",
        "    with contextlib.suppress(ImportError):\n",
        "        # check_requirements('tflite_support')\n",
        "        from tflite_support import flatbuffers\n",
        "        from tflite_support import metadata as _metadata\n",
        "        from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
        "\n",
        "        tmp_file = Path('/tmp/meta.txt')\n",
        "        with open(tmp_file, 'w') as meta_f:\n",
        "            meta_f.write(str(metadata))\n",
        "\n",
        "        model_meta = _metadata_fb.ModelMetadataT()\n",
        "        label_file = _metadata_fb.AssociatedFileT()\n",
        "        label_file.name = tmp_file.name\n",
        "        model_meta.associatedFiles = [label_file]\n",
        "\n",
        "        subgraph = _metadata_fb.SubGraphMetadataT()\n",
        "        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n",
        "        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n",
        "        model_meta.subgraphMetadata = [subgraph]\n",
        "\n",
        "        b = flatbuffers.Builder(0)\n",
        "        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
        "        metadata_buf = b.Output()\n",
        "\n",
        "        populator = _metadata.MetadataPopulator.with_model_file(file)\n",
        "        populator.load_metadata_buffer(metadata_buf)\n",
        "        populator.load_associated_files([str(tmp_file)])\n",
        "        populator.populate()\n",
        "        tmp_file.unlink()\n",
        "\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run(\n",
        "        data=ROOT / 'data/coco128.yaml',  # 'dataset.yaml path'\n",
        "        weights=ROOT / 'yolov5s.pt',  # weights path\n",
        "        imgsz=(640, 640),  # image (height, width)\n",
        "        batch_size=1,  # batch size\n",
        "        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        include=('torchscript', 'onnx'),  # include formats\n",
        "        half=False,  # FP16 half-precision export\n",
        "        inplace=False,  # set YOLOv5 Detect() inplace=True\n",
        "        keras=False,  # use Keras\n",
        "        optimize=False,  # TorchScript: optimize for mobile\n",
        "        int8=False,  # CoreML/TF INT8 quantization\n",
        "        dynamic=False,  # ONNX/TF/TensorRT: dynamic axes\n",
        "        simplify=False,  # ONNX: simplify model\n",
        "        opset=12,  # ONNX: opset version\n",
        "        verbose=False,  # TensorRT: verbose log\n",
        "        workspace=4,  # TensorRT: workspace size (GB)\n",
        "        nms=False,  # TF: add NMS to model\n",
        "        agnostic_nms=False,  # TF: add agnostic NMS to model\n",
        "        topk_per_class=100,  # TF.js NMS: topk per class to keep\n",
        "        topk_all=100,  # TF.js NMS: topk for all classes to keep\n",
        "        iou_thres=0.45,  # TF.js NMS: IoU threshold\n",
        "        conf_thres=0.25,  # TF.js NMS: confidence threshold\n",
        "):\n",
        "    t = time.time()\n",
        "    include = [x.lower() for x in include]  # to lowercase\n",
        "    fmts = tuple(export_formats()['Argument'][1:])  # --include arguments\n",
        "    flags = [x in include for x in fmts]\n",
        "    assert sum(flags) == len(include), f'ERROR: Invalid --include {include}, valid --include arguments are {fmts}'\n",
        "    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n",
        "    file = Path(url2file(weights) if str(weights).startswith(('http:/', 'https:/')) else weights)  # PyTorch weights\n",
        "\n",
        "    # Load PyTorch model\n",
        "    device = select_device(device)\n",
        "    if half:\n",
        "        assert device.type != 'cpu' or coreml, '--half only compatible with GPU export, i.e. use --device 0'\n",
        "        assert not dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both'\n",
        "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
        "\n",
        "    nc, names = model.nc, model.names  # number of classes, class names\n",
        "\n",
        "    # Checks\n",
        "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
        "    if optimize:\n",
        "        assert device.type == 'cpu', '--optimize not compatible with cuda devices, i.e. use --device cpu'\n",
        "\n",
        "    # Input\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n",
        "    im = torch.zeros(batch_size, 3, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n",
        "\n",
        "    # Update model\n",
        "    model.eval()\n",
        "    for k, m in model.named_modules():\n",
        "        if isinstance(m, Detect):\n",
        "            m.inplace = inplace\n",
        "            m.dynamic = dynamic\n",
        "            m.export = True\n",
        "\n",
        "    for _ in range(2):\n",
        "        y = model(im)  # dry runs\n",
        "    if half and not coreml:\n",
        "        im, model = im.half(), model.half()  # to FP16\n",
        "    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n",
        "\n",
        "    metadata = {'stride': int(max(model.stride)), 'names': model.names}  # model metadata\n",
        "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)\")\n",
        "\n",
        "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} model.stride =  {model.stride}  model.names =  {model.names}\")\n",
        "\n",
        "    # Exports\n",
        "    f = [''] * len(fmts)  # exported filenames\n",
        "    warnings.filterwarnings(action='ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\n",
        "    if jit:  # TorchScript\n",
        "        f[0], _ = export_torchscript(model, im, file, optimize)\n",
        "    if engine:  # TensorRT required before ONNX\n",
        "        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose)\n",
        "    if onnx or xml:  # OpenVINO requires ONNX\n",
        "        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)\n",
        "    if xml:  # OpenVINO\n",
        "        f[3], _ = export_openvino(file, metadata, half)\n",
        "    if coreml:  # CoreML\n",
        "        # f[4], _ = export_coreml(model, im, file, int8, half)\n",
        "        ###\n",
        "        nb = shape[1]\n",
        "        f[4], _ = export_coreml(model, im, file, int8, half, nb, nc, names, conf_thres, iou_thres)\n",
        "        ###\n",
        "\n",
        "    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n",
        "        assert not tflite or not tfjs, 'TFLite and TF.js models must be exported separately, please pass only one type.'\n",
        "        assert not isinstance(model, ClassificationModel), 'ClassificationModel export to TF formats not yet supported.'\n",
        "        f[5], s_model = export_saved_model(model.cpu(),\n",
        "                                           im,\n",
        "                                           file,\n",
        "                                           dynamic,\n",
        "                                           tf_nms=nms or agnostic_nms or tfjs,\n",
        "                                           agnostic_nms=agnostic_nms or tfjs,\n",
        "                                           topk_per_class=topk_per_class,\n",
        "                                           topk_all=topk_all,\n",
        "                                           iou_thres=iou_thres,\n",
        "                                           conf_thres=conf_thres,\n",
        "                                           keras=keras)\n",
        "        if pb or tfjs:  # pb prerequisite to tfjs\n",
        "            f[6], _ = export_pb(s_model, file)\n",
        "        if tflite or edgetpu:\n",
        "            f[7], _ = export_tflite(s_model, im, file, int8 or edgetpu, data=data, nms=nms, agnostic_nms=agnostic_nms)\n",
        "            if edgetpu:\n",
        "                f[8], _ = export_edgetpu(file)\n",
        "            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))\n",
        "        if tfjs:\n",
        "            f[9], _ = export_tfjs(file)\n",
        "    if paddle:  # PaddlePaddle\n",
        "        f[10], _ = export_paddle(model, im, file, metadata)\n",
        "\n",
        "    # Finish\n",
        "    f = [str(x) for x in f if x]  # filter out '' and None\n",
        "    if any(f):\n",
        "        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # type\n",
        "        dir = Path('segment' if seg else 'classify' if cls else '')\n",
        "        h = '--half' if half else ''  # --half FP16 inference arg\n",
        "        s = \"# WARNING ‚ö†Ô∏è ClassificationModel not yet supported for PyTorch Hub AutoShape inference\" if cls else \\\n",
        "            \"# WARNING ‚ö†Ô∏è SegmentationModel not yet supported for PyTorch Hub AutoShape inference\" if seg else ''\n",
        "        LOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\n",
        "                    f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n",
        "                    f\"\\nDetect:          python {dir / ('detect.py' if det else 'predict.py')} --weights {f[-1]} {h}\"\n",
        "                    f\"\\nValidate:        python {dir / 'val.py'} --weights {f[-1]} {h}\"\n",
        "                    f\"\\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')  {s}\"\n",
        "                    f\"\\nVisualize:       https://netron.app\")\n",
        "    return f  # return list of exported files/dirs\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n",
        "    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')\n",
        "    parser.add_argument('--keras', action='store_true', help='TF: use Keras')\n",
        "    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')\n",
        "    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')\n",
        "    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF/TensorRT: dynamic axes')\n",
        "    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')\n",
        "    parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')\n",
        "    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')\n",
        "    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')\n",
        "    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')\n",
        "    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')\n",
        "    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')\n",
        "    parser.add_argument(\n",
        "        '--include',\n",
        "        nargs='+',\n",
        "        default=['torchscript'],\n",
        "        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')\n",
        "    opt = parser.parse_args()\n",
        "    print_args(vars(opt))\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n",
        "        run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)"
      ],
      "metadata": {
        "id": "f2iuyPV0SPdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 640\n",
        "weight_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "\n",
        "%cd yolov5\n",
        "!python export-coreml-nms.py --img-size $img_size --weights $weight_path --include \"coreml\"\n",
        "\n"
      ],
      "metadata": {
        "id": "QjbP6vV9Nck5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}