{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLeCwcvI2U6eTi18otYzGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Export%20YOLOv5%20into%20CoreML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Export YOLOv5 into CoreML**\n",
        "\n",
        "https://github.com/junmcenroe/YOLOv5-CoreML-Export-with-NMS/blob/main/export-coreml-nms.py\n",
        "\n",
        "正方形の画像でしか正しい判定が出ない。\n",
        "\n",
        "長方形の画像を入力するときには、letterbox加工をしてから入力すればうまくいく\n",
        "\n",
        "640pxで設定しているが、正方形であればサイズは大きくても小さくても良さそう"
      ],
      "metadata": {
        "id": "FFvTNR0mQBMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5PbxXfcDzUs",
        "outputId": "8b8ebf8a-e561-45ef-fb70-bfb39f82c10c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "import shutil\n",
        "!pip install coremltools==6.2 --q\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "LTYPIiE7D1sO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce43840a-9a7d-4e7a-e0a1-44db904a7c3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m588.4 kB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/content\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16057, done.\u001b[K\n",
            "remote: Total 16057 (delta 0), reused 0 (delta 0), pack-reused 16057\u001b[K\n",
            "Receiving objects: 100% (16057/16057), 14.60 MiB | 16.18 MiB/s, done.\n",
            "Resolving deltas: 100% (11032/11032), done.\n",
            "/content/yolov5\n",
            "Collecting gitpython>=3.1.30 (from -r requirements.txt (line 5))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.8.0.76)\n",
            "Collecting Pillow>=10.0.1 (from -r requirements.txt (line 9))\n",
            "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.11.3)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.1)\n",
            "Collecting ultralytics>=8.0.147 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.0.210-py3-none-any.whl (645 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.3/645.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.12.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (67.7.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r requirements.txt (line 5))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.147->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.3.post1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Installing collected packages: smmap, Pillow, gitdb, thop, gitpython, ultralytics\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.1.0 gitdb-4.0.11 gitpython-3.1.40 smmap-5.0.1 thop-0.1.1.post2209072238 ultralytics-8.0.210\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(weight_path, \"/content/test.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pRphJf_IZ9rH",
        "outputId": "f9cf79db-7a11-47ce-bc8d-7ea423907f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/test.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip install coremltools==6.2\n",
        "!git clone https://github.com/junmcenroe/YOLOv5-CoreML-Export-with-NMS.git\n",
        "shutil.copy(\"/content/YOLOv5-CoreML-Export-with-NMS/export-coreml-nms.py\", \"/content/yolov5/export-coreml-nms.py\")\n",
        "\n",
        "###↓↓ git cloneがうまくいかなくなった時の予備"
      ],
      "metadata": {
        "id": "nWLnY5QIJV5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 640\n",
        "# weight_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "\n",
        "%cd /content/yolov5\n",
        "!python export-coreml-nms.py --img-size $img_size --weights /content/test.pt --include \"coreml\"\n",
        "\n"
      ],
      "metadata": {
        "id": "QjbP6vV9Nck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**↓↓ git cloneがうまくいかなくなった時の予備**\n",
        "\n",
        "長方形でもうまく座標を作るようにしたかったがうまくいかなかった..."
      ],
      "metadata": {
        "id": "hC9lge4tlw_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/yolov5/export-coreml-nms.py\n",
        "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit\n",
        "\n",
        "Format                      | `export.py --include`         | Model\n",
        "---                         | ---                           | ---\n",
        "PyTorch                     | -                             | yolov5s.pt\n",
        "TorchScript                 | `torchscript`                 | yolov5s.torchscript\n",
        "ONNX                        | `onnx`                        | yolov5s.onnx\n",
        "OpenVINO                    | `openvino`                    | yolov5s_openvino_model/\n",
        "TensorRT                    | `engine`                      | yolov5s.engine\n",
        "CoreML                      | `coreml`                      | yolov5s.mlmodel\n",
        "TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\n",
        "TensorFlow GraphDef         | `pb`                          | yolov5s.pb\n",
        "TensorFlow Lite             | `tflite`                      | yolov5s.tflite\n",
        "TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\n",
        "TensorFlow.js               | `tfjs`                        | yolov5s_web_model/\n",
        "PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/\n",
        "\n",
        "Requirements:\n",
        "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU\n",
        "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU\n",
        "\n",
        "Usage:\n",
        "    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...\n",
        "\n",
        "Inference:\n",
        "    $ python detect.py --weights yolov5s.pt                 # PyTorch\n",
        "                                 yolov5s.torchscript        # TorchScript\n",
        "                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
        "                                 yolov5s_openvino_model     # OpenVINO\n",
        "                                 yolov5s.engine             # TensorRT\n",
        "                                 yolov5s.mlmodel            # CoreML (macOS-only)\n",
        "                                 yolov5s_saved_model        # TensorFlow SavedModel\n",
        "                                 yolov5s.pb                 # TensorFlow GraphDef\n",
        "                                 yolov5s.tflite             # TensorFlow Lite\n",
        "                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
        "                                 yolov5s_paddle_model       # PaddlePaddle\n",
        "\n",
        "TensorFlow.js:\n",
        "    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example\n",
        "    $ npm install\n",
        "    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model\n",
        "    $ npm start\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import contextlib\n",
        "import json\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "if platform.system() != 'Windows':\n",
        "    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\n",
        "from utils.dataloaders import LoadImages\n",
        "from utils.general import (LOGGER, Profile, check_dataset, check_img_size, check_requirements, check_version,\n",
        "                           check_yaml, colorstr, file_size, get_default_args, print_args, url2file, yaml_save)\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "MACOS = platform.system() == 'Darwin'  # macOS environment\n",
        "\n",
        "\n",
        "def export_formats():\n",
        "    # YOLOv5 export formats\n",
        "    x = [\n",
        "        ['PyTorch', '-', '.pt', True, True],\n",
        "        ['TorchScript', 'torchscript', '.torchscript', True, True],\n",
        "        ['ONNX', 'onnx', '.onnx', True, True],\n",
        "        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n",
        "        ['TensorRT', 'engine', '.engine', False, True],\n",
        "        ['CoreML', 'coreml', '.mlmodel', True, False],\n",
        "        ['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],\n",
        "        ['TensorFlow GraphDef', 'pb', '.pb', True, True],\n",
        "        ['TensorFlow Lite', 'tflite', '.tflite', True, False],\n",
        "        ['TensorFlow Edge TPU', 'edgetpu', '_edgetpu.tflite', False, False],\n",
        "        ['TensorFlow.js', 'tfjs', '_web_model', False, False],\n",
        "        ['PaddlePaddle', 'paddle', '_paddle_model', True, True],]\n",
        "    return pd.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n",
        "\n",
        "\n",
        "def try_export(inner_func):\n",
        "    # YOLOv5 export decorator, i..e @try_export\n",
        "    inner_args = get_default_args(inner_func)\n",
        "\n",
        "    def outer_func(*args, **kwargs):\n",
        "        prefix = inner_args['prefix']\n",
        "        try:\n",
        "            with Profile() as dt:\n",
        "                f, model = inner_func(*args, **kwargs)\n",
        "            LOGGER.info(f'{prefix} export success ✅ {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')\n",
        "            return f, model\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'{prefix} export failure ❌ {dt.t:.1f}s: {e}')\n",
        "            return None, None\n",
        "\n",
        "    return outer_func\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n",
        "    # YOLOv5 TorchScript model export\n",
        "    LOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n",
        "    f = file.with_suffix('.torchscript')\n",
        "\n",
        "    ts = torch.jit.trace(model, im, strict=False)\n",
        "    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n",
        "    extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n",
        "    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n",
        "        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n",
        "    else:\n",
        "        ts.save(str(f), _extra_files=extra_files)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n",
        "    # YOLOv5 ONNX export\n",
        "    check_requirements('onnx')\n",
        "    import onnx\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\n",
        "    f = file.with_suffix('.onnx')\n",
        "\n",
        "    output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']\n",
        "    if dynamic:\n",
        "        dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\n",
        "        if isinstance(model, SegmentationModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "            dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\n",
        "        elif isinstance(model, DetectionModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n",
        "        im.cpu() if dynamic else im,\n",
        "        f,\n",
        "        verbose=False,\n",
        "        opset_version=opset,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['images'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes=dynamic or None)\n",
        "\n",
        "    # Checks\n",
        "    model_onnx = onnx.load(f)  # load onnx model\n",
        "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
        "\n",
        "    # Metadata\n",
        "    d = {'stride': int(max(model.stride)), 'names': model.names}\n",
        "    for k, v in d.items():\n",
        "        meta = model_onnx.metadata_props.add()\n",
        "        meta.key, meta.value = k, str(v)\n",
        "    onnx.save(model_onnx, f)\n",
        "\n",
        "    # Simplify\n",
        "    if simplify:\n",
        "        try:\n",
        "            cuda = torch.cuda.is_available()\n",
        "            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))\n",
        "            import onnxsim\n",
        "\n",
        "            LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')\n",
        "            model_onnx, check = onnxsim.simplify(model_onnx)\n",
        "            assert check, 'assert check failed'\n",
        "            onnx.save(model_onnx, f)\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'{prefix} simplifier failure: {e}')\n",
        "    return f, model_onnx\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_openvino(file, metadata, half, prefix=colorstr('OpenVINO:')):\n",
        "    # YOLOv5 OpenVINO export\n",
        "    check_requirements('openvino-dev')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n",
        "    import openvino.inference_engine as ie\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with openvino {ie.__version__}...')\n",
        "    f = str(file).replace('.pt', f'_openvino_model{os.sep}')\n",
        "\n",
        "    cmd = f\"mo --input_model {file.with_suffix('.onnx')} --output_dir {f} --data_type {'FP16' if half else 'FP32'}\"\n",
        "    subprocess.run(cmd.split(), check=True, env=os.environ)  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_paddle(model, im, file, metadata, prefix=colorstr('PaddlePaddle:')):\n",
        "    # YOLOv5 Paddle export\n",
        "    check_requirements(('paddlepaddle', 'x2paddle'))\n",
        "    import x2paddle\n",
        "    from x2paddle.convert import pytorch2paddle\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n",
        "    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n",
        "\n",
        "    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[im])  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "# @try_export\n",
        "# def export_coreml(model, im, file, int8, half, prefix=colorstr('CoreML:')):\n",
        "#     # YOLOv5 CoreML export\n",
        "#     check_requirements('coremltools')\n",
        "#     import coremltools as ct\n",
        "\n",
        "#     LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
        "#     f = file.with_suffix('.mlmodel')\n",
        "\n",
        "#     ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n",
        "#     ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "#     bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "#     if bits < 32:\n",
        "#         if MACOS:  # quantization only supported on macOS\n",
        "#             with warnings.catch_warnings():\n",
        "#                 warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "#                 ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
        "#         else:\n",
        "#             print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "#     ct_model.save(f)\n",
        "#     return f, ct_model\n",
        "\n",
        "class CoreMLExportModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, img_size):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)[0]\n",
        "        x = x.squeeze(0)\n",
        "        # Convert box coords to normalized coordinates [0 ... 1]\n",
        "        w = self.img_size[0]\n",
        "        h = self.img_size[1]\n",
        "        objectness = x[:, 4:5]\n",
        "        class_probs = x[:, 5:] * objectness\n",
        "        # バウンディングボックスの座標を取得\n",
        "        x1, y1, x2, y2 = x[:, 0], x[:, 1], x[:, 2], x[:, 3]\n",
        "\n",
        "        # パディングの計算\n",
        "        # ここではimg_widthとimg_heightが定義されていると仮定します。\n",
        "        padding_x = (h - min(w, h)) / 2\n",
        "        padding_y = (w - min(w, h)) / 2\n",
        "\n",
        "        # バウンディングボックスの座標を調整し、正規化\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "\n",
        "        x1 = x1*max(w,h)/(640*w)\n",
        "        y1 = y1*max(w,h)/(640*h)\n",
        "        x2 = x2*max(w,h)/(640*w)\n",
        "        y2 = y2*max(w,h)/(640*h)\n",
        "\n",
        "        # 新しいバウンディングボックスのテンソルを作成\n",
        "        boxes = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "        return class_probs, boxes\n",
        "\n",
        "@try_export\n",
        "#def export_coreml(model, im, file, num_boxes, num_classes, labels, conf_thres, iou_thres, prefix=colorstr('CoreML:')):\n",
        "def export_coreml(model, im, file, int8, half, num_boxes, num_classes, labels, conf_thres, iou_thres, prefix=colorstr('CoreML:')):\n",
        "    # YOLOv5 CoreML export\n",
        "    check_requirements(('coremltools',))\n",
        "    import coremltools as ct\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
        "    f = file.with_suffix('.mlmodel')\n",
        "\n",
        "    export_model = CoreMLExportModel(model, img_size=opt.imgsz)\n",
        "\n",
        "    ts = torch.jit.trace(export_model.eval(), im, strict=False)  # TorchScript model\n",
        "\n",
        "    orig_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "\n",
        "    # quantize\n",
        "    bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "    if bits < 32:\n",
        "        if MACOS:  # quantization only supported on macOS\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "                orig_model = ct.models.neural_network.quantization_utils.quantize_weights(orig_model, bits, mode)\n",
        "        else:\n",
        "            print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "\n",
        "    spec = orig_model.get_spec()\n",
        "    old_box_output_name = spec.description.output[1].name\n",
        "    old_scores_output_name = spec.description.output[0].name\n",
        "    ct.utils.rename_feature(spec, old_scores_output_name, \"raw_confidence\")\n",
        "    ct.utils.rename_feature(spec, old_box_output_name, \"raw_coordinates\")\n",
        "    spec.description.output[0].type.multiArrayType.shape.extend([num_boxes, num_classes])\n",
        "    spec.description.output[1].type.multiArrayType.shape.extend([num_boxes, 4])\n",
        "    spec.description.output[0].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "    spec.description.output[1].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "\n",
        "    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\n",
        "\n",
        "    # Model from spec\n",
        "    yolo_model = ct.models.MLModel(spec)\n",
        "\n",
        "    # 3. Create NMS protobuf\n",
        "    # Build Non Maximum Suppression model\n",
        "    nms_spec = ct.proto.Model_pb2.Model()\n",
        "    # nms_spec.specificationVersion = 3\n",
        "    nms_spec.specificationVersion = 5\n",
        "\n",
        "    for i in range(2):\n",
        "        decoder_output = spec.description.output[i].SerializeToString()\n",
        "        nms_spec.description.input.add()\n",
        "        nms_spec.description.input[i].ParseFromString(decoder_output)\n",
        "        nms_spec.description.output.add()\n",
        "        nms_spec.description.output[i].ParseFromString(decoder_output)\n",
        "\n",
        "    nms_spec.description.output[0].name = \"confidence\"\n",
        "    nms_spec.description.output[1].name = \"coordinates\"\n",
        "\n",
        "    output_sizes = [num_classes, 4]\n",
        "    for i in range(2):\n",
        "        ma_type = nms_spec.description.output[i].type.multiArrayType\n",
        "        ma_type.shapeRange.sizeRanges.add()\n",
        "        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n",
        "        ma_type.shapeRange.sizeRanges[0].upperBound = -1\n",
        "        ma_type.shapeRange.sizeRanges.add()\n",
        "        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n",
        "        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n",
        "        del ma_type.shape[:]\n",
        "\n",
        "    nms = nms_spec.nonMaximumSuppression\n",
        "    nms.confidenceInputFeatureName = \"raw_confidence\"\n",
        "    nms.coordinatesInputFeatureName = \"raw_coordinates\"\n",
        "    nms.confidenceOutputFeatureName = \"confidence\"\n",
        "    nms.coordinatesOutputFeatureName = \"coordinates\"\n",
        "    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n",
        "    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n",
        "    nms.iouThreshold = iou_thres\n",
        "    nms.confidenceThreshold = conf_thres\n",
        "    # nms.pickTop.perClass = False\n",
        "    nms.pickTop.perClass = True\n",
        "    nms.stringClassLabels.vector.extend(labels.values())\n",
        "    nms_model = ct.models.MLModel(nms_spec)\n",
        "\n",
        "    # 4. Pipeline models together\n",
        "    # Assembling a pipeline model from the two models\n",
        "    # input_features = [(\"image\", ct.models.datatypes.Array(3, 300, 300)),\n",
        "    input_features = [(\"image\", ct.models.datatypes.Array(3, ny, nx)),\n",
        "                        (\"iouThreshold\", ct.models.datatypes.Double()),\n",
        "                        (\"confidenceThreshold\", ct.models.datatypes.Double())]\n",
        "    output_features = [\"confidence\", \"coordinates\"]\n",
        "    pipeline = ct.models.pipeline.Pipeline(input_features, output_features)\n",
        "    pipeline.add_model(yolo_model)\n",
        "    pipeline.add_model(nms_model)\n",
        "\n",
        "    # Correct datatypes\n",
        "    # The \"image\" input should really be an image, not a multi-array\n",
        "    pipeline.spec.description.input[0].ParseFromString(spec.description.input[0].SerializeToString())\n",
        "    # Copy the declarations of the \"confidence\" and \"coordinates\" outputs\n",
        "    # The Pipeline makes these strings by default\n",
        "    pipeline.spec.description.output[0].ParseFromString(nms_spec.description.output[0].SerializeToString())\n",
        "    pipeline.spec.description.output[1].ParseFromString(nms_spec.description.output[1].SerializeToString())\n",
        "\n",
        "    # Add descriptions to the inputs and outputs\n",
        "    pipeline.spec.description.input[1].shortDescription = \"(optional) IOU Threshold override\"\n",
        "    pipeline.spec.description.input[2].shortDescription = \"(optional) Confidence Threshold override\"\n",
        "    pipeline.spec.description.output[0].shortDescription = \"Boxes Class confidence\"\n",
        "    pipeline.spec.description.output[1].shortDescription = \"Boxes [x, y, width, height] (normalized to [0...1])\"\n",
        "\n",
        "    # Add metadata to the model\n",
        "    pipeline.spec.description.metadata.shortDescription = \"YOLOv5 object detector\"\n",
        "    pipeline.spec.description.metadata.author = \"Ultralytics\"\n",
        "\n",
        "    # Add the default threshold values and list of class labels\n",
        "    user_defined_metadata = {\n",
        "        \"iou_threshold\": str(iou_thres),\n",
        "        \"confidence_threshold\": str(conf_thres),\n",
        "        \"classes\": \", \".join(labels.values())}\n",
        "    pipeline.spec.description.metadata.userDefined.update(user_defined_metadata)\n",
        "\n",
        "    # Don't forget this or Core ML might attempt to run the model on an unsupported operating system version!\n",
        "    pipeline.spec.specificationVersion = 5\n",
        "\n",
        "    ct_model = ct.models.MLModel(pipeline.spec)\n",
        "\n",
        "    f = str(file).replace('.pt', '.mlmodel')\n",
        "    ct_model.save(f)\n",
        "\n",
        "    LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
        "    return  f, ct_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n",
        "    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n",
        "    assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n",
        "    try:\n",
        "        import tensorrt as trt\n",
        "    except Exception:\n",
        "        if platform.system() == 'Linux':\n",
        "            check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\n",
        "        import tensorrt as trt\n",
        "\n",
        "    if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n",
        "        grid = model.model[-1].anchor_grid\n",
        "        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "        model.model[-1].anchor_grid = grid\n",
        "    else:  # TensorRT >= 8\n",
        "        check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt>=8.0.0\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "    onnx = file.with_suffix('.onnx')\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\n",
        "    assert onnx.exists(), f'failed to export ONNX file: {onnx}'\n",
        "    f = file.with_suffix('.engine')  # TensorRT engine file\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    if verbose:\n",
        "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
        "\n",
        "    builder = trt.Builder(logger)\n",
        "    config = builder.create_builder_config()\n",
        "    config.max_workspace_size = workspace * 1 << 30\n",
        "    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)  # fix TRT 8.4 deprecation notice\n",
        "\n",
        "    flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "    network = builder.create_network(flag)\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "    if not parser.parse_from_file(str(onnx)):\n",
        "        raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
        "\n",
        "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "    for inp in inputs:\n",
        "        LOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n",
        "    for out in outputs:\n",
        "        LOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n",
        "\n",
        "    if dynamic:\n",
        "        if im.shape[0] <= 1:\n",
        "            LOGGER.warning(f\"{prefix} WARNING ⚠️ --dynamic model requires maximum --batch-size argument\")\n",
        "        profile = builder.create_optimization_profile()\n",
        "        for inp in inputs:\n",
        "            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)\n",
        "        config.add_optimization_profile(profile)\n",
        "\n",
        "    LOGGER.info(f'{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}')\n",
        "    if builder.platform_has_fast_fp16 and half:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "    with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n",
        "        t.write(engine.serialize())\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_saved_model(model,\n",
        "                       im,\n",
        "                       file,\n",
        "                       dynamic,\n",
        "                       tf_nms=False,\n",
        "                       agnostic_nms=False,\n",
        "                       topk_per_class=100,\n",
        "                       topk_all=100,\n",
        "                       iou_thres=0.45,\n",
        "                       conf_thres=0.25,\n",
        "                       keras=False,\n",
        "                       prefix=colorstr('TensorFlow SavedModel:')):\n",
        "    # YOLOv5 TensorFlow SavedModel export\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except Exception:\n",
        "        check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\n",
        "        import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    from models.tf import TFModel\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    f = str(file).replace('.pt', '_saved_model')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "\n",
        "    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n",
        "    im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n",
        "    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)\n",
        "    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    keras_model.trainable = False\n",
        "    keras_model.summary()\n",
        "    if keras:\n",
        "        keras_model.save(f, save_format='tf')\n",
        "    else:\n",
        "        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n",
        "        m = tf.function(lambda x: keras_model(x))  # full model\n",
        "        m = m.get_concrete_function(spec)\n",
        "        frozen_func = convert_variables_to_constants_v2(m)\n",
        "        tfm = tf.Module()\n",
        "        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])\n",
        "        tfm.__call__(im)\n",
        "        tf.saved_model.save(tfm,\n",
        "                            f,\n",
        "                            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False) if check_version(\n",
        "                                tf.__version__, '2.6') else tf.saved_model.SaveOptions())\n",
        "    return f, keras_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_pb(keras_model, file, prefix=colorstr('TensorFlow GraphDef:')):\n",
        "    # YOLOv5 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    f = file.with_suffix('.pb')\n",
        "\n",
        "    m = tf.function(lambda x: keras_model(x))  # full model\n",
        "    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n",
        "    frozen_func = convert_variables_to_constants_v2(m)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tflite(keras_model, im, file, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n",
        "    # YOLOv5 TensorFlow Lite export\n",
        "    import tensorflow as tf\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "    f = str(file).replace('.pt', '-fp16.tflite')\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    if int8:\n",
        "        from models.tf import representative_dataset_gen\n",
        "        dataset = LoadImages(check_dataset(check_yaml(data))['train'], img_size=imgsz, auto=False)\n",
        "        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.target_spec.supported_types = []\n",
        "        converter.inference_input_type = tf.uint8  # or tf.int8\n",
        "        converter.inference_output_type = tf.uint8  # or tf.int8\n",
        "        converter.experimental_new_quantizer = True\n",
        "        f = str(file).replace('.pt', '-int8.tflite')\n",
        "    if nms or agnostic_nms:\n",
        "        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "    open(f, \"wb\").write(tflite_model)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_edgetpu(file, prefix=colorstr('Edge TPU:')):\n",
        "    # YOLOv5 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n",
        "    cmd = 'edgetpu_compiler --version'\n",
        "    help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n",
        "    assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n",
        "    if subprocess.run(f'{cmd} >/dev/null', shell=True).returncode != 0:\n",
        "        LOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\n",
        "        sudo = subprocess.run('sudo --version >/dev/null', shell=True).returncode == 0  # sudo installed on system\n",
        "        for c in (\n",
        "                'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n",
        "                'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n",
        "                'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):\n",
        "            subprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\n",
        "    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\n",
        "    f = str(file).replace('.pt', '-int8_edgetpu.tflite')  # Edge TPU model\n",
        "    f_tfl = str(file).replace('.pt', '-int8.tflite')  # TFLite model\n",
        "\n",
        "    cmd = f\"edgetpu_compiler -s -d -k 10 --out_dir {file.parent} {f_tfl}\"\n",
        "    subprocess.run(cmd.split(), check=True)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tfjs(file, prefix=colorstr('TensorFlow.js:')):\n",
        "    # YOLOv5 TensorFlow.js export\n",
        "    check_requirements('tensorflowjs')\n",
        "    import tensorflowjs as tfjs\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\n",
        "    f = str(file).replace('.pt', '_web_model')  # js dir\n",
        "    f_pb = file.with_suffix('.pb')  # *.pb path\n",
        "    f_json = f'{f}/model.json'  # *.json path\n",
        "\n",
        "    cmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n",
        "          f'--output_node_names=Identity,Identity_1,Identity_2,Identity_3 {f_pb} {f}'\n",
        "    subprocess.run(cmd.split())\n",
        "\n",
        "    json = Path(f_json).read_text()\n",
        "    with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n",
        "        subst = re.sub(\n",
        "            r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}', r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n",
        "            r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n",
        "            r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n",
        "            r'\"Identity_3\": {\"name\": \"Identity_3\"}}}', json)\n",
        "        j.write(subst)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "def add_tflite_metadata(file, metadata, num_outputs):\n",
        "    # Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\n",
        "    with contextlib.suppress(ImportError):\n",
        "        # check_requirements('tflite_support')\n",
        "        from tflite_support import flatbuffers\n",
        "        from tflite_support import metadata as _metadata\n",
        "        from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
        "\n",
        "        tmp_file = Path('/tmp/meta.txt')\n",
        "        with open(tmp_file, 'w') as meta_f:\n",
        "            meta_f.write(str(metadata))\n",
        "\n",
        "        model_meta = _metadata_fb.ModelMetadataT()\n",
        "        label_file = _metadata_fb.AssociatedFileT()\n",
        "        label_file.name = tmp_file.name\n",
        "        model_meta.associatedFiles = [label_file]\n",
        "\n",
        "        subgraph = _metadata_fb.SubGraphMetadataT()\n",
        "        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n",
        "        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n",
        "        model_meta.subgraphMetadata = [subgraph]\n",
        "\n",
        "        b = flatbuffers.Builder(0)\n",
        "        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
        "        metadata_buf = b.Output()\n",
        "\n",
        "        populator = _metadata.MetadataPopulator.with_model_file(file)\n",
        "        populator.load_metadata_buffer(metadata_buf)\n",
        "        populator.load_associated_files([str(tmp_file)])\n",
        "        populator.populate()\n",
        "        tmp_file.unlink()\n",
        "\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run(\n",
        "        data=ROOT / 'data/coco128.yaml',  # 'dataset.yaml path'\n",
        "        weights=ROOT / 'yolov5s.pt',  # weights path\n",
        "        imgsz=(640, 640),  # image (height, width)\n",
        "        batch_size=1,  # batch size\n",
        "        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        include=('torchscript', 'onnx'),  # include formats\n",
        "        half=False,  # FP16 half-precision export\n",
        "        inplace=False,  # set YOLOv5 Detect() inplace=True\n",
        "        keras=False,  # use Keras\n",
        "        optimize=False,  # TorchScript: optimize for mobile\n",
        "        int8=False,  # CoreML/TF INT8 quantization\n",
        "        dynamic=False,  # ONNX/TF/TensorRT: dynamic axes\n",
        "        simplify=False,  # ONNX: simplify model\n",
        "        opset=12,  # ONNX: opset version\n",
        "        verbose=False,  # TensorRT: verbose log\n",
        "        workspace=4,  # TensorRT: workspace size (GB)\n",
        "        nms=False,  # TF: add NMS to model\n",
        "        agnostic_nms=False,  # TF: add agnostic NMS to model\n",
        "        topk_per_class=100,  # TF.js NMS: topk per class to keep\n",
        "        topk_all=100,  # TF.js NMS: topk for all classes to keep\n",
        "        iou_thres=0.45,  # TF.js NMS: IoU threshold\n",
        "        conf_thres=0.25,  # TF.js NMS: confidence threshold\n",
        "):\n",
        "    t = time.time()\n",
        "    include = [x.lower() for x in include]  # to lowercase\n",
        "    fmts = tuple(export_formats()['Argument'][1:])  # --include arguments\n",
        "    flags = [x in include for x in fmts]\n",
        "    assert sum(flags) == len(include), f'ERROR: Invalid --include {include}, valid --include arguments are {fmts}'\n",
        "    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n",
        "    file = Path(url2file(weights) if str(weights).startswith(('http:/', 'https:/')) else weights)  # PyTorch weights\n",
        "\n",
        "    # Load PyTorch model\n",
        "    device = select_device(device)\n",
        "    if half:\n",
        "        assert device.type != 'cpu' or coreml, '--half only compatible with GPU export, i.e. use --device 0'\n",
        "        assert not dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both'\n",
        "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
        "\n",
        "    nc, names = model.nc, model.names  # number of classes, class names\n",
        "\n",
        "    # Checks\n",
        "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
        "    if optimize:\n",
        "        assert device.type == 'cpu', '--optimize not compatible with cuda devices, i.e. use --device cpu'\n",
        "\n",
        "    # Input\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n",
        "    im = torch.zeros(batch_size, 3, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n",
        "\n",
        "    # Update model\n",
        "    model.eval()\n",
        "    for k, m in model.named_modules():\n",
        "        if isinstance(m, Detect):\n",
        "            m.inplace = inplace\n",
        "            m.dynamic = dynamic\n",
        "            m.export = True\n",
        "\n",
        "    for _ in range(2):\n",
        "        y = model(im)  # dry runs\n",
        "    if half and not coreml:\n",
        "        im, model = im.half(), model.half()  # to FP16\n",
        "    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n",
        "\n",
        "    metadata = {'stride': int(max(model.stride)), 'names': model.names}  # model metadata\n",
        "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)\")\n",
        "\n",
        "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} model.stride =  {model.stride}  model.names =  {model.names}\")\n",
        "\n",
        "    # Exports\n",
        "    f = [''] * len(fmts)  # exported filenames\n",
        "    warnings.filterwarnings(action='ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\n",
        "    if jit:  # TorchScript\n",
        "        f[0], _ = export_torchscript(model, im, file, optimize)\n",
        "    if engine:  # TensorRT required before ONNX\n",
        "        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose)\n",
        "    if onnx or xml:  # OpenVINO requires ONNX\n",
        "        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)\n",
        "    if xml:  # OpenVINO\n",
        "        f[3], _ = export_openvino(file, metadata, half)\n",
        "    if coreml:  # CoreML\n",
        "        # f[4], _ = export_coreml(model, im, file, int8, half)\n",
        "        ###\n",
        "        nb = shape[1]\n",
        "        f[4], _ = export_coreml(model, im, file, int8, half, nb, nc, names, conf_thres, iou_thres)\n",
        "        ###\n",
        "\n",
        "    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n",
        "        assert not tflite or not tfjs, 'TFLite and TF.js models must be exported separately, please pass only one type.'\n",
        "        assert not isinstance(model, ClassificationModel), 'ClassificationModel export to TF formats not yet supported.'\n",
        "        f[5], s_model = export_saved_model(model.cpu(),\n",
        "                                           im,\n",
        "                                           file,\n",
        "                                           dynamic,\n",
        "                                           tf_nms=nms or agnostic_nms or tfjs,\n",
        "                                           agnostic_nms=agnostic_nms or tfjs,\n",
        "                                           topk_per_class=topk_per_class,\n",
        "                                           topk_all=topk_all,\n",
        "                                           iou_thres=iou_thres,\n",
        "                                           conf_thres=conf_thres,\n",
        "                                           keras=keras)\n",
        "        if pb or tfjs:  # pb prerequisite to tfjs\n",
        "            f[6], _ = export_pb(s_model, file)\n",
        "        if tflite or edgetpu:\n",
        "            f[7], _ = export_tflite(s_model, im, file, int8 or edgetpu, data=data, nms=nms, agnostic_nms=agnostic_nms)\n",
        "            if edgetpu:\n",
        "                f[8], _ = export_edgetpu(file)\n",
        "            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))\n",
        "        if tfjs:\n",
        "            f[9], _ = export_tfjs(file)\n",
        "    if paddle:  # PaddlePaddle\n",
        "        f[10], _ = export_paddle(model, im, file, metadata)\n",
        "\n",
        "    # Finish\n",
        "    f = [str(x) for x in f if x]  # filter out '' and None\n",
        "    if any(f):\n",
        "        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # type\n",
        "        dir = Path('segment' if seg else 'classify' if cls else '')\n",
        "        h = '--half' if half else ''  # --half FP16 inference arg\n",
        "        s = \"# WARNING ⚠️ ClassificationModel not yet supported for PyTorch Hub AutoShape inference\" if cls else \\\n",
        "            \"# WARNING ⚠️ SegmentationModel not yet supported for PyTorch Hub AutoShape inference\" if seg else ''\n",
        "        LOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\n",
        "                    f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n",
        "                    f\"\\nDetect:          python {dir / ('detect.py' if det else 'predict.py')} --weights {f[-1]} {h}\"\n",
        "                    f\"\\nValidate:        python {dir / 'val.py'} --weights {f[-1]} {h}\"\n",
        "                    f\"\\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')  {s}\"\n",
        "                    f\"\\nVisualize:       https://netron.app\")\n",
        "    return f  # return list of exported files/dirs\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n",
        "    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')\n",
        "    parser.add_argument('--keras', action='store_true', help='TF: use Keras')\n",
        "    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')\n",
        "    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')\n",
        "    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF/TensorRT: dynamic axes')\n",
        "    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')\n",
        "    parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')\n",
        "    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')\n",
        "    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')\n",
        "    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')\n",
        "    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')\n",
        "    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')\n",
        "    parser.add_argument(\n",
        "        '--include',\n",
        "        nargs='+',\n",
        "        default=['torchscript'],\n",
        "        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')\n",
        "    opt = parser.parse_args()\n",
        "    print_args(vars(opt))\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n",
        "        run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)"
      ],
      "metadata": {
        "id": "f2iuyPV0SPdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec76cb2-7e97-4962-b819-cde1d378e2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/yolov5/export-coreml-nms.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/yolov5/export-coreml-nms.py\n",
        "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit\n",
        "\n",
        "Format                      | `export.py --include`         | Model\n",
        "---                         | ---                           | ---\n",
        "PyTorch                     | -                             | yolov5s.pt\n",
        "TorchScript                 | `torchscript`                 | yolov5s.torchscript\n",
        "ONNX                        | `onnx`                        | yolov5s.onnx\n",
        "OpenVINO                    | `openvino`                    | yolov5s_openvino_model/\n",
        "TensorRT                    | `engine`                      | yolov5s.engine\n",
        "CoreML                      | `coreml`                      | yolov5s.mlmodel\n",
        "TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\n",
        "TensorFlow GraphDef         | `pb`                          | yolov5s.pb\n",
        "TensorFlow Lite             | `tflite`                      | yolov5s.tflite\n",
        "TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\n",
        "TensorFlow.js               | `tfjs`                        | yolov5s_web_model/\n",
        "PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/\n",
        "\n",
        "Requirements:\n",
        "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU\n",
        "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU\n",
        "\n",
        "Usage:\n",
        "    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...\n",
        "\n",
        "Inference:\n",
        "    $ python detect.py --weights yolov5s.pt                 # PyTorch\n",
        "                                 yolov5s.torchscript        # TorchScript\n",
        "                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
        "                                 yolov5s_openvino_model     # OpenVINO\n",
        "                                 yolov5s.engine             # TensorRT\n",
        "                                 yolov5s.mlmodel            # CoreML (macOS-only)\n",
        "                                 yolov5s_saved_model        # TensorFlow SavedModel\n",
        "                                 yolov5s.pb                 # TensorFlow GraphDef\n",
        "                                 yolov5s.tflite             # TensorFlow Lite\n",
        "                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
        "                                 yolov5s_paddle_model       # PaddlePaddle\n",
        "\n",
        "TensorFlow.js:\n",
        "    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example\n",
        "    $ npm install\n",
        "    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model\n",
        "    $ npm start\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import contextlib\n",
        "import json\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "if platform.system() != 'Windows':\n",
        "    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\n",
        "from utils.dataloaders import LoadImages\n",
        "from utils.general import (LOGGER, Profile, check_dataset, check_img_size, check_requirements, check_version,\n",
        "                           check_yaml, colorstr, file_size, get_default_args, print_args, url2file, yaml_save)\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "MACOS = platform.system() == 'Darwin'  # macOS environment\n",
        "\n",
        "\n",
        "def export_formats():\n",
        "    # YOLOv5 export formats\n",
        "    x = [\n",
        "        ['PyTorch', '-', '.pt', True, True],\n",
        "        ['TorchScript', 'torchscript', '.torchscript', True, True],\n",
        "        ['ONNX', 'onnx', '.onnx', True, True],\n",
        "        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n",
        "        ['TensorRT', 'engine', '.engine', False, True],\n",
        "        ['CoreML', 'coreml', '.mlmodel', True, False],\n",
        "        ['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],\n",
        "        ['TensorFlow GraphDef', 'pb', '.pb', True, True],\n",
        "        ['TensorFlow Lite', 'tflite', '.tflite', True, False],\n",
        "        ['TensorFlow Edge TPU', 'edgetpu', '_edgetpu.tflite', False, False],\n",
        "        ['TensorFlow.js', 'tfjs', '_web_model', False, False],\n",
        "        ['PaddlePaddle', 'paddle', '_paddle_model', True, True],]\n",
        "    return pd.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n",
        "\n",
        "\n",
        "def try_export(inner_func):\n",
        "    # YOLOv5 export decorator, i..e @try_export\n",
        "    inner_args = get_default_args(inner_func)\n",
        "\n",
        "    def outer_func(*args, **kwargs):\n",
        "        prefix = inner_args['prefix']\n",
        "        try:\n",
        "            with Profile() as dt:\n",
        "                f, model = inner_func(*args, **kwargs)\n",
        "            LOGGER.info(f'{prefix} export success ✅ {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')\n",
        "            return f, model\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'{prefix} export failure ❌ {dt.t:.1f}s: {e}')\n",
        "            return None, None\n",
        "\n",
        "    return outer_func\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n",
        "    # YOLOv5 TorchScript model export\n",
        "    LOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n",
        "    f = file.with_suffix('.torchscript')\n",
        "\n",
        "    ts = torch.jit.trace(model, im, strict=False)\n",
        "    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n",
        "    extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n",
        "    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n",
        "        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n",
        "    else:\n",
        "        ts.save(str(f), _extra_files=extra_files)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n",
        "    # YOLOv5 ONNX export\n",
        "    check_requirements('onnx')\n",
        "    import onnx\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\n",
        "    f = file.with_suffix('.onnx')\n",
        "\n",
        "    output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']\n",
        "    if dynamic:\n",
        "        dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\n",
        "        if isinstance(model, SegmentationModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "            dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\n",
        "        elif isinstance(model, DetectionModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n",
        "        im.cpu() if dynamic else im,\n",
        "        f,\n",
        "        verbose=False,\n",
        "        opset_version=opset,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['images'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes=dynamic or None)\n",
        "\n",
        "    # Checks\n",
        "    model_onnx = onnx.load(f)  # load onnx model\n",
        "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
        "\n",
        "    # Metadata\n",
        "    d = {'stride': int(max(model.stride)), 'names': model.names}\n",
        "    for k, v in d.items():\n",
        "        meta = model_onnx.metadata_props.add()\n",
        "        meta.key, meta.value = k, str(v)\n",
        "    onnx.save(model_onnx, f)\n",
        "\n",
        "    # Simplify\n",
        "    if simplify:\n",
        "        try:\n",
        "            cuda = torch.cuda.is_available()\n",
        "            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))\n",
        "            import onnxsim\n",
        "\n",
        "            LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')\n",
        "            model_onnx, check = onnxsim.simplify(model_onnx)\n",
        "            assert check, 'assert check failed'\n",
        "            onnx.save(model_onnx, f)\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'{prefix} simplifier failure: {e}')\n",
        "    return f, model_onnx\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_openvino(file, metadata, half, prefix=colorstr('OpenVINO:')):\n",
        "    # YOLOv5 OpenVINO export\n",
        "    check_requirements('openvino-dev')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n",
        "    import openvino.inference_engine as ie\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with openvino {ie.__version__}...')\n",
        "    f = str(file).replace('.pt', f'_openvino_model{os.sep}')\n",
        "\n",
        "    cmd = f\"mo --input_model {file.with_suffix('.onnx')} --output_dir {f} --data_type {'FP16' if half else 'FP32'}\"\n",
        "    subprocess.run(cmd.split(), check=True, env=os.environ)  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_paddle(model, im, file, metadata, prefix=colorstr('PaddlePaddle:')):\n",
        "    # YOLOv5 Paddle export\n",
        "    check_requirements(('paddlepaddle', 'x2paddle'))\n",
        "    import x2paddle\n",
        "    from x2paddle.convert import pytorch2paddle\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n",
        "    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n",
        "\n",
        "    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[im])  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "# @try_export\n",
        "# def export_coreml(model, im, file, int8, half, prefix=colorstr('CoreML:')):\n",
        "#     # YOLOv5 CoreML export\n",
        "#     check_requirements('coremltools')\n",
        "#     import coremltools as ct\n",
        "\n",
        "#     LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
        "#     f = file.with_suffix('.mlmodel')\n",
        "\n",
        "#     ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n",
        "#     ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "#     bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "#     if bits < 32:\n",
        "#         if MACOS:  # quantization only supported on macOS\n",
        "#             with warnings.catch_warnings():\n",
        "#                 warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "#                 ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
        "#         else:\n",
        "#             print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "#     ct_model.save(f)\n",
        "#     return f, ct_model\n",
        "\n",
        "class CoreMLExportModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, img_size):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)[0]\n",
        "        x = x.squeeze(0)\n",
        "        # Convert box coords to normalized coordinates [0 ... 1]\n",
        "        w = self.img_size[0]\n",
        "        h = self.img_size[1]\n",
        "        objectness = x[:, 4:5]\n",
        "        class_probs = x[:, 5:] * objectness\n",
        "        boxes = x[:, :4] * torch.tensor([1. / w, 1. / h, 1. / w, 1. / h])\n",
        "        return class_probs, boxes\n",
        "\n",
        "@try_export\n",
        "#def export_coreml(model, im, file, num_boxes, num_classes, labels, conf_thres, iou_thres, prefix=colorstr('CoreML:')):\n",
        "def export_coreml(model, im, file, int8, half, num_boxes, num_classes, labels, conf_thres, iou_thres, prefix=colorstr('CoreML:')):\n",
        "    # YOLOv5 CoreML export\n",
        "    check_requirements(('coremltools',))\n",
        "    import coremltools as ct\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
        "    f = file.with_suffix('.mlmodel')\n",
        "\n",
        "    export_model = CoreMLExportModel(model, img_size=opt.imgsz)\n",
        "\n",
        "    ts = torch.jit.trace(export_model.eval(), im, strict=False)  # TorchScript model\n",
        "\n",
        "    orig_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "\n",
        "    # quantize\n",
        "    bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "    if bits < 32:\n",
        "        if MACOS:  # quantization only supported on macOS\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "                orig_model = ct.models.neural_network.quantization_utils.quantize_weights(orig_model, bits, mode)\n",
        "        else:\n",
        "            print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "\n",
        "    spec = orig_model.get_spec()\n",
        "    old_box_output_name = spec.description.output[1].name\n",
        "    old_scores_output_name = spec.description.output[0].name\n",
        "    ct.utils.rename_feature(spec, old_scores_output_name, \"raw_confidence\")\n",
        "    ct.utils.rename_feature(spec, old_box_output_name, \"raw_coordinates\")\n",
        "    spec.description.output[0].type.multiArrayType.shape.extend([num_boxes, num_classes])\n",
        "    spec.description.output[1].type.multiArrayType.shape.extend([num_boxes, 4])\n",
        "    spec.description.output[0].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "    spec.description.output[1].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "\n",
        "    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\n",
        "\n",
        "    # Model from spec\n",
        "    yolo_model = ct.models.MLModel(spec)\n",
        "\n",
        "    # 3. Create NMS protobuf\n",
        "    # Build Non Maximum Suppression model\n",
        "    nms_spec = ct.proto.Model_pb2.Model()\n",
        "    # nms_spec.specificationVersion = 3\n",
        "    nms_spec.specificationVersion = 5\n",
        "\n",
        "    for i in range(2):\n",
        "        decoder_output = spec.description.output[i].SerializeToString()\n",
        "        nms_spec.description.input.add()\n",
        "        nms_spec.description.input[i].ParseFromString(decoder_output)\n",
        "        nms_spec.description.output.add()\n",
        "        nms_spec.description.output[i].ParseFromString(decoder_output)\n",
        "\n",
        "    nms_spec.description.output[0].name = \"confidence\"\n",
        "    nms_spec.description.output[1].name = \"coordinates\"\n",
        "\n",
        "    output_sizes = [num_classes, 4]\n",
        "    for i in range(2):\n",
        "        ma_type = nms_spec.description.output[i].type.multiArrayType\n",
        "        ma_type.shapeRange.sizeRanges.add()\n",
        "        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n",
        "        ma_type.shapeRange.sizeRanges[0].upperBound = -1\n",
        "        ma_type.shapeRange.sizeRanges.add()\n",
        "        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n",
        "        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n",
        "        del ma_type.shape[:]\n",
        "\n",
        "    nms = nms_spec.nonMaximumSuppression\n",
        "    nms.confidenceInputFeatureName = \"raw_confidence\"\n",
        "    nms.coordinatesInputFeatureName = \"raw_coordinates\"\n",
        "    nms.confidenceOutputFeatureName = \"confidence\"\n",
        "    nms.coordinatesOutputFeatureName = \"coordinates\"\n",
        "    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n",
        "    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n",
        "    nms.iouThreshold = iou_thres\n",
        "    nms.confidenceThreshold = conf_thres\n",
        "    # nms.pickTop.perClass = False\n",
        "    nms.pickTop.perClass = True\n",
        "    nms.stringClassLabels.vector.extend(labels.values())\n",
        "    nms_model = ct.models.MLModel(nms_spec)\n",
        "\n",
        "    # 4. Pipeline models together\n",
        "    # Assembling a pipeline model from the two models\n",
        "    # input_features = [(\"image\", ct.models.datatypes.Array(3, 300, 300)),\n",
        "    input_features = [(\"image\", ct.models.datatypes.Array(3, ny, nx)),\n",
        "                        (\"iouThreshold\", ct.models.datatypes.Double()),\n",
        "                        (\"confidenceThreshold\", ct.models.datatypes.Double())]\n",
        "    output_features = [\"confidence\", \"coordinates\"]\n",
        "    pipeline = ct.models.pipeline.Pipeline(input_features, output_features)\n",
        "    pipeline.add_model(yolo_model)\n",
        "    pipeline.add_model(nms_model)\n",
        "\n",
        "    # Correct datatypes\n",
        "    # The \"image\" input should really be an image, not a multi-array\n",
        "    pipeline.spec.description.input[0].ParseFromString(spec.description.input[0].SerializeToString())\n",
        "    # Copy the declarations of the \"confidence\" and \"coordinates\" outputs\n",
        "    # The Pipeline makes these strings by default\n",
        "    pipeline.spec.description.output[0].ParseFromString(nms_spec.description.output[0].SerializeToString())\n",
        "    pipeline.spec.description.output[1].ParseFromString(nms_spec.description.output[1].SerializeToString())\n",
        "\n",
        "    # Add descriptions to the inputs and outputs\n",
        "    pipeline.spec.description.input[1].shortDescription = \"(optional) IOU Threshold override\"\n",
        "    pipeline.spec.description.input[2].shortDescription = \"(optional) Confidence Threshold override\"\n",
        "    pipeline.spec.description.output[0].shortDescription = \"Boxes Class confidence\"\n",
        "    pipeline.spec.description.output[1].shortDescription = \"Boxes [x, y, width, height] (normalized to [0...1])\"\n",
        "\n",
        "    # Add metadata to the model\n",
        "    pipeline.spec.description.metadata.shortDescription = \"YOLOv5 object detector\"\n",
        "    pipeline.spec.description.metadata.author = \"Ultralytics\"\n",
        "\n",
        "    # Add the default threshold values and list of class labels\n",
        "    user_defined_metadata = {\n",
        "        \"iou_threshold\": str(iou_thres),\n",
        "        \"confidence_threshold\": str(conf_thres),\n",
        "        \"classes\": \", \".join(labels.values())}\n",
        "    pipeline.spec.description.metadata.userDefined.update(user_defined_metadata)\n",
        "\n",
        "    # Don't forget this or Core ML might attempt to run the model on an unsupported operating system version!\n",
        "    pipeline.spec.specificationVersion = 5\n",
        "\n",
        "    ct_model = ct.models.MLModel(pipeline.spec)\n",
        "\n",
        "    f = str(file).replace('.pt', '.mlmodel')\n",
        "    ct_model.save(f)\n",
        "\n",
        "    LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
        "    return  f, ct_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n",
        "    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n",
        "    assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n",
        "    try:\n",
        "        import tensorrt as trt\n",
        "    except Exception:\n",
        "        if platform.system() == 'Linux':\n",
        "            check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\n",
        "        import tensorrt as trt\n",
        "\n",
        "    if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n",
        "        grid = model.model[-1].anchor_grid\n",
        "        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "        model.model[-1].anchor_grid = grid\n",
        "    else:  # TensorRT >= 8\n",
        "        check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt>=8.0.0\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "    onnx = file.with_suffix('.onnx')\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\n",
        "    assert onnx.exists(), f'failed to export ONNX file: {onnx}'\n",
        "    f = file.with_suffix('.engine')  # TensorRT engine file\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    if verbose:\n",
        "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
        "\n",
        "    builder = trt.Builder(logger)\n",
        "    config = builder.create_builder_config()\n",
        "    config.max_workspace_size = workspace * 1 << 30\n",
        "    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)  # fix TRT 8.4 deprecation notice\n",
        "\n",
        "    flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "    network = builder.create_network(flag)\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "    if not parser.parse_from_file(str(onnx)):\n",
        "        raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
        "\n",
        "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "    for inp in inputs:\n",
        "        LOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n",
        "    for out in outputs:\n",
        "        LOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n",
        "\n",
        "    if dynamic:\n",
        "        if im.shape[0] <= 1:\n",
        "            LOGGER.warning(f\"{prefix} WARNING ⚠️ --dynamic model requires maximum --batch-size argument\")\n",
        "        profile = builder.create_optimization_profile()\n",
        "        for inp in inputs:\n",
        "            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)\n",
        "        config.add_optimization_profile(profile)\n",
        "\n",
        "    LOGGER.info(f'{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}')\n",
        "    if builder.platform_has_fast_fp16 and half:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "    with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n",
        "        t.write(engine.serialize())\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_saved_model(model,\n",
        "                       im,\n",
        "                       file,\n",
        "                       dynamic,\n",
        "                       tf_nms=False,\n",
        "                       agnostic_nms=False,\n",
        "                       topk_per_class=100,\n",
        "                       topk_all=100,\n",
        "                       iou_thres=0.45,\n",
        "                       conf_thres=0.25,\n",
        "                       keras=False,\n",
        "                       prefix=colorstr('TensorFlow SavedModel:')):\n",
        "    # YOLOv5 TensorFlow SavedModel export\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except Exception:\n",
        "        check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\n",
        "        import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    from models.tf import TFModel\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    f = str(file).replace('.pt', '_saved_model')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "\n",
        "    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n",
        "    im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n",
        "    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)\n",
        "    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    keras_model.trainable = False\n",
        "    keras_model.summary()\n",
        "    if keras:\n",
        "        keras_model.save(f, save_format='tf')\n",
        "    else:\n",
        "        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n",
        "        m = tf.function(lambda x: keras_model(x))  # full model\n",
        "        m = m.get_concrete_function(spec)\n",
        "        frozen_func = convert_variables_to_constants_v2(m)\n",
        "        tfm = tf.Module()\n",
        "        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])\n",
        "        tfm.__call__(im)\n",
        "        tf.saved_model.save(tfm,\n",
        "                            f,\n",
        "                            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False) if check_version(\n",
        "                                tf.__version__, '2.6') else tf.saved_model.SaveOptions())\n",
        "    return f, keras_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_pb(keras_model, file, prefix=colorstr('TensorFlow GraphDef:')):\n",
        "    # YOLOv5 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    f = file.with_suffix('.pb')\n",
        "\n",
        "    m = tf.function(lambda x: keras_model(x))  # full model\n",
        "    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n",
        "    frozen_func = convert_variables_to_constants_v2(m)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tflite(keras_model, im, file, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n",
        "    # YOLOv5 TensorFlow Lite export\n",
        "    import tensorflow as tf\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "    f = str(file).replace('.pt', '-fp16.tflite')\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    if int8:\n",
        "        from models.tf import representative_dataset_gen\n",
        "        dataset = LoadImages(check_dataset(check_yaml(data))['train'], img_size=imgsz, auto=False)\n",
        "        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.target_spec.supported_types = []\n",
        "        converter.inference_input_type = tf.uint8  # or tf.int8\n",
        "        converter.inference_output_type = tf.uint8  # or tf.int8\n",
        "        converter.experimental_new_quantizer = True\n",
        "        f = str(file).replace('.pt', '-int8.tflite')\n",
        "    if nms or agnostic_nms:\n",
        "        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "    open(f, \"wb\").write(tflite_model)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_edgetpu(file, prefix=colorstr('Edge TPU:')):\n",
        "    # YOLOv5 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n",
        "    cmd = 'edgetpu_compiler --version'\n",
        "    help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n",
        "    assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n",
        "    if subprocess.run(f'{cmd} >/dev/null', shell=True).returncode != 0:\n",
        "        LOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\n",
        "        sudo = subprocess.run('sudo --version >/dev/null', shell=True).returncode == 0  # sudo installed on system\n",
        "        for c in (\n",
        "                'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n",
        "                'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n",
        "                'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):\n",
        "            subprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\n",
        "    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\n",
        "    f = str(file).replace('.pt', '-int8_edgetpu.tflite')  # Edge TPU model\n",
        "    f_tfl = str(file).replace('.pt', '-int8.tflite')  # TFLite model\n",
        "\n",
        "    cmd = f\"edgetpu_compiler -s -d -k 10 --out_dir {file.parent} {f_tfl}\"\n",
        "    subprocess.run(cmd.split(), check=True)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tfjs(file, prefix=colorstr('TensorFlow.js:')):\n",
        "    # YOLOv5 TensorFlow.js export\n",
        "    check_requirements('tensorflowjs')\n",
        "    import tensorflowjs as tfjs\n",
        "\n",
        "    LOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\n",
        "    f = str(file).replace('.pt', '_web_model')  # js dir\n",
        "    f_pb = file.with_suffix('.pb')  # *.pb path\n",
        "    f_json = f'{f}/model.json'  # *.json path\n",
        "\n",
        "    cmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n",
        "          f'--output_node_names=Identity,Identity_1,Identity_2,Identity_3 {f_pb} {f}'\n",
        "    subprocess.run(cmd.split())\n",
        "\n",
        "    json = Path(f_json).read_text()\n",
        "    with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n",
        "        subst = re.sub(\n",
        "            r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}', r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n",
        "            r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n",
        "            r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n",
        "            r'\"Identity_3\": {\"name\": \"Identity_3\"}}}', json)\n",
        "        j.write(subst)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "def add_tflite_metadata(file, metadata, num_outputs):\n",
        "    # Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\n",
        "    with contextlib.suppress(ImportError):\n",
        "        # check_requirements('tflite_support')\n",
        "        from tflite_support import flatbuffers\n",
        "        from tflite_support import metadata as _metadata\n",
        "        from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
        "\n",
        "        tmp_file = Path('/tmp/meta.txt')\n",
        "        with open(tmp_file, 'w') as meta_f:\n",
        "            meta_f.write(str(metadata))\n",
        "\n",
        "        model_meta = _metadata_fb.ModelMetadataT()\n",
        "        label_file = _metadata_fb.AssociatedFileT()\n",
        "        label_file.name = tmp_file.name\n",
        "        model_meta.associatedFiles = [label_file]\n",
        "\n",
        "        subgraph = _metadata_fb.SubGraphMetadataT()\n",
        "        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n",
        "        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n",
        "        model_meta.subgraphMetadata = [subgraph]\n",
        "\n",
        "        b = flatbuffers.Builder(0)\n",
        "        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
        "        metadata_buf = b.Output()\n",
        "\n",
        "        populator = _metadata.MetadataPopulator.with_model_file(file)\n",
        "        populator.load_metadata_buffer(metadata_buf)\n",
        "        populator.load_associated_files([str(tmp_file)])\n",
        "        populator.populate()\n",
        "        tmp_file.unlink()\n",
        "\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run(\n",
        "        data=ROOT / 'data/coco128.yaml',  # 'dataset.yaml path'\n",
        "        weights=ROOT / 'yolov5s.pt',  # weights path\n",
        "        imgsz=(640, 640),  # image (height, width)\n",
        "        batch_size=1,  # batch size\n",
        "        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        include=('torchscript', 'onnx'),  # include formats\n",
        "        half=False,  # FP16 half-precision export\n",
        "        inplace=False,  # set YOLOv5 Detect() inplace=True\n",
        "        keras=False,  # use Keras\n",
        "        optimize=False,  # TorchScript: optimize for mobile\n",
        "        int8=False,  # CoreML/TF INT8 quantization\n",
        "        dynamic=False,  # ONNX/TF/TensorRT: dynamic axes\n",
        "        simplify=False,  # ONNX: simplify model\n",
        "        opset=12,  # ONNX: opset version\n",
        "        verbose=False,  # TensorRT: verbose log\n",
        "        workspace=4,  # TensorRT: workspace size (GB)\n",
        "        nms=False,  # TF: add NMS to model\n",
        "        agnostic_nms=False,  # TF: add agnostic NMS to model\n",
        "        topk_per_class=100,  # TF.js NMS: topk per class to keep\n",
        "        topk_all=100,  # TF.js NMS: topk for all classes to keep\n",
        "        iou_thres=0.45,  # TF.js NMS: IoU threshold\n",
        "        conf_thres=0.25,  # TF.js NMS: confidence threshold\n",
        "):\n",
        "    t = time.time()\n",
        "    include = [x.lower() for x in include]  # to lowercase\n",
        "    fmts = tuple(export_formats()['Argument'][1:])  # --include arguments\n",
        "    flags = [x in include for x in fmts]\n",
        "    assert sum(flags) == len(include), f'ERROR: Invalid --include {include}, valid --include arguments are {fmts}'\n",
        "    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n",
        "    file = Path(url2file(weights) if str(weights).startswith(('http:/', 'https:/')) else weights)  # PyTorch weights\n",
        "\n",
        "    # Load PyTorch model\n",
        "    device = select_device(device)\n",
        "    if half:\n",
        "        assert device.type != 'cpu' or coreml, '--half only compatible with GPU export, i.e. use --device 0'\n",
        "        assert not dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both'\n",
        "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
        "\n",
        "    nc, names = model.nc, model.names  # number of classes, class names\n",
        "\n",
        "    # Checks\n",
        "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
        "    if optimize:\n",
        "        assert device.type == 'cpu', '--optimize not compatible with cuda devices, i.e. use --device cpu'\n",
        "\n",
        "    # Input\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n",
        "    im = torch.zeros(batch_size, 3, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n",
        "\n",
        "    # Update model\n",
        "    model.eval()\n",
        "    for k, m in model.named_modules():\n",
        "        if isinstance(m, Detect):\n",
        "            m.inplace = inplace\n",
        "            m.dynamic = dynamic\n",
        "            m.export = True\n",
        "\n",
        "    for _ in range(2):\n",
        "        y = model(im)  # dry runs\n",
        "    if half and not coreml:\n",
        "        im, model = im.half(), model.half()  # to FP16\n",
        "    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n",
        "\n",
        "    metadata = {'stride': int(max(model.stride)), 'names': model.names}  # model metadata\n",
        "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)\")\n",
        "\n",
        "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} model.stride =  {model.stride}  model.names =  {model.names}\")\n",
        "\n",
        "    # Exports\n",
        "    f = [''] * len(fmts)  # exported filenames\n",
        "    warnings.filterwarnings(action='ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\n",
        "    if jit:  # TorchScript\n",
        "        f[0], _ = export_torchscript(model, im, file, optimize)\n",
        "    if engine:  # TensorRT required before ONNX\n",
        "        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose)\n",
        "    if onnx or xml:  # OpenVINO requires ONNX\n",
        "        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)\n",
        "    if xml:  # OpenVINO\n",
        "        f[3], _ = export_openvino(file, metadata, half)\n",
        "    if coreml:  # CoreML\n",
        "        # f[4], _ = export_coreml(model, im, file, int8, half)\n",
        "        ###\n",
        "        nb = shape[1]\n",
        "        f[4], _ = export_coreml(model, im, file, int8, half, nb, nc, names, conf_thres, iou_thres)\n",
        "        ###\n",
        "\n",
        "    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n",
        "        assert not tflite or not tfjs, 'TFLite and TF.js models must be exported separately, please pass only one type.'\n",
        "        assert not isinstance(model, ClassificationModel), 'ClassificationModel export to TF formats not yet supported.'\n",
        "        f[5], s_model = export_saved_model(model.cpu(),\n",
        "                                           im,\n",
        "                                           file,\n",
        "                                           dynamic,\n",
        "                                           tf_nms=nms or agnostic_nms or tfjs,\n",
        "                                           agnostic_nms=agnostic_nms or tfjs,\n",
        "                                           topk_per_class=topk_per_class,\n",
        "                                           topk_all=topk_all,\n",
        "                                           iou_thres=iou_thres,\n",
        "                                           conf_thres=conf_thres,\n",
        "                                           keras=keras)\n",
        "        if pb or tfjs:  # pb prerequisite to tfjs\n",
        "            f[6], _ = export_pb(s_model, file)\n",
        "        if tflite or edgetpu:\n",
        "            f[7], _ = export_tflite(s_model, im, file, int8 or edgetpu, data=data, nms=nms, agnostic_nms=agnostic_nms)\n",
        "            if edgetpu:\n",
        "                f[8], _ = export_edgetpu(file)\n",
        "            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))\n",
        "        if tfjs:\n",
        "            f[9], _ = export_tfjs(file)\n",
        "    if paddle:  # PaddlePaddle\n",
        "        f[10], _ = export_paddle(model, im, file, metadata)\n",
        "\n",
        "    # Finish\n",
        "    f = [str(x) for x in f if x]  # filter out '' and None\n",
        "    if any(f):\n",
        "        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # type\n",
        "        dir = Path('segment' if seg else 'classify' if cls else '')\n",
        "        h = '--half' if half else ''  # --half FP16 inference arg\n",
        "        s = \"# WARNING ⚠️ ClassificationModel not yet supported for PyTorch Hub AutoShape inference\" if cls else \\\n",
        "            \"# WARNING ⚠️ SegmentationModel not yet supported for PyTorch Hub AutoShape inference\" if seg else ''\n",
        "        LOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\n",
        "                    f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n",
        "                    f\"\\nDetect:          python {dir / ('detect.py' if det else 'predict.py')} --weights {f[-1]} {h}\"\n",
        "                    f\"\\nValidate:        python {dir / 'val.py'} --weights {f[-1]} {h}\"\n",
        "                    f\"\\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')  {s}\"\n",
        "                    f\"\\nVisualize:       https://netron.app\")\n",
        "    return f  # return list of exported files/dirs\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n",
        "    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')\n",
        "    parser.add_argument('--keras', action='store_true', help='TF: use Keras')\n",
        "    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')\n",
        "    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')\n",
        "    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF/TensorRT: dynamic axes')\n",
        "    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')\n",
        "    parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')\n",
        "    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')\n",
        "    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')\n",
        "    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')\n",
        "    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')\n",
        "    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')\n",
        "    parser.add_argument(\n",
        "        '--include',\n",
        "        nargs='+',\n",
        "        default=['torchscript'],\n",
        "        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')\n",
        "    opt = parser.parse_args()\n",
        "    print_args(vars(opt))\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n",
        "        run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRmuSgq-aF4L",
        "outputId": "c81431ac-7573-45bc-e26d-d084187c92cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/yolov5/export-coreml-nms.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JD0MUm3nuZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rnXstk6nubJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldIkLdzknuey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "SEfq_sP1ntz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "dvX4cPGHr18S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_1.pth'\n",
        "save_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_1.mlpackage'\n",
        "# save_path = '/content/mobilenetv3_large_100_1.mlpackage'\n",
        "\n",
        "\n",
        "# Load model\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "\n",
        "print(model_ft)\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchRegressionModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            #nn.Softmax(dim=1) #回帰なので不要\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchRegressionModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "#Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    convert_to=\"mlprogram\",\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gu1tnHuUhPpv",
        "outputId": "4a556537-d662-4bfe-a825-2e63fcc5403d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV3(\n",
            "  (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNormAct2d(\n",
            "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "    (drop): Identity()\n",
            "    (act): Hardswish()\n",
            "  )\n",
            "  (blocks): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): DepthwiseSeparableConv(\n",
            "        (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pwl): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv_dw): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pwl): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv_dw): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pwl): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pwl): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (3): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): Identity()\n",
            "        (conv_pwl): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
            "        (bn2): BatchNormAct2d(\n",
            "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): ReLU(inplace=True)\n",
            "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Hardsigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNormAct2d(\n",
            "          160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Identity()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): ConvBnAct(\n",
            "        (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNormAct2d(\n",
            "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (drop): Identity()\n",
            "          (act): Hardswish()\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
            "  (conv_head): Conv2d(960, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (act2): Hardswish()\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (classifier): Linear(in_features=1280, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-698920b2f903>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Trace with random data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mexample_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# after test, will get 'size mismatch' error message with size 256x256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtraced_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#Set the image scale and bias for input image preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example_kwarg_inputs should be a dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         return trace_module(\n\u001b[0m\u001b[1;32m    799\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                 module._c._create_method_from_trace(\n\u001b[0m\u001b[1;32m   1066\u001b[0m                     \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-698920b2f903>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#single tensorで返す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/models/mobilenetv3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/models/mobilenetv3.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/layers/norm_act.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# cut & paste of torch.nn.BatchNorm2d.forward impl to avoid issues with torchscript and tracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'expected 4D input (got {x.ndim}D input)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# exponential_average_factor is set to self.momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: expected 4D input (got 3D input)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###output形状の確認\n",
        "\n",
        "model_ft.eval()\n",
        "\n",
        "filename = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/dataset_yolo_cropped/1000_L.png\"\n",
        "\n",
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "# if torch.cuda.is_available():\n",
        "#     input_batch = input_batch.to('cuda')\n",
        "#     model_ft.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model_ft(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
        "print(output[0])\n",
        "print(output)\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "# print(probabilities)"
      ],
      "metadata": {
        "id": "Jjuow3tGK0wx",
        "outputId": "4c8812b1-a057-41cb-ff6f-aae3b7235345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14.9153])\n",
            "tensor([[14.9153]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EwOj3lTl8gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Npup_Zz5l8iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install coremltools --q\n",
        "import coremltools as ct\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "class ImageFilteringModel(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x + 100\n",
        "\n",
        "torch_model = ImageFilteringModel()\n",
        "shape = (1, 3, 256, 256)\n",
        "traced_model = torch.jit.trace(torch_model, torch.rand(*shape))\n",
        "\n",
        "coreml_model = ct.convert(traced_model,\n",
        "                          convert_to=\"mlprogram\",\n",
        "                          inputs=[ct.ImageType(name=\"colorImage\",\n",
        "                                               shape=shape,\n",
        "                                               color_layout=ct.colorlayout.RGB,)],\n",
        "                          outputs=[ct.ImageType(name=\"colorOutput\",\n",
        "                                                color_layout=ct.colorlayout.RGB,)],\n",
        "                          )\n",
        "\n",
        "coreml_model.save(\"/content/drive/MyDrive/Deep_learning/ColorToColorModel.mlpackage\")\n"
      ],
      "metadata": {
        "id": "ZPAdEBHEl8p3",
        "outputId": "8bbd0495-86a0-494c-b018-285756977efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:coremltools:Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n",
            "Converting PyTorch Frontend ==> MIL Ops:  67%|██████▋   | 2/3 [00:00<00:00, 547.17 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 18444.61 passes/s]\n",
            "Running MIL default pipeline: 100%|██████████| 71/71 [00:00<00:00, 3427.51 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 4547.49 passes/s]\n"
          ]
        }
      ]
    }
  ]
}