{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPh82Z5Uy72CKWYdlASlp3m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/YOLOv8_to_EfficientNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv8 to EfficientNetv2 pipeline**"
      ],
      "metadata": {
        "id": "w2bPHz9wew6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Olympia datasetã‹ã‚‰ç›®ã‚’åˆ‡ã‚ŠæŠœãï¼ˆYOLOv8nï¼‰**\n",
        "#**åˆ‡ã‚ŠæŠœã„ãŸç›®ã‚’EfficientNetV2ã§è§£æã™ã‚‹**"
      ],
      "metadata": {
        "id": "LXWb3YtIFX1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Olympia dataset\n",
        "Dlibã§ç›®ãŒ2ã¤æ¤œå‡ºã•ã‚Œã‚‹ã‚‚ã®ã‚’æŠœãå‡ºã™\n",
        "YOLOv8ã‚’ç”¨ã„ã¦å·¦å³ã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’èªè­˜ã•ã›ã‚‹\n",
        "åˆ‡ã‚ŠæŠœã„ãŸãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹å†…ã®ç”»åƒã«ã¤ã„ã¦ã€EfficientNetv2ã‚’ç”¨ã„ã¦çœ¼çƒçªå‡ºåº¦ã€MRD-1ã€MRD-2ã‚’å›å¸°ã•ã›ã‚‹\n",
        "ã‚¹ãƒãƒ›ã«å®Ÿè£…\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UnyBedKNbfS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7MdRVDhFJmO",
        "outputId": "311250d1-2a53-477e-ef02-50457c774353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "#ã‚µãƒãƒ¼ãƒˆãƒ‘ãƒƒãƒã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8'\n",
        "\n",
        "#å…ƒç”»åƒãƒ•ã‚©ãƒ«ãƒ€\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#å…ƒç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#åˆ‡ã‚Šã¬ã„ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€\n",
        "csv_path = f\"{parent_dir}/dataset_uni_for_YOLO8.csv\""
      ],
      "metadata": {
        "id": "_DI1wc0eF5uR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parent_dirãŒã‚ã‚Œã°å‰Šé™¤ã™ã‚‹\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# æ–°ã—ãparent_dirã‚’ä½œæˆã™ã‚‹\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirã‚’æ–°è¦ã«ä½œæˆã™ã‚‹\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "# orig_dirã«dataset_dirç›´ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦ã‚³ãƒ”ãƒ¼ã™ã‚‹\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2ZGZkPOK84g",
        "outputId": "1f0b69d3-c779-4faf-ee65-0be7b9e8baed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1016/1016 [00:27<00:00, 37.07file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeã‚’ç”¨ã„ã¦ç›®ã‚’æ¤œå‡º**"
      ],
      "metadata": {
        "id": "6Y6OC0UoMOZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰åˆ†é¡å™¨ã®ç‰¹å¾´é‡å–å¾—\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n"
      ],
      "metadata": {
        "id": "f2rM0ZbTMN50"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ç›®ãŒ2ã¤ä»¥ä¸Šæ¤œå‡ºã•ã‚ŒãŸã‚‚ã®ã‚’æŠœãå‡ºã™**"
      ],
      "metadata": {
        "id": "FWLibTiZc1Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #ãƒ•ã‚©ãƒ«ãƒ€æ•°ã®åˆ†ã ã‘\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # ç”»åƒã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«åŒ–\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pixä»¥ä¸Šã®ã‚‚ã®ã§ç›®ã«è¦‹ãˆã‚‹ã‚‚ã®ã‚’æŠ½å‡º\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # çœ¼æ¤œå‡ºåˆ¤å®š\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('ç›®ãŒ' + str(len(eye_list)) +'å€‹æ¤œå‡ºã•ã‚Œã¾ã—ãŸ')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #ç”»åƒã®åˆ‡ã‚ŠæŠœãã¨ä¿å­˜ï¼ˆ2å€‹ä»¥ä¸Šæ¤œå‡ºã®æ™‚ã«é™ã‚‹ï¼‰\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      #img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #æœ¬æ¥ã®åˆ‡ã‚ŠæŠœãã‚ˆã‚Šå¹…ã®0.1å€ãšã¤æ°´å¢—ã—ã™ã‚‹\n",
        "                      img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #æ¨ªå¹…ã®åŠåˆ†ã‚ˆã‚Šå·¦ã«ã‚ã‚‹ã®ã¯å³çœ¼\n",
        "                      else:\n",
        "                          side = \"L\" #æ¨ªå¹…ã®åŠåˆ†ã‚ˆã‚Šã‚ˆã‚Šå³ã«ã‚ã‚‹ã®ã¯å·¦çœ¼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #å¯¾å¿œè¡¨ã®ä½œæˆ\n",
        "                      writer.writerow([id, file_path, side, ex+round(ew/2), ey+round(eh/2), ew, eh])\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "pnVFMJJVMW8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ã“ã“ã§ã€ç›®ä»¥å¤–ãŒèª¤æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’æ‰‹å‹•ã§æŠœãå‡ºã—ã¦å‰Šé™¤ã™ã‚‹**"
      ],
      "metadata": {
        "id": "tm4HsJszc9VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ä¸é©åˆ‡ãªç”»åƒãŒã‚ã‚Œã°ã€å·¦å³ã¾ã¨ã‚ã¦æ¶ˆã™\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v3abWZGGe9sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CSVã®ãƒªã‚¹ãƒˆã¨ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹æ¯”ã¹ã¦ã€æ¶ˆã—ãŸç”»åƒã‚’åˆ¤åˆ¥ã€‚CSVã‚’ä¿®æ­£ã™ã‚‹ã€‚\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Get a list of unique ids\n",
        "csv_id_list = df['id'].unique().tolist()\n",
        "\n",
        "print(f\"csv_id_list: {csv_id_list}\")\n",
        "\n",
        "\n",
        "# Construct the output directory path\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "# Get a list of all image file names in the directory\n",
        "image_file_names = os.listdir(out_dir)\n",
        "\n",
        "# Split each file name at \"_\" and take the first part, then make sure the list is unique\n",
        "image_id_list = list(set([int(name.split('_')[0]) for name in image_file_names]))\n",
        "print(f\"image_id_list: {image_id_list}\")\n",
        "\n",
        "# Find the ids that are in the CSV list but not in the image file list\n",
        "missing_ids = [id for id in csv_id_list if id not in image_id_list]\n",
        "print(f\"missing_ids: {missing_ids}\")\n",
        "\n",
        "\n",
        "\n",
        "# Remove the rows with ids in missing_ids\n",
        "df = df[~df['id'].isin(missing_ids)]\n",
        "\n",
        "# Save the dataframe back to a CSV\n",
        "df.to_csv(f\"{parent_dir}/dataset_uni_for_YOLO8_modified.csv\", index=False)"
      ],
      "metadata": {
        "id": "aGVqU2n9i3wR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4004df66-6b11-420c-fe17-ab315df8f36d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv_id_list: [19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 41, 45, 47, 46, 48, 49, 50, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 88, 90, 91, 92, 93, 94, 95, 96, 97, 936, 99, 101, 100, 102, 103, 104, 105, 106, 107, 109, 112, 110, 111, 113, 115, 116, 937, 117, 118, 120, 121, 124, 123, 125, 122, 938, 126, 127, 128, 129, 939, 131, 133, 132, 134, 135, 137, 136, 940, 139, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 155, 941, 157, 158, 159, 160, 162, 161, 945, 163, 164, 165, 166, 167, 168, 169, 946, 170, 947, 171, 172, 173, 174, 175, 176, 177, 178, 948, 179, 949, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 199, 201, 950, 202, 203, 951, 204, 205, 206, 208, 207, 210, 209, 212, 213, 214, 215, 952, 216, 217, 953, 219, 954, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 955, 234, 235, 236, 237, 239, 240, 241, 243, 242, 244, 245, 246, 247, 248, 249, 250, 957, 251, 252, 253, 254, 958, 255, 256, 959, 257, 258, 960, 259, 260, 261, 262, 263, 264, 266, 265, 267, 961, 268, 269, 270, 271, 272, 273, 274, 275, 276, 962, 278, 279, 280, 963, 282, 284, 283, 285, 287, 288, 964, 289, 290, 292, 965, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 302, 304, 305, 306, 307, 966, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 321, 323, 324, 325, 326, 328, 327, 967, 329, 330, 331, 332, 968, 333, 334, 335, 336, 337, 969, 338, 339, 340, 342, 343, 344, 970, 971, 345, 346, 347, 349, 350, 935, 351, 972, 353, 352, 354, 356, 357, 358, 359, 360, 361, 362, 973, 363, 364, 974, 365, 367, 368, 369, 975, 371, 372, 373, 375, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 976, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 977, 978, 399, 979, 400, 402, 403, 405, 404, 406, 407, 408, 410, 409, 980, 411, 981, 412, 413, 414, 416, 415, 417, 418, 419, 420, 421, 422, 982, 983, 425, 426, 428, 427, 429, 430, 431, 432, 434, 433, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 984, 445, 446, 449, 448, 447, 450, 451, 452, 453, 455, 454, 456, 457, 458, 459, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 488, 490, 489, 491, 987, 988, 492, 493, 494, 495, 496, 498, 499, 500, 989, 990, 501, 502, 503, 504, 505, 506, 507, 509, 508, 992, 993, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 528, 527, 529, 530, 994, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 995, 546, 996, 547, 548, 549, 551, 552, 997, 553, 554, 998, 556, 555, 557, 558, 559, 560, 561, 999, 562, 563, 564, 565, 566, 567, 1000, 568, 569, 570, 1001, 571, 572, 573, 1002, 574, 575, 1003, 576, 578, 579, 580, 581, 583, 582, 584, 585, 587, 588, 589, 590, 592, 593, 594, 595, 596, 597, 598, 599, 601, 600, 602, 1004, 603, 604, 605, 606, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 619, 618, 621, 622, 623, 625, 626, 627, 628, 629, 630, 631, 633, 635, 634, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 658, 657, 659, 660, 661, 1006, 662, 663, 2, 664, 665, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 1007, 678, 679, 1008, 682, 683, 684, 688, 687, 689, 690, 691, 692, 693, 694, 695, 697, 698, 699, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 1, 714, 715, 716, 717, 718, 719, 1009, 720, 986, 1010, 1011, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 742, 743, 744, 745, 746, 748, 747, 750, 749, 751, 752, 753, 754, 755, 757, 758, 759, 760, 761, 1012, 762, 763, 764, 766, 765, 1013, 1014, 767, 768, 770, 769, 771, 772, 1017, 773, 774, 1018, 775, 776, 778, 777, 779, 1016, 780, 781, 782, 1019, 783, 784, 785, 787, 786, 788, 789, 791, 790, 792, 793, 794, 795, 796, 798, 799, 797, 800, 801, 802, 803, 804, 805, 806, 807, 808, 1020, 809, 810, 811, 812, 813, 1021, 814, 815, 816, 818, 819, 820, 821, 822, 823, 825, 824, 826, 827, 828, 829, 1022, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 852, 853, 854, 855, 856, 857, 858, 859, 1024, 1025, 861, 862, 864, 865, 866, 867, 1026, 1027, 1028, 868, 870, 869, 1029, 871, 872, 874, 873, 875, 876, 877, 878, 879, 1031, 883, 884, 886, 887, 885, 888, 889, 890, 891, 892, 894, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 918, 919, 917, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
            "image_id_list: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 282, 283, 284, 285, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 492, 493, 494, 495, 496, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 546, 547, 548, 549, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 578, 579, 580, 581, 582, 583, 584, 585, 587, 588, 589, 590, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 621, 622, 623, 625, 626, 627, 628, 629, 630, 631, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 678, 679, 682, 683, 684, 687, 688, 689, 690, 691, 692, 693, 694, 695, 697, 698, 699, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 852, 853, 854, 855, 856, 857, 858, 859, 861, 862, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 894, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 986, 987, 988, 989, 990, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1024, 1025, 1026, 1027, 1028, 1029, 1031]\n",
            "missing_ids: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "â‘ \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_orig\"ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆ1.jpgã¨ã™ã‚‹ã¨1ï¼‰ã¨ã€â‘¡\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_uni\"ã®ã‚¢ãƒ³ãƒ€ãƒ¼ãƒãƒ¼ã®æ‰‹å‰ï¼ˆ1_R.jpgã¨ã™ã‚‹ã¨1)ã‚’è¦‹æ¯”ã¹ã‚‹\n",
        "\n",
        "\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_dlib_detected\"ã¨ã„ã†ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ\n",
        "â‘ ã¨â‘¡ã§ä¸€è‡´ã™ã‚‹ã‚‚ã®(2ã¤ç›®ãŒæ¤œå‡ºã•ã‚ŒãŸã‚‚ã®)ã«é–¢ã—ã¦ã€â‘ ã®ç”»åƒï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã‚’ä¸€è¾º512pixã®é»’å¡—ã‚Šletterboxã«ã—ã¦å¤‰æ›ã—ã€æ–°ã—ãä½œæˆã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹\n",
        "...ã§ã¯ãªãletterboxã¯ä½¿ã‚ãšãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ã™ã‚‹ã“ã¨ã¨ã—ãŸ\n",
        "\"\"\"\n",
        "\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# def letterbox_image(image, size):\n",
        "#     \"\"\"\n",
        "#     ç”»åƒã‚’æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºã®é»’å¡—ã‚Šletterboxã«å¤‰æ›ã™ã‚‹é–¢æ•°\n",
        "#     :param image: å¤‰æ›ã™ã‚‹ç”»åƒï¼ˆPIL Imageï¼‰\n",
        "#     :param size: å¤‰æ›å¾Œã®ã‚µã‚¤ã‚ºï¼ˆå¹…, é«˜ã•ï¼‰ã®ã‚¿ãƒ—ãƒ«\n",
        "#     :return: å¤‰æ›å¾Œã®ç”»åƒï¼ˆPIL Imageï¼‰\n",
        "#     \"\"\"\n",
        "#     new_width, new_height = size\n",
        "#     old_width, old_height = image.size\n",
        "#     ratio = min(new_width / old_width, new_height / old_height)\n",
        "#     new_width = int(old_width * ratio)\n",
        "#     new_height = int(old_height * ratio)\n",
        "#     resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
        "#     boxed_image = Image.new(\"RGB\", size, (0, 0, 0))\n",
        "#     boxed_image.paste(resized_image, ((size[0] - new_width) // 2, (size[1] - new_height) // 2))\n",
        "#     return boxed_image\n",
        "\n",
        "def main():\n",
        "    # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
        "    output_folder = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_dlib_detected\"\n",
        "\n",
        "    # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
        "    if os.path.exists(output_folder):\n",
        "        shutil.rmtree(output_folder)\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "    # â‘ ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ã—ã¦ã€1.jpgã®ã‚ˆã†ãªå½¢å¼ã«å¤‰æ›\n",
        "    orig_folder = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_orig\"\n",
        "    orig_files = os.listdir(orig_folder)\n",
        "    orig_filenames = [os.path.splitext(filename)[0] for filename in orig_files]\n",
        "\n",
        "    # â‘¡ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚¢ãƒ³ãƒ€ãƒ¼ãƒãƒ¼ã®æ‰‹å‰ã‚’å–å¾—ã—ã¦ã€1_R.jpgã®ã‚ˆã†ãªå½¢å¼ã«å¤‰æ›\n",
        "    uni_folder = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_uni\"\n",
        "    uni_files = os.listdir(uni_folder)\n",
        "    uni_filenames = [filename.split(\"_\")[0] for filename in uni_files]\n",
        "\n",
        "    # â‘ ã¨â‘¡ã§ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã—ã¦ã€å‡¦ç†ã‚’å®Ÿè¡Œ\n",
        "    orig_set = set(orig_filenames)\n",
        "    uni_set = set(uni_filenames)\n",
        "\n",
        "    common_filenames = orig_set.intersection(uni_set)\n",
        "\n",
        "    print(f\"In {len(common_filenames)} images, both eyes were detected. Copy files to the folder 'dataset_dlib_detected'.\")\n",
        "\n",
        "    for filename in tqdm(common_filenames, desc=\"Processing Images\"):\n",
        "        orig_file_path = os.path.join(orig_folder, filename + \".JPG\")\n",
        "        #uni_file_path = os.path.join(uni_folder, filename + \"_R.JPG\")  # \"R\"ã‚’æŒ‡å®šã—ã¦ã„ã¾ã™ãŒã€\"L\"ã®å ´åˆã‚‚ã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“\n",
        "        output_file_path = os.path.join(output_folder, filename + \".JPG\")  # æ‹¡å¼µå­ã‚’.JPGã«å¤‰æ›´\n",
        "\n",
        "\n",
        "        print(f\"output_file_path: {output_file_path}\")\n",
        "\n",
        "        # # â‘ ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿\n",
        "        # orig_image = Image.open(orig_file_path)\n",
        "        # # ä¸€è¾º512pxã®é»’å¡—ã‚Šletterboxã«å¤‰æ›\n",
        "        # new_image = letterbox_image(orig_image, (512, 512))\n",
        "        # # å¤‰æ›å¾Œã®ç”»åƒã‚’ä¿å­˜\n",
        "        # new_image.save(output_file_path)\n",
        "\n",
        "        #ãƒªã‚µã‚¤ã‚ºã›ãšã«ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ã™ã‚‹\n",
        "        shutil.copy(orig_file_path, output_file_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "JsWZBevoSmhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = os.listdir(\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_dlib_detected\")\n",
        "print(len(num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgphB4aDYNx_",
        "outputId": "ef47dc6a-3b40-43be-abb2-22e845e3cb52"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv8 annotations**"
      ],
      "metadata": {
        "id": "FdvZB6GrKCe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8'\n",
        "\n",
        "#å…ƒç”»åƒãƒ•ã‚©ãƒ«ãƒ€\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#å…ƒç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "\n",
        "#åˆ‡ã‚Šã¬ã„ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#åˆ‡ã‚Šã¬ã„ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
        "dlib_detected_dir = f\"{parent_dir}/dataset_dlib_datected\"\n",
        "\n",
        "#CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€\n",
        "csv_path = f\"{parent_dir}/dataset_uni_for_YOLO8_modified.csv\""
      ],
      "metadata": {
        "id": "t3x7BL3UKGc6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§DataFrameã«æ ¼ç´\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# å‰å‡¦ç†æ¸ˆã¿ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®š\n",
        "df_dlib_detected = df\n",
        "df_dlib_detected[\"img_path\"] = df_dlib_detected[\"img_path\"].str.replace(\"dataset_orig\", \"dataset_dlib_detected\")\n",
        "\n",
        "# R/Lã‚’æ•°å­—ã«å¤‰æ›´\n",
        "df_dlib_detected[\"side R/L\"] = df_dlib_detected[\"side R/L\"].replace({\"R\": 0, \"L\": 1})\n",
        "\n",
        "# DataFrameã®å†…å®¹ã‚’è¡¨ç¤º\n",
        "df_dlib_detected"
      ],
      "metadata": {
        "id": "EXDkoZoVKuwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é‡è¤‡ã®ãªã„idã®ç¢ºèª\n",
        "unique_ids = list(set(df[\"id\"].tolist()))\n",
        "print(unique_ids)\n",
        "len(unique_ids)"
      ],
      "metadata": {
        "id": "n9m_dg7jf_Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# ç”»åƒãƒ•ã‚©ãƒ«ãƒ€å†…ã«åŒåã®YOLOå½¢å¼txtã‚’ä½œæˆã™ã‚‹\n",
        "##########################\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def xywh_to_yolo(class_label, img_width, img_height, x, y, w, h):\n",
        "    x_center = x / img_width\n",
        "    y_center = y / img_height\n",
        "    width = w / img_width\n",
        "    height = h / img_height\n",
        "\n",
        "    print(f\"img_width: {img_width}\")\n",
        "    print(f\"x,y,w,h: {x}, {y}, {w}, {h}\")\n",
        "\n",
        "    return f\"{class_label} {x_center} {y_center} {width} {height}\"\n",
        "\n",
        "def create_yolo_txt(df):\n",
        "    grouped_data = df.groupby('id')\n",
        "\n",
        "    for group_id, group_df in grouped_data:\n",
        "        txt_file_path = os.path.join(os.path.dirname(group_df.iloc[0]['img_path']), f\"{group_id}.txt\")\n",
        "\n",
        "        with open(txt_file_path, 'w') as txt_file:\n",
        "            for _, row in group_df.iterrows():\n",
        "                img_path = row['img_path']\n",
        "                class_label = row['side R/L']\n",
        "                x, y, w, h = row['ex'], row['ey'], row['ew'], row['eh']\n",
        "\n",
        "                try:\n",
        "                    img = Image.open(img_path)\n",
        "                    img_width, img_height = img.size\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: Failed to open '{img_path}' or get image dimensions. Skipping annotation.\")\n",
        "                    continue\n",
        "\n",
        "                # YOLOå½¢å¼ã«å¤‰æ›\n",
        "                yolo_annotation = xywh_to_yolo(class_label, img_width, img_height, x, y, w, h)\n",
        "\n",
        "                # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿\n",
        "                yolo_annotation += '\\n'  # æ”¹è¡Œã‚’è¿½åŠ \n",
        "                txt_file.write(yolo_annotation)        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å‚ç…§ã™ã‚‹\n",
        "\n",
        "        with open(txt_file_path, 'r') as txt_file_read:\n",
        "            file_contents = txt_file_read.read()\n",
        "            print(f\"Contents of '{txt_file_path}':\\n{file_contents}\")\n",
        "\n",
        "# YOLOå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ä¿å­˜\n",
        "df_sample = df_dlib_detected[0:]\n",
        "create_yolo_txt(df_sample)\n"
      ],
      "metadata": {
        "id": "cjbhluZHLyQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dlib_detected"
      ],
      "metadata": {
        "id": "dBylsWVIxSxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLOv8_training**"
      ],
      "metadata": {
        "id": "DX_lR8bf2QSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8'\n",
        "eyecrop_dir = os.path.join(parent_dir, 'for_eyecrop_training')\n",
        "\n",
        "\n",
        "# ã‚‚ã—æ—¢ã«\"eyecrop\"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å‰Šé™¤ã—ã¾ã™\n",
        "if os.path.exists(eyecrop_dir):\n",
        "    try:\n",
        "        shutil.rmtree(eyecrop_dir)  # ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ãªã‚‰ã“ã‚Œã§å‰Šé™¤ã§ãã¾ã™\n",
        "    except OSError as e:\n",
        "        print(f\"å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "else:\n",
        "    print(\"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æ–°ã—ãä½œæˆã—ã¾ã™ã€‚\")\n",
        "\n",
        "# \"eyecrop\"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™\n",
        "try:\n",
        "    os.mkdir(eyecrop_dir)\n",
        "    print(\"eyecropãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\")\n",
        "except OSError as e:\n",
        "    print(f\"ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "\n",
        "# \"train\"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™\n",
        "train_dir = os.path.join(eyecrop_dir, 'train')\n",
        "try:\n",
        "    os.mkdir(train_dir)\n",
        "    print(\"trainãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\")\n",
        "except OSError as e:\n",
        "    print(f\"ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "\n",
        "# \"val\"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™\n",
        "val_dir = os.path.join(eyecrop_dir, 'val')\n",
        "try:\n",
        "    os.mkdir(val_dir)\n",
        "    print(\"valãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\")\n",
        "except OSError as e:\n",
        "    print(f\"ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "\n",
        "\n",
        "# YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š\"\n",
        "yaml_file_path = os.path.join(eyecrop_dir, 'data.yaml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esqHHl4R8giz",
        "outputId": "09aa2697-a79e-4c34-9fc9-1185b02a43a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eyecropãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n",
            "trainãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n",
            "valãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YAML ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è¨­å®š\n",
        "%%writefile $yaml_file_path\n",
        "# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/train\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/val\n",
        "\n",
        "# number of classes\n",
        "nc: 2\n",
        "\n",
        "# class names\n",
        "names: ['R', 'L']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iECkqqgu71rl",
        "outputId": "77bc77cb-81b1-43b5-aab6-9c6624f9c010"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/data.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ï¼—ï¼šï¼“ã«åˆ†å‰²\n",
        "\n",
        "Deep_learning\n",
        "â””â”€â”€ Olympia_dataset\n",
        "    â””â”€â”€ dataset_uni_for_YOLOv8\n",
        "        â””â”€â”€ for_eyecrop_training\n",
        "            â”œâ”€â”€ train\n",
        "            â”œâ”€â”€ val\n",
        "            â””â”€â”€ data.yaml\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®random_state\n",
        "random.seed(42)\n",
        "\n",
        "# å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
        "src_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_dlib_detected'\n",
        "train_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/train'\n",
        "val_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/val'\n",
        "\n",
        "# å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰JPGãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
        "jpg_files = glob.glob(os.path.join(src_dir, '*.JPG'))\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "random.shuffle(jpg_files)\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’70%:30%ã«åˆ†å‰²\n",
        "split_point = int(len(jpg_files) * 0.7)\n",
        "train_files = jpg_files[:split_point]\n",
        "val_files = jpg_files[split_point:]\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã‚Œãã‚Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼\n",
        "for file_path in tqdm(train_files, desc=\"Copying train files\"):\n",
        "    shutil.copy2(file_path, train_dir)\n",
        "    txt_file = os.path.splitext(file_path)[0] + '.txt'\n",
        "    shutil.copy2(txt_file, train_dir)\n",
        "\n",
        "for file_path in tqdm(val_files, desc=\"Copying validation files\"):\n",
        "    shutil.copy2(file_path, val_dir)\n",
        "    txt_file = os.path.splitext(file_path)[0] + '.txt'\n",
        "    shutil.copy2(txt_file, val_dir)\n"
      ],
      "metadata": {
        "id": "AhIlcPgZCOGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv8 setup\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "O6SawQZdE5DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_file_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/data.yaml'\n",
        "\n",
        "%cd /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y_l04WrGVKs",
        "outputId": "a14aca4c-21e2-4dd3-fa3e-7043fee0e1e0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8n.pt data=$yaml_file_path epochs=100 imgsz=640\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVhZngabFHOL",
        "outputId": "cc4b83e5-da97-4290-eb5c-09f23d35d4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 80.8MB/s]\n",
            "Ultralytics YOLOv8.0.143 ğŸš€ Python-3.10.6 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/data.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3011238 parameters, 3011222 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/train... 672 images, 0 backgrounds, 0 corrupt: 100% 672/672 [00:02<00:00, 270.87it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/val... 289 images, 0 backgrounds, 0 corrupt: 100% 289/289 [00:01<00:00, 151.27it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/val.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100      2.55G      1.645       2.32      1.689         72        640: 100% 42/42 [00:28<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.36it/s]\n",
            "                   all        289        579      0.497      0.993      0.505      0.278\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100      2.28G      1.187      1.541      1.254         62        640: 100% 42/42 [00:27<00:00,  1.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.13it/s]\n",
            "                   all        289        579      0.498      0.995      0.517      0.331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100      2.31G      1.134      1.441      1.244         59        640: 100% 42/42 [00:27<00:00,  1.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.27it/s]\n",
            "                   all        289        579      0.493       0.99      0.677      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100      2.28G       1.14      1.358      1.237         57        640: 100% 42/42 [00:27<00:00,  1.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.21it/s]\n",
            "                   all        289        579      0.492      0.995      0.535      0.386\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100      2.28G      1.061      1.272      1.193         74        640: 100% 42/42 [00:28<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.26it/s]\n",
            "                   all        289        579      0.498      0.991      0.609      0.438\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100      2.28G      1.045      1.217      1.206         60        640: 100% 42/42 [00:28<00:00,  1.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.15it/s]\n",
            "                   all        289        579      0.668      0.908       0.83       0.62\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100      2.26G      1.039      1.139      1.186         57        640: 100% 42/42 [00:27<00:00,  1.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.40it/s]\n",
            "                   all        289        579      0.695      0.949      0.913      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100      2.28G      1.012      1.013      1.194         60        640: 100% 42/42 [00:28<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.29it/s]\n",
            "                   all        289        579      0.911      0.915      0.953      0.678\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100      2.28G     0.9684     0.9501       1.17         66        640: 100% 42/42 [00:27<00:00,  1.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.28it/s]\n",
            "                   all        289        579      0.967      0.958      0.988      0.754\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100      2.28G     0.9295     0.8638       1.15         69        640: 100% 42/42 [00:26<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:06<00:00,  1.44it/s]\n",
            "                   all        289        579      0.932      0.945       0.98      0.748\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100       2.3G     0.9119     0.7984      1.133         65        640: 100% 42/42 [00:29<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:06<00:00,  1.47it/s]\n",
            "                   all        289        579      0.912      0.931      0.987      0.759\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100      2.28G     0.9142      0.779      1.137         67        640: 100% 42/42 [00:27<00:00,  1.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:09<00:00,  1.09it/s]\n",
            "                   all        289        579      0.922       0.96      0.988      0.767\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100      2.28G      0.909     0.7612      1.143         71        640: 100% 42/42 [00:27<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.25it/s]\n",
            "                   all        289        579      0.941      0.969      0.987      0.769\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100      2.28G     0.9029     0.7445      1.141         80        640: 100% 42/42 [00:32<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:09<00:00,  1.01it/s]\n",
            "                   all        289        579      0.963      0.974       0.99      0.734\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100      2.28G     0.8994     0.7202      1.142         69        640: 100% 42/42 [00:30<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.12it/s]\n",
            "                   all        289        579      0.977      0.961      0.988      0.732\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100      2.28G     0.9005     0.7047      1.127         77        640: 100% 42/42 [00:29<00:00,  1.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:09<00:00,  1.08it/s]\n",
            "                   all        289        579      0.952      0.971       0.99      0.786\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100      2.28G     0.9107     0.7115      1.144         67        640: 100% 42/42 [00:30<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:09<00:00,  1.01it/s]\n",
            "                   all        289        579      0.949      0.965      0.987      0.765\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100      2.28G     0.8296     0.6336      1.088         68        640: 100% 42/42 [00:27<00:00,  1.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.18it/s]\n",
            "                   all        289        579      0.909      0.921      0.984      0.783\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100      2.28G     0.8735     0.6648      1.132         80        640: 100% 42/42 [00:29<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:06<00:00,  1.46it/s]\n",
            "                   all        289        579      0.946      0.962      0.989      0.783\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100      2.26G     0.8719       0.66      1.133         61        640: 100% 42/42 [00:28<00:00,  1.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.17it/s]\n",
            "                   all        289        579      0.958      0.974      0.991      0.778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100      2.28G      0.853     0.6514       1.11         57        640: 100% 42/42 [00:26<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.16it/s]\n",
            "                   all        289        579      0.956      0.979      0.993      0.757\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100      2.28G     0.8332     0.6203      1.097         81        640: 100% 42/42 [00:28<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.33it/s]\n",
            "                   all        289        579      0.958      0.974       0.99      0.767\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100      2.26G     0.8181     0.6034      1.097         61        640: 100% 42/42 [00:27<00:00,  1.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.29it/s]\n",
            "                   all        289        579      0.952      0.979      0.991      0.773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100      2.28G     0.8631      0.614       1.11         57        640: 100% 42/42 [00:28<00:00,  1.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.22it/s]\n",
            "                   all        289        579      0.957      0.967       0.99      0.744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100      2.28G      0.824     0.5994      1.094         71        640: 100% 42/42 [00:27<00:00,  1.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.14it/s]\n",
            "                   all        289        579       0.97      0.982      0.994      0.796\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100      2.28G     0.8107     0.5891      1.083         57        640: 100% 42/42 [00:27<00:00,  1.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.18it/s]\n",
            "                   all        289        579      0.996      0.969      0.994      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100       2.3G     0.8305     0.5809      1.106         75        640: 100% 42/42 [00:27<00:00,  1.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.29it/s]\n",
            "                   all        289        579      0.955      0.981       0.99      0.777\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100      2.26G     0.8082     0.5711      1.083         75        640: 100% 42/42 [00:30<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.24it/s]\n",
            "                   all        289        579      0.972      0.982      0.993      0.808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100      2.28G     0.8374     0.5943      1.101         76        640: 100% 42/42 [00:29<00:00,  1.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.25it/s]\n",
            "                   all        289        579      0.929      0.938      0.988      0.808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100      2.28G     0.8208     0.5699      1.088         63        640: 100% 42/42 [00:31<00:00,  1.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:08<00:00,  1.16it/s]\n",
            "                   all        289        579      0.912      0.964      0.985      0.785\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100      2.26G     0.8083     0.5586      1.083         79        640: 100% 42/42 [00:29<00:00,  1.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  40% 4/10 [00:02<00:03,  1.85it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ç¶šãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "from ultralytics import YOLO\n",
        "\n",
        "yaml_file_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/data.yaml'\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/yolov8n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "model.train(data=yaml_file_path, epochs=100, imgsz=640, resume=True)"
      ],
      "metadata": {
        "id": "PNjb7qWNPW-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv8 interference**"
      ],
      "metadata": {
        "id": "ySmQamVspQKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv8 setup\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh0dYAFu3vjY",
        "outputId": "16186673-1b00-47e0-9db4-814c2374da98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.142 ğŸš€ Python-3.10.6 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 24.3/225.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "dataset_orig_dir = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/dataset_orig\"\n",
        "sample_img = random.choice(glob.glob(f\"{dataset_orig_dir}/*\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "Zk5NbELPMCDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import Image as DisplayImage\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿(detectionãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)\n",
        "model = YOLO('/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv8/for_eyecrop_training/yolov8n.pt')\n",
        "# predictãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ (çµæœã ã‘æ¬²ã—ã„ã®ã§ã€projectãƒ»nameãƒ»exist_okã¯ãªãã¦ã‚‚OK)\n",
        "results = model.predict(source=sample_img)\n",
        "# Resultsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æç”»ã«å¿…è¦ãªæƒ…å ±ã‚’å–å¾—\n",
        "coordinate_bbox = results[0].boxes.xyxy #bbox\n",
        "classes=results[0].boxes.cls # æ¤œå‡ºã‚¯ãƒ©ã‚¹\n",
        "classes_map = results[0].names # ã‚¯ãƒ©ã‚¹ç•ªå·ã¨åç§°\n",
        "# ç”»åƒã®èª­ã¿è¾¼ã¿\n",
        "img = Image.open(results[0].path)\n",
        "# è‰²ã®æŒ‡å®š (ã‚¯ãƒ©ã‚¹ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã«è‰²ã‚’é¸æŠã™ã‚‹å ´åˆä½¿ã†)\n",
        "# colors = ['red', 'green', 'blue', 'yellow', 'cyan', 'magenta', 'olive', 'purple']\n",
        "# colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255), (128, 128, 0), (128, 0, 128)]\n",
        "# æç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒˆã®å–å¾—\n",
        "draw = ImageDraw.Draw(img)\n",
        "\"\"\"\n",
        "ãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šã€‚Macã ã¨/System/Library/Fontsã«è‰²ã€…ã‚ã‚‹ã®ã§ã“ã“ã‹ã‚‰é¸ã‚“ã ã€‚\n",
        "bboxã ã‘ã®æç”»ã§ã‚ã‚Œã°å¿…è¦ãªã„ã€‚å¿…è¦ãªã„å ´åˆã¯draw.text(font=font)ã®fontéƒ¨åˆ†ã‚’æ¶ˆã™ã€‚\n",
        "ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ•ãƒ«ãƒ‘ã‚¹(/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒä¸¸ã‚´ ProN W4.ttc)ã§è¨­å®šã—ã¦ã‚ã’ã‚‹\n",
        "\"\"\"\n",
        "font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 4000)\n",
        "# ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®æç”»\n",
        "i=1\n",
        "for bbox, cls in zip(coordinate_bbox, classes):\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    #color = colors[int(cls) % len(colors)] ä»Šå›ã¯è‰²ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹ã®ã§ä½¿ã‚ãªã„\n",
        "    color=(255,0,0) # red\n",
        "    # bboxã®æç”»\n",
        "    draw.rectangle([x1, y1, x2, y2], outline=color, width=5)\n",
        "    cls_text = classes_map.get(int(cls))\n",
        "    # æ¤œå‡ºã‚¯ãƒ©ã‚¹åã®æç”»\n",
        "    draw.text((x1, y1 - 50), cls_text+\"#\"+str(i), fill=\"orange\")\n",
        "    print(f\"class: {cls_text}\")\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "    i += 1\n",
        "# ç”»åƒã®ãƒªã‚µã‚¤ã‚º (å¿…è¦ã§ã‚ã‚Œã°)\n",
        "#img = img.resize((640,640))\n",
        "# å‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã€‚model.predictã§ä½œæˆã•ã›ã¦ã‚‚OKã€‚\n",
        "directories = ['/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict']\n",
        "for directory in directories:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "# ç”»åƒã®ä¿å­˜ (å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã ã¨FileNotFoundErrorã«ãªã‚‹ã®ã§æ³¨æ„)\n",
        "img.save('/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict/sample_bbox.png')\n",
        "# ç”»åƒã®è¡¨ç¤º\n",
        "display(DisplayImage(filename='/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict/sample_bbox.png'))"
      ],
      "metadata": {
        "id": "q4mjNJMIMCF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhyAS02tMCIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mp0v-saxOElF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk3r2PCCOEno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_N89zEa_OEqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget /content https://pds.exblog.jp/logo/1/197001/01/18/b043001820221113221721.jpg\n",
        "!wget /content https://plugins-media.makeupar.com/smb/blog/post/2023-06-15/37011ea9-289b-4700-9e61-6a1429128ae6.jpg\n",
        "!wget /content https://www.city.gosen.lg.jp/material/images/group/5/chugatasu.JPG\n",
        "img_path = [\"/content/b043001820221113221721.jpg\", \"37011ea9-289b-4700-9e61-6a1429128ae6.jpg\", \"chugatasu.JPG\"]"
      ],
      "metadata": {
        "id": "GZ1Pi39p4D2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import Image as DisplayImage\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿(detectionãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)\n",
        "model = YOLO('yolov8n.pt')\n",
        "# predictãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ (çµæœã ã‘æ¬²ã—ã„ã®ã§ã€projectãƒ»nameãƒ»exist_okã¯ãªãã¦ã‚‚OK)\n",
        "results = model.predict(source=img_path[2])\n",
        "# Resultsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æç”»ã«å¿…è¦ãªæƒ…å ±ã‚’å–å¾—\n",
        "coordinate_bbox = results[0].boxes.xyxy #bbox\n",
        "classes=results[0].boxes.cls # æ¤œå‡ºã‚¯ãƒ©ã‚¹\n",
        "classes_map = results[0].names # ã‚¯ãƒ©ã‚¹ç•ªå·ã¨åç§°\n",
        "# ç”»åƒã®èª­ã¿è¾¼ã¿\n",
        "img = Image.open(results[0].path)\n",
        "# è‰²ã®æŒ‡å®š (ã‚¯ãƒ©ã‚¹ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã«è‰²ã‚’é¸æŠã™ã‚‹å ´åˆä½¿ã†)\n",
        "# colors = ['red', 'green', 'blue', 'yellow', 'cyan', 'magenta', 'olive', 'purple']\n",
        "# colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255), (128, 128, 0), (128, 0, 128)]\n",
        "# æç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒˆã®å–å¾—\n",
        "draw = ImageDraw.Draw(img)\n",
        "\"\"\"\n",
        "ãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šã€‚Macã ã¨/System/Library/Fontsã«è‰²ã€…ã‚ã‚‹ã®ã§ã“ã“ã‹ã‚‰é¸ã‚“ã ã€‚\n",
        "bboxã ã‘ã®æç”»ã§ã‚ã‚Œã°å¿…è¦ãªã„ã€‚å¿…è¦ãªã„å ´åˆã¯draw.text(font=font)ã®fontéƒ¨åˆ†ã‚’æ¶ˆã™ã€‚\n",
        "ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ•ãƒ«ãƒ‘ã‚¹(/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒä¸¸ã‚´ ProN W4.ttc)ã§è¨­å®šã—ã¦ã‚ã’ã‚‹\n",
        "\"\"\"\n",
        "font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 4000)\n",
        "# ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®æç”»\n",
        "i=1\n",
        "for bbox, cls in zip(coordinate_bbox, classes):\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    #color = colors[int(cls) % len(colors)] ä»Šå›ã¯è‰²ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹ã®ã§ä½¿ã‚ãªã„\n",
        "    color=(255,0,0) # red\n",
        "    # bboxã®æç”»\n",
        "    draw.rectangle([x1, y1, x2, y2], outline=color, width=5)\n",
        "    cls_text = classes_map.get(int(cls))\n",
        "    # æ¤œå‡ºã‚¯ãƒ©ã‚¹åã®æç”»\n",
        "    draw.text((x1, y1 - 50), cls_text+\"#\"+str(i), fill=\"orange\")\n",
        "    print(f\"class: {cls_text}\")\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "    i += 1\n",
        "# ç”»åƒã®ãƒªã‚µã‚¤ã‚º (å¿…è¦ã§ã‚ã‚Œã°)\n",
        "#img = img.resize((640,640))\n",
        "# å‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã€‚model.predictã§ä½œæˆã•ã›ã¦ã‚‚OKã€‚\n",
        "directories = ['/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict']\n",
        "for directory in directories:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "# ç”»åƒã®ä¿å­˜ (å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã ã¨FileNotFoundErrorã«ãªã‚‹ã®ã§æ³¨æ„)\n",
        "img.save('/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict/sample_bbox.png')\n",
        "# ç”»åƒã®è¡¨ç¤º\n",
        "display(DisplayImage(filename='/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict/sample_bbox.png'))"
      ],
      "metadata": {
        "id": "2zTw01xTZlu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import Image as DisplayImage\n",
        "from IPython.display import display\n",
        "\n",
        "# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿(detectionãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)\n",
        "model = YOLO('yolov8n.pt')\n",
        "# predictãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ (çµæœã ã‘æ¬²ã—ã„ã®ã§ã€projectãƒ»nameãƒ»exist_okã¯ãªãã¦ã‚‚OK)\n",
        "results = model.predict(source=img_path[1])\n",
        "# Resultsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æç”»ã«å¿…è¦ãªæƒ…å ±ã‚’å–å¾—\n",
        "coordinate_bbox = results[0].boxes.xyxy #bbox\n",
        "classes=results[0].boxes.cls # æ¤œå‡ºã‚¯ãƒ©ã‚¹\n",
        "classes_map = results[0].names # ã‚¯ãƒ©ã‚¹ç•ªå·ã¨åç§°\n",
        "\n",
        "# Open the image file\n",
        "img_cv2 = cv2.imread(results[0].path)\n",
        "\n",
        "# Set the font scale and thickness\n",
        "font_scale = 3\n",
        "font_thickness = 2\n",
        "\n",
        "# Loop through each bounding box\n",
        "i=1\n",
        "for bbox, cls in zip(coordinate_bbox, classes):\n",
        "    print(bbox)\n",
        "\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    color=(0,0,255) # red\n",
        "    # Draw the bounding box\n",
        "    cv2.rectangle(img_cv2, (x1, y1), (x2, y2), color, 5)\n",
        "    cls_text = classes_map.get(int(cls))\n",
        "    # Put the class label text\n",
        "    cv2.putText(img_cv2, cls_text+\"#\"+str(i), (x1, y1 - 50), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, font_thickness)\n",
        "    i += 1\n",
        "\n",
        "# Save the image\n",
        "cv2.imwrite('/Users/hinomaruc/Desktop/blog/dataset/yolov8/runs/mypredict/sample_bbox.png', img_cv2)\n",
        "\n",
        "\n",
        "#plt.imshow(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))\n",
        "# Convert the color from BGR to RGB\n",
        "img_rgb = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Use matplotlib to display the image\n",
        "plt.imshow(img_rgb)\n",
        "\n",
        "# Show the axes\n",
        "plt.axis('on')\n",
        "\n",
        "# Show the image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aTBwFQPBet5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}