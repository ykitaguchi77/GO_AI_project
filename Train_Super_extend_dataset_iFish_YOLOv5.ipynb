{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Train_Super_extend_dataset_iFish_YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train GO super-extend dataset iFish YOLOv5**\n",
        "\n",
        "Train YOLOv5 using iFish augmentation\n",
        "\n",
        "```\n",
        "iFish: https://github.com/Gil-Mor/iFish.git\n",
        "\n",
        "yolov5_iFish: https://github.com/ykitaguchi77/yolov5-iFish.git\n",
        "※ FishAugment(distortion_range=(0, 0.3), p=0.5)\n",
        "\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "43479fe3-bf65-448c-af67-df9b2d964120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep  9 12:33:34 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    23W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d84c53a-bf2f-4b41-b54b-d15cfcb0cbf8"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc3e90f-9652-48e4-a3fb-3e10a8afc21c"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.87"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_images: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images'))}\")\n",
        "print(f\"val_images: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images'))}\")\n",
        "print(f\"train_labels: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels'))}\")\n",
        "print(f\"val_labels: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels'))}\")\n",
        "\n",
        "def check_basename(image_dir, label_dir):\n",
        "    # 画像ファイルのベースネーム一覧を取得\n",
        "    image_files = [os.path.splitext(file)[0] for file in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, file))]\n",
        "    image_files = set(image_files)  # 重複を削除\n",
        "\n",
        "    # ラベルファイルのベースネーム一覧を取得\n",
        "    label_files = [os.path.splitext(file)[0] for file in os.listdir(label_dir) if os.path.isfile(os.path.join(label_dir, file))]\n",
        "    label_files = set(label_files)  # 重複を削除\n",
        "\n",
        "    # ファイルのベースネームが完全に一致するか確認\n",
        "    match = image_files == label_files\n",
        "\n",
        "    if match:\n",
        "        print(\"すべてのファイルのベースネームが一致しています。\")\n",
        "    else:\n",
        "        print(\"ファイルのベースネームが一致していません。\")\n",
        "\n",
        "check_basename('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images', '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels')\n",
        "check_basename('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images', '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8WcREJmtMLv",
        "outputId": "de7bf91f-f32c-4baf-e4ac-8ed64d245062"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images: 3498\n",
            "val_images: 795\n",
            "train_labels: 3498\n",
            "val_labels: 795\n",
            "すべてのファイルのベースネームが一致しています。\n",
            "すべてのファイルのベースネームが一致しています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels'\n",
        "\n",
        "# ディレクトリ内のファイルを取得\n",
        "files = os.listdir(directory)\n",
        "\n",
        "# テキストファイルを読み込んで表示\n",
        "for file in files:\n",
        "    if file.endswith('.txt'):\n",
        "        file_path = os.path.join(directory, file)\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read().strip()\n",
        "            if content:\n",
        "                print(f\"ファイル名: {file}\\n内容: {content}\\n\")\n",
        "            else:\n",
        "                print(f\"ファイル名: {file}\\nエラー: ファイルの内容が空欄です。\\n\")\n",
        "                sys.exit(1)  # プログラムを中止\n"
      ],
      "metadata": {
        "id": "E-NkFCNSwQLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Albumentationによるデータ拡張**\n",
        "\n",
        "(YOLOv5-iFish https://github.com/ykitaguchi77/yolov5-iFish.git に組み込み済み)"
      ],
      "metadata": {
        "id": "ORyTzZcA7sPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from albumentations import DualTransform\n",
        "from math import sqrt\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "class FishEyeEffect(DualTransform):\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(FishEyeEffect, self).__init__(always_apply, p)\n",
        "        self.image_shape = None  # Initialize the instance variable to store image shape\n",
        "\n",
        "\n",
        "    def get_fish_xn_yn(self, source_x, source_y, radius, distortion):\n",
        "        if 1 - distortion*(radius**2) == 0:\n",
        "            return source_x, source_y\n",
        "        return source_x / (1 - (distortion*(radius**2))), source_y / (1 - (distortion*(radius**2)))\n",
        "\n",
        "    def get_original_xn_yn(self, dest_x, dest_y, radius, distortion):\n",
        "        return dest_x * (1 - distortion * (radius**2)), dest_y * (1 - distortion * (radius**2))\n",
        "\n",
        "\n",
        "    def apply(self, img, distortion=0, **params):\n",
        "        self.image_shape = img.shape  # set image shape here\n",
        "        # your fish-eye effect implementation here\n",
        "        dstimg = np.zeros_like(img)\n",
        "        w, h = float(img.shape[0]), float(img.shape[1])\n",
        "        for x in range(len(dstimg)):\n",
        "            for y in range(len(dstimg[x])):\n",
        "                xnd, ynd = float((2*x - w)/w), float((2*y - h)/h)\n",
        "                rd = sqrt(xnd**2 + ynd**2)\n",
        "                xdu, ydu = self.get_fish_xn_yn(xnd, ynd, rd, distortion)\n",
        "                xu, yu = int(((xdu + 1)*w)/2), int(((ydu + 1)*h)/2)\n",
        "                if 0 <= xu < img.shape[0] and 0 <= yu < img.shape[1]:\n",
        "                    dstimg[x][y] = img[xu][yu]\n",
        "        return dstimg\n",
        "\n",
        "    def apply_to_bbox(self, bbox, distortion=0, **params):\n",
        "        img_h, img_w = self.image_shape[:2]  # Use the stored image shape\n",
        "\n",
        "        # YOLO形式から4つの角の座標を取得\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "\n",
        "        # 4つの角の座標を正規化\n",
        "        corners = [(x_min, y_min),\n",
        "                  (x_max, y_min),\n",
        "                  (x_min, y_max),\n",
        "                  (x_max, y_max)]\n",
        "\n",
        "        # 正規化された座標を魚眼変換用に変換\n",
        "        corners = [(2*x - 1, 2*y - 1) for x, y in corners]\n",
        "\n",
        "        print(\"Transformed corners:\", corners)\n",
        "\n",
        "        # 各角をFishEyeで逆変換\n",
        "        distorted_corners = [self.get_original_xn_yn(x, y, sqrt(x**2 + y**2), distortion) for x, y in corners]\n",
        "\n",
        "        print(\"Distorted corners:\", distorted_corners)\n",
        "\n",
        "        # 逆変換された点を含む最小の長方形を求める\n",
        "        x_min_new = min(x for x, y in distorted_corners)\n",
        "        x_max_new = max(x for x, y in distorted_corners)\n",
        "        y_min_new = min(y for x, y in distorted_corners)\n",
        "        y_max_new = max(y for x, y in distorted_corners)\n",
        "\n",
        "        # 魚眼逆変換後の座標を画像座標系に戻す\n",
        "        x_min_new = (x_min_new + 1) / 2\n",
        "        x_max_new = (x_max_new + 1) / 2\n",
        "        y_min_new = (y_min_new + 1)  / 2\n",
        "        y_max_new = (y_max_new + 1)  / 2\n",
        "\n",
        "        print(x_min_new, y_min_new, x_max_new, y_max_new)\n",
        "\n",
        "        return x_min_new, y_min_new, x_max_new, y_max_new\n",
        "\n",
        "\n",
        "    def get_params(self):\n",
        "        # Random distortion coefficient between 0 and 0.3\n",
        "        return {'distortion': random.uniform(0, 0.3)}\n",
        "\n",
        "\n",
        "# 変換する関数を定義\n",
        "transform = A.Compose([\n",
        "    #A.RandomCrop(width=400, height=400),\n",
        "    FishEyeEffect(p=1),\n",
        "    #A.HorizontalFlip(p=1),\n",
        "    #A.VerticalFlip(p=1),\n",
        "    A.RandomBrightnessContrast(p=1),\n",
        "], bbox_params=A.BboxParams(format='yolo',min_area=1024,\n",
        "min_visibility=0.1, label_fields=['class_labels']))\n",
        "\n",
        "\n",
        "\n",
        "# 画像読み込み\n",
        "image = cv2.imread(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images/1043.jpg\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# yoloのアノテーションファイルを読み込み\n",
        "f = open('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels/1043.txt', 'r')\n",
        "anno_lists = f.readlines()\n",
        "f.close()\n",
        "\n",
        "# 空の配列を用意\n",
        "bboxes = []\n",
        "class_labels = []\n",
        "\n",
        "# アノテーションファイルをパイプラインに与える形に変換\n",
        "for anno_list in anno_lists:\n",
        "    # スペースで区切る\n",
        "    anno_list = anno_list.split()\n",
        "\n",
        "    # 0番目の要素からクラスラベルを取り出し\n",
        "    class_labels.append(anno_list[0])\n",
        "\n",
        "    # BBoxの値をfloatに変換\n",
        "    anno_list = [float(i) for i in anno_list[1:]]\n",
        "\n",
        "    # BBoxの配列に追加\n",
        "    bboxes.append(anno_list)\n",
        "\n",
        "\n",
        "# 変換を実行し、変換後のimage, bboxes, class_labelsを取得\n",
        "transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "transformed_image = transformed['image']\n",
        "transformed_bboxes = transformed['bboxes']\n",
        "transformed_class_labels = transformed['class_labels']\n",
        "\n",
        "\n",
        "\n",
        "# 関数：YOLO形式のバウンディングボックスをmatplotlibのrectangleに変換\n",
        "def yolo_to_rect(bbox, img_shape):\n",
        "    cx, cy, w, h = bbox\n",
        "    img_h, img_w = img_shape[:2]\n",
        "    tlx = int((cx - w/2) * img_w)\n",
        "    tly = int((cy - h/2) * img_h)\n",
        "    rect_w = int(w * img_w)\n",
        "    rect_h = int(h * img_h)\n",
        "    return tlx, tly, rect_w, rect_h\n",
        "\n",
        "# 元の画像とバウンディングボックスを表示\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "for bbox in bboxes:\n",
        "    tlx, tly, rect_w, rect_h = yolo_to_rect(bbox, image.shape)\n",
        "    rect = plt.Rectangle((tlx, tly), rect_w, rect_h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "plt.title('Before Augmentation')\n",
        "\n",
        "# 変換後の画像とバウンディングボックスを表示\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(transformed_image)\n",
        "plt.axis('off')\n",
        "for bbox in transformed_bboxes:\n",
        "    tlx, tly, rect_w, rect_h = yolo_to_rect(bbox, transformed_image.shape)\n",
        "    rect = plt.Rectangle((tlx, tly), rect_w, rect_h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "plt.title('After Augmentation')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # To save the transformed annotations:\n",
        "# with open('result.txt', 'w') as f:\n",
        "#     for i, transformed_bbox in enumerate(transformed_bboxes):\n",
        "#         f.write(\"{} {} {} {} {}\\n\".format(\n",
        "#             transformed_class_labels[i],\n",
        "#             transformed_bbox[0], transformed_bbox[1],\n",
        "#             transformed_bbox[2], transformed_bbox[3]\n",
        "#         ))\n",
        "\n"
      ],
      "metadata": {
        "id": "TfgxSSHW70zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training\n",
        "!git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationを実装したバージョン\n",
        "%cd yolov5-iFish\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "ZjV_xXLpd5__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4be16f8-7cad-4f92-d951-0169a2b25de3"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2269e2e Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 29.3/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e9e3a9-9ef4-418b-85e2-b0efd9d0e2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n.pt, cfg=, data=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "remote: Enumerating objects: 15943, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 15943 (delta 28), reused 34 (delta 12), pack-reused 15880\u001b[K\n",
            "Receiving objects: 100% (15943/15943), 14.67 MiB | 18.19 MiB/s, done.\n",
            "Resolving deltas: 100% (10923/10923), done.\n",
            "From https://github.com/ultralytics/yolov5\n",
            " * [new branch]      add/weights_dir   -> ultralytics/add/weights_dir\n",
            " * [new branch]      benchmarks        -> ultralytics/benchmarks\n",
            " * [new branch]      exp/scaleFill     -> ultralytics/exp/scaleFill\n",
            " * [new branch]      exp12             -> ultralytics/exp12\n",
            " * [new branch]      exp13             -> ultralytics/exp13\n",
            " * [new branch]      exp13-nosoft      -> ultralytics/exp13-nosoft\n",
            " * [new branch]      exp13-soft        -> ultralytics/exp13-soft\n",
            " * [new branch]      fix/rgb_albumentations -> ultralytics/fix/rgb_albumentations\n",
            " * [new branch]      ghost             -> ultralytics/ghost\n",
            " * [new branch]      master            -> ultralytics/master\n",
            " * [new branch]      study_activations -> ultralytics/study_activations\n",
            " * [new branch]      ultralytics/HUB   -> ultralytics/ultralytics/HUB\n",
            " * [new branch]      update/cls-album  -> ultralytics/update/cls-album\n",
            " * [new branch]      update/textlogger -> ultralytics/update/textlogger\n",
            " * [new branch]      update/threaded   -> ultralytics/update/threaded\n",
            " * [new tag]         v1.0              -> v1.0\n",
            " * [new tag]         v2.0              -> v2.0\n",
            " * [new tag]         v3.0              -> v3.0\n",
            " * [new tag]         v3.1              -> v3.1\n",
            " * [new tag]         v4.0              -> v4.0\n",
            " * [new tag]         v5.0              -> v5.0\n",
            " * [new tag]         v6.0              -> v6.0\n",
            " * [new tag]         v6.1              -> v6.1\n",
            " * [new tag]         v6.2              -> v6.2\n",
            " * [new tag]         v7.0              -> v7.0\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 2699 commits. Use 'git pull ultralytics master' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
            "YOLOv5 🚀 2269e2e Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\n",
            "100% 3.87M/3.87M [00:00<00:00, 17.6MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1      9471  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "Model summary: 214 layers, 1766623 parameters, 1766623 gradients, 4.2 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5n.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mClass utils.augmentations.FishEyeEffect is not serializable because the `get_transform_init_args_names` method is not implemented\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels.cache... 3498 images, 0 backgrounds, 0 corrupt: 100% 3498/3498 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels.cache... 795 images, 0 backgrounds, 0 corrupt: 100% 795/795 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m1.64 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      0/299         2G     0.1005    0.03219    0.02756         51        640:  10% 22/219 [01:16<06:08,  1.87s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 途中から\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/dataset.yaml --resume /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\n"
      ],
      "metadata": {
        "id": "5126RlFFfXWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d5e42b-65df-4597-bea2-2a3fb686ba85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
            "WARNING ⚠️ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
            "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
            "    import torch\n",
            "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
            "    torch.save(ckpt, \"updated-model.pt\")\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
            "    from tensorboard.compat import notf  # noqa: F401\n",
            "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5/train.py\", line 56, in <module>\n",
            "    from utils.loggers import Loggers\n",
            "  File \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5/utils/loggers/__init__.py\", line 23, in <module>\n",
            "    from torch.utils.tensorboard import SummaryWriter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
            "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 16, in <module>\n",
            "    from ._embedding import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/_embedding.py\", line 9, in <module>\n",
            "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
            "    return getattr(load_once(self), attr_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
            "    cache[arg] = f(arg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 50, in load_once\n",
            "    module = load_fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
            "    import tensorflow\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 52, in <module>\n",
            "    from ._api.v2 import compat\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/__init__.py\", line 37, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 31, in <module>\n",
            "    from . import compat\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 37, in <module>\n",
            "    from . import v1\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_t1_bMiSeo09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCJ9m4Swo0Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KrBGgvJ_AvBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRe67Lg1AvD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKOIspiWAvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxDheZVGAvJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7xRpeWqObKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0UeNKxikObNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Show examples**"
      ],
      "metadata": {
        "id": "MjJoQlzAOTS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "val_images_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images\"\n",
        "val_labels_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels\"\n",
        "\n",
        "for sample_num in range(21):\n",
        "    sample_image_path = glob.glob(f\"{val_images_dir}/*\")[sample_num]\n",
        "    sample_label_path = glob.glob(f\"{val_labels_dir}/*\")[sample_num]\n",
        "\n",
        "    # 画像の読み込み\n",
        "    image = cv2.imread(sample_image_path)\n",
        "    height, width = image.shape[:2]\n",
        "    image = cv2.resize(image, (640, int(height * 640 / width)))\n",
        "\n",
        "    # バウンディングボックスの情報\n",
        "    with open(sample_label_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "    content_list = content.split()\n",
        "\n",
        "    # バウンディングボックスの座標を計算\n",
        "    x = float(content_list[1]) * image.shape[1]\n",
        "    y = float(content_list[2]) * image.shape[0]\n",
        "    box_width = float(content_list[3]) * image.shape[1]\n",
        "    box_height = float(content_list[4]) * image.shape[0]\n",
        "\n",
        "    # バウンディングボックスの座標を計算\n",
        "    left = int(x - (box_width / 2))\n",
        "    top = int(y - (box_height / 2))\n",
        "    right = int(x + (box_width / 2))\n",
        "    bottom = int(y + (box_height / 2))\n",
        "\n",
        "    # バウンディングボックスを画像に重ねる\n",
        "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
        "\n",
        "    # 画像の表示\n",
        "    cv2_imshow(image)\n"
      ],
      "metadata": {
        "id": "iRp4OAwxOXIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effd129b-8479-432c-93d0-713b01ec8cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-176-g76ea9ed Python-3.10.12 torch-2.0.1+cu118 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (4 CPUs, 25.5 GB RAM, 23.9/225.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_olympia_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "dataset_olympia_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "dataset_handai_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/grav\"\n",
        "dataset_handai_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/cont\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "\n",
        "image_path = glob.glob(f\"{dataset_olympia_grav}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"cont\", 1: \"grav\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testsetの正解率を確認\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "class_names = {0: \"cont\", 1: \"grav\"}\n",
        "\n",
        "\n",
        "\n",
        "def interference_testset(image_path, answer):\n",
        "    pred_list = []\n",
        "    correct_list = [] #正解なら1、不正解なら0\n",
        "\n",
        "    for img in image_path:\n",
        "        pred = interference(img, weight)\n",
        "\n",
        "        # output result\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "        # probability\n",
        "        prob = pred[0][0][4].item()\n",
        "\n",
        "        # class\n",
        "        class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "        if class_name == answer:\n",
        "            correct_list.append(1)\n",
        "        else:\n",
        "            correct_list.append(0)\n",
        "\n",
        "        print(\"診断は %s、確率は%.1f％です。\" % (class_name, prob * 100))\n",
        "\n",
        "    total_count = len(correct_list)\n",
        "    one_count = correct_list.count(1)\n",
        "    percentage = (one_count / total_count) * 100\n",
        "    print(\"\")\n",
        "    print(f\"image_path: {image_path}\")\n",
        "    print(f\"The percentage of corrects in the list is: {percentage:.2f}%\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "interference_testset(glob.glob(f\"{dataset_olympia_grav}/*\"), \"grav\")\n",
        "interference_testset(glob.glob(f\"{dataset_olympia_cont}/*\"), \"cont\")\n",
        "interference_testset(glob.glob(f\"{dataset_handai_grav}/*\"), \"grav\")\n",
        "interference_testset(glob.glob(f\"{dataset_handai_cont}/*\"), \"cont\")\n"
      ],
      "metadata": {
        "id": "4iGCwXZrAcRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "・外部のデータセット（Treated）を洗い出し\n",
        "\n",
        "・内部のデータセットをさらに水増し\n",
        "\n",
        "・内部および外部データセットより、test用各100枚（grav50枚、cont50枚）を抜き出しておき、合体する\n",
        "\n",
        "・既存のYOLOv5を用いてbounding boxを抜き出し、新たにトレーニングする"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}