{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Hertel_YOLOv5_MobileNetv3_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation YOLOv5-MobileNetv3 pipeline**\n",
        "\n",
        "Train YOLOv5\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlibで目が2つ検出されるものを抜き出す\n",
        "YOLOv5を用いて左右とバウンディングボックスを認識させる\n",
        "抜き出した画像についてMobileNetV3で回帰（5-fold ensemble）を行う\n",
        "スマホに実装\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###はじめの設定"
      ],
      "metadata": {
        "id": "TEGv1MI4DnFM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a081e2f-115f-414d-8d9a-b3e2b376395b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 13 00:57:42 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362557b3-9966-413c-a3cd-0d8b5474752b"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.99"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78d22fe-7899-421e-ae09-3c420ec4b903"
      },
      "source": [
        "'''\n",
        "Google Colabをマウント\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#フォルダ設定\n",
        "#親フォルダ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#元画像フォルダ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#元画像をコピー\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#切りぬいた画像を保存するフォルダ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#トレーニングされたYOLOv5で切り抜いた画像を保存するフォルダ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSVファイルのフォルダ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Refresh folder (内容が削除されるので注意！！) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# parent_dirがあれば削除する\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# 新しくparent_dirを作成する\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirを新規に作成する\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dirにdataset_dir直下のファイルをすべてコピーする\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"処理が完了しました。\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "EnV1hEgIu87W",
        "outputId": "9a63ef42-ae9d-4448-d096-002d30eebd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# parent_dirがあれば削除する\\nif os.path.exists(parent_dir):\\n    shutil.rmtree(parent_dir)\\n\\n# 新しくparent_dirを作成する\\nos.makedirs(parent_dir)\\n\\n# orig_dir, out_dirを新規に作成する\\nos.makedirs(orig_dir)\\nos.makedirs(out_dir)\\nos.makedirs(cropped_dir)\\n\\n# orig_dirにdataset_dir直下のファイルをすべてコピーする\\nfile_list = os.listdir(dataset_dir)\\nfor filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\\n    src_path = os.path.join(dataset_dir, filename)\\n    dst_path = os.path.join(orig_dir, filename)\\n    shutil.copy(src_path, dst_path)\\n\\nprint(\"処理が完了しました。\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeを用いて目を検出**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**目が2つ以上検出されたものを抜き出す**\n",
        "\n",
        "dlibで検出されたものから、上下左右に0.1倍ずつ拡大した範囲を抜き出している"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #フォルダ数の分だけ\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # 画像グレースケール化\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix以上のもので目に見えるものを抽出\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # 眼検出判定\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #画像の切り抜きと保存（2個以上検出の時に限る）\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #本来の切り抜きより幅の0.1倍ずつ水増しする\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #横幅の半分より左にあるのは右眼\n",
        "                      else:\n",
        "                          side = \"L\" #横幅の半分よりより右にあるのは左眼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #対応表の作成\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ここで、目以外が誤検出されているものを手動で抜き出して削除する**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csvから、削除して画像のパスが存在しなくなっている行を削除する\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrameを読み込む\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# 存在しない画像パスをチェックし、そのリストを保持する\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# 存在しない画像パスを表示\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # 存在しない画像パスの行を削除\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # 更新されたDataFrameを保存する\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "35178cc0-37a3-4148-b3fd-fa12e141342a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframeの整理**\n",
        "\n",
        "・ hertel_dfを参照して、coordinates_dfにヘルテル値を記入する\n",
        "\n",
        "・idが\"16_R, 16_L\"という形式になるようにデータフレームを整理する"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6e553e1b-105b-4366-fd88-7dd9508f2c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-817e306f-678a-4ab3-a935-a6e022b87f2b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-817e306f-678a-4ab3-a935-a6e022b87f2b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-817e306f-678a-4ab3-a935-a6e022b87f2b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "画像パスの抽出（RLともに揃っているもの）\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "画像の分割 train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "フォルダの作成\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "画像のコピー\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d78c062-ef26-47b7-b3b2-e8be204b050a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|██████████| 760/760 [00:09<00:00, 82.08it/s]\n",
            "Copying valid images: 100%|██████████| 190/190 [00:02<00:00, 67.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\" {side} {cx} {cy} {w} {h}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# trainとvalidのディレクトリパス\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# trainディレクトリでラベルファイルを生成\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# validディレクトリでラベルファイルを生成\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e09a5a-a1ab-48cf-b03b-385a93882b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 760/760 [00:05<00:00, 132.46it/s]\n",
            "Processing images: 100%|██████████| 190/190 [00:01<00:00, 142.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## バウンディングボックスのサンプル描画 ##\n",
        "## (これは実行しなくて良い)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# バウンディングボックスを描画する関数\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            class_id, cx, cy, bw, bh = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得する関数\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# 画像パスとラベルファイルパス\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# 画像のサイズを取得\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "id": "OhSI3Wc_-ylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "4237beaa-b891-4844-fbf8-9e1204bc6422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5**"
      ],
      "metadata": {
        "id": "473ybkfvE_4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup YOLOv5"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5_iFish --> train_batchの精度が低いので一旦却下とした\n",
        "# %cd $out_dir\n",
        "# !git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationを実装したバージョン\n",
        "# %cd yolov5-iFish\n",
        "# %pip install -qr requirements.txt\n",
        "\n",
        "# import torch\n",
        "# import utils\n",
        "# display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "x-33rbP1-iQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "%cd $out_dir\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "vdYFiF39egKw",
        "outputId": "d6748592-94d0-408c-a413-b6ed0a67e7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train YOLOv5##"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "id": "7iRcXa8I-iSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 途中から\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/data.yaml --resume $out_dir/yolov5/runs/train/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = f\"{out_dir}/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)\n",
        "\n",
        "dst_pt = f\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2",
        "outputId": "50b709c2-ed57-4272-e70d-d3438424c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##YOLOv5 Inference original dataset"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d93b49-888a-4615-b755-12b658a234af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "#weight = f\"{out_dir}/dataset_uni/eyecrop-yolov5n-iFish_169epoch.pt\"\n",
        "shutil.copy(\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\", \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\")\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "\n",
        "# もともとのデータセット\n",
        "orig_dir = orig_dir #元画像\n",
        "cropped_dir = cropped_dir #YOLOv5で切り抜いた画像用\n",
        "\n",
        "# if os.path.exists(cropped_dir):\n",
        "#     shutil.rmtree(cropped_dir)\n",
        "# os.makedirs(cropped_dir)\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image作成、リサイズ\n",
        "    # 元の画像のサイズを取得\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # 正方形のサイズを決定（元の画像の長辺に合わせる）\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # 正方形のキャンバスを作成（背景は黒）\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # 元の画像を正方形の画像の中央に配置するための開始点（x,y）を計算\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # 元の画像を正方形のキャンバスにコピーする\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    # 変換した画像を保存\n",
        "    return letterbox_img_resized"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## バウンディングボックス&切り抜き demo ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    print(img)\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #バウンディングボックスが左に切れる場合の対処\n",
        "            x1 = 0\n",
        "        if y2>img_width: #バウンディングボックスが右に切れる場合の対処\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "P-J5WiSyXGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# ## 切り抜き + 保存 ##\n",
        "# #################\n",
        "# \"\"\"\n",
        "# Letterbox & 250px正方形にリサイズ\n",
        "# cropped_dirに保存\n",
        "# \"\"\"\n",
        "\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "# start_index = 0\n",
        "# end_index = len(os.listdir(orig_dir))\n",
        "\n",
        "# class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# for i in range(start_index, end_index):\n",
        "#     img = image_path[i]\n",
        "#     pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "#     img_cv2 = cv2.imread(img)\n",
        "#     img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "#     for bbox in pred[0]:\n",
        "#         x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "#         prob = bbox[4].item()\n",
        "#         class_name = class_names[bbox[5].item()]\n",
        "\n",
        "#         #print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "#         # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "#         img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "#         #print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "#         padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "#         padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "#         x1 = x1 - padding_x\n",
        "#         y1 = y1 - padding_y\n",
        "#         x2 = x2 - padding_x\n",
        "#         y2 = y2 - padding_y\n",
        "#         #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "#         # Crop and save the image\n",
        "#         mag = 640 / img_cv2.shape[1]\n",
        "#         cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "#         letterbox_img = make_letterbox_image(cropped_img)\n",
        "#         #cv2_imshow(letterbox_img)\n",
        "\n",
        "#         base_name = os.path.splitext(os.path.basename(img))[0]\n",
        "#         cropped_img_path = os.path.join(f\"{cropped_dir}/cropped_images\", f\"{base_name}_{class_name}.png\")\n",
        "#         cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "#         print(f\"succefully saved, image {i}: {cropped_img_path}\")\n"
      ],
      "metadata": {
        "id": "iAelqChiXGne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Load image paths\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 0\n",
        "end_index = len(image_paths)\n",
        "\n",
        "# Define class names\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# List to hold images with incorrect detections\n",
        "incorrect_detections = []\n",
        "\n",
        "# Iterate over images\n",
        "for i in range(start_index, end_index):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "    # Check if the detections are not equal to 2\n",
        "    if len(pred[0]) != 2:\n",
        "        incorrect_detections.append((img_path, len(pred[0])))\n",
        "        continue  # Skip the rest of the loop and do not process this image\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #バウンディングボックスが左に切れる場合の対処\n",
        "            x1 = 0\n",
        "        if y2>img_width: #バウンディングボックスが右に切れる場合の対処\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        # Save the cropped image\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        class_name = class_names[class_num]\n",
        "        cropped_img_path = os.path.join(f\"{cropped_dir}\", f\"{base_name}_{class_name}.png\")\n",
        "        cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "        print(f\"Successfully saved, image {i}: {cropped_img_path}\")\n",
        "\n",
        "# Output images with incorrect detections\n",
        "print(\"Images with incorrect detections:\")\n",
        "for img_path, num_detections in incorrect_detections:\n",
        "    print(f\"{img_path} - Number of detections: {num_detections}\")\n",
        "incorrect_paths = [path for path, _ in incorrect_detections]"
      ],
      "metadata": {
        "id": "WdowrfNNRhNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## うまく左右眼を検出できなかった例の確認 ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = [\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig/92.JPG\"]\n",
        "#image_path = incorrect_paths\n",
        "start_index = 0\n",
        "end_index = len(image_path)\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img_cv2_resized, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "PCBjJv1KlEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder\n",
        "%cd \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5\"\n",
        "folder_path = \"dataset_yolo_cropped\"\n",
        "\n",
        "# Zip the folder\n",
        "!zip -r /content/cropped_images.zip \"$folder_path\"\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('/content/cropped_images.zip')"
      ],
      "metadata": {
        "id": "vEepFKfW2HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To do\n",
        "\n",
        "・Group 5-fold split\n",
        "・MobileNetV3でcross validation --> 精度プロット作成\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_estimation_crossvalidation_noTestset.ipynb\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_ensemble_quick.ipynb\n",
        "・当院データセットでtestする\n",
        "'''"
      ],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "def letterbox_image(image_path, output_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Calculate padding sizes\n",
        "    h, w = image.shape[:2]\n",
        "    max_side = max(h, w)\n",
        "    top = bottom = (max_side - h) // 2\n",
        "    left = right = (max_side - w) // 2\n",
        "\n",
        "    # Add black padding to make the image square\n",
        "    square_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "    # Resize the image to 640x640\n",
        "    resized_image = cv2.resize(square_image, (1080, 1080))\n",
        "\n",
        "    # Save the processed image\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "def select_and_process_files(input_directory, output_directory, num_files=5):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # List all files in the input directory\n",
        "    all_files = [f for f in os.listdir(input_directory) if os.path.isfile(os.path.join(input_directory, f))]\n",
        "\n",
        "    # Randomly select num_files files\n",
        "    selected_files = random.sample(all_files, num_files)\n",
        "\n",
        "    # Copy and process selected files\n",
        "    for file in selected_files:\n",
        "        input_path = os.path.join(input_directory, file)\n",
        "        output_path = os.path.join(output_directory, file)\n",
        "        shutil.copy(input_path, output_path)\n",
        "        letterbox_image(output_path, output_path)\n",
        "\n",
        "    return selected_files\n",
        "\n",
        "# Example usage\n",
        "input_directory = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig'\n",
        "output_directory = '/content/letterbox'  # Replace with your desired output directory path\n",
        "if os.path.exists(output_directory):\n",
        "    shutil.rmtree(output_directory)\n",
        "os.makedirs(output_directory)\n",
        "selected_files = select_and_process_files(input_directory, output_directory)\n",
        "print(\"Selected and processed files:\", selected_files)\n"
      ],
      "metadata": {
        "id": "pUYctpy0gNIN",
        "outputId": "fd06c8dc-747a-4d25-9bf2-a1e8c2c62dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected and processed files: ['917.JPG', '42.JPG', '333.JPG', '46.JPG', '530.JPG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3 using cropped images**\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_yolo_cropped/cropped_images\n",
        "の画像を手動で確認、不適切な画像を削除\n",
        "\n",
        "・https://tcd-theme.com/2019/12/mac-zip-compression.html\n",
        "\n",
        "を参考にして圧縮\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_cropped_for_MobileNet_training/cropped_images.zipとしてアップロード"
      ],
      "metadata": {
        "id": "lvkfZ9aQ9S1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv5で抜き出した画像を規定のフォルダに移動"
      ],
      "metadata": {
        "id": "rKFHyFisE2Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5で抜き出した画像を規定のフォルダに移動\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# parent_folder = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training'\n",
        "\n",
        "# # zipファイルのパス\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/cropped_images.zip'\n",
        "\n",
        "# # zipファイルを解凍\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(parent_folder)\n"
      ],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ここから"
      ],
      "metadata": {
        "id": "vdOpequeE781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer --q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install pingouin --q\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True"
      ],
      "metadata": {
        "id": "q7Zp_wf_JQjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89944e89-b3f0-4003-e715-326eeb3f8d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRandom Seed:  1234\n",
            "Random Seed:  1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training\"\n",
        "# os.chdir(path)\n",
        "\n",
        "# contains train, val\n",
        "DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/dataset_yolo_cropped\"\n",
        "#DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_250px_uni_periocular\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_unilateral.csv\"\n",
        "LOG_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/log_multi.txt\"\n",
        "ROC_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/roc_multi.png\"\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# transforms param\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])])"
      ],
      "metadata": {
        "id": "8lzHTjGEL8eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5-Foldに分割"
      ],
      "metadata": {
        "id": "kW9iYtvtOPiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(DATASET_PATH)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"patiend num: {}\".format(len(id)))"
      ],
      "metadata": {
        "id": "eMO4j991OO8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6473425b-1b05-45fb-c5f1-8c98c8edcb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patiend num: 1986\n",
            "patiend num: 1986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testsetなし。Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=patient):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(path_list[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(path_list[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(train_set[0])[0]))"
      ],
      "metadata": {
        "id": "4QNmg_1gOO9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d47d23-2b46-4671-ce12-3bf07743199c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1588\n",
            "val_dataset: 398\n",
            "extracted_id (example): 870_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split ＋　Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9で分割\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "groups = patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "zjQNXUD2OO_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da839bf8-332d-4e3f-e7e3-f5375160e456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1431\n",
            "val_dataset: 358\n",
            "test_dataset: 197\n",
            "\n",
            "extracted_id (example): 563_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シンプルなK-fold (group_K_Foldではない)\n",
        "\n",
        "num_folds = 5 #number of folds\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "#まず全体の1割をテストセットとしてよけておく\n",
        "remain_set, test_set = train_test_split(path_list, test_size=0.1, shuffle=True, random_state=random_seed)\n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_set):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "JatpoewI699u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3c4782-6ce1-4274-d689-6778e6ae47c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1429\n",
            "val_dataset: 358\n",
            "test_dataset: 199\n",
            "\n",
            "extracted_id (example): 830_L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Datasets"
      ],
      "metadata": {
        "id": "_hiIomNwbBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "\n",
        "    def __init__(self, img_list, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for image_path in img_list:\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([image_path, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.item_paths[index][0]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image).float()\n",
        "        hertel = self.item_paths[index][1]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "#test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "#test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))"
      ],
      "metadata": {
        "id": "IteKm-r5brwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5016de0a-3ace-4d0a-99f1-219a83a4d8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1431\n",
            "val_dataset_size: 358\n",
            "train_dataset_size: 1588\n",
            "val_dataset_size: 398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test with early-stopping"
      ],
      "metadata": {
        "id": "JGWnnohkWBpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size,optimizer, patience, n_epochs, device,  alpha=0):\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "\n",
        "            #l2_normalization\n",
        "            l2 = torch.tensor(0., requires_grad=True)\n",
        "            for w in model.parameters():\n",
        "                l2 = l2 + torch.norm(w)**2\n",
        "            loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(n_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "\n",
        "        print(print_msg)\n",
        "\n",
        "\n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "iJyYRN8SOPB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ConvNetの調整"
      ],
      "metadata": {
        "id": "E2WdAbf9WMMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###オリジナルRepVGG-A2使用\n"
      ],
      "metadata": {
        "id": "w4uM5uvHV8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc1 = nn.Linear(in_features=1408, out_features=512) #out_featuresを1に\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep_learning/666mai_dataset/RepVGG-A2-train.pth\"))\n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "xooKuBLyV_xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Timm使用の場合"
      ],
      "metadata": {
        "id": "DLA4qqaoYeCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "efFP6EU1WH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc = nn.Linear(in_features=1280, out_features=1) #モデルに応じてin_featuresを調整、out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x) #dropoutを1層追加\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # fc layer 2つのバージョン\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc1 = nn.Linear(in_features=1280, out_features=512) #モデルに応じてin_featuresを調整\n",
        "#         self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Batch_norm plus dropout (for RepVGG_A2: イマイチ)\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.bn = nn.BatchNorm2d(1408)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "#         self.dropout = nn.Dropout(0.25)\n",
        "#         self.fc = nn.Linear(in_features=12672, out_features=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = self.bn(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.maxpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "#model_ft = timm.create_model(model_name = 'efficientnetv2_rw_m', pretrained=True, num_classes=1)\n",
        "#model_ft = timm.create_model(model_name = 'repvgg_a2', pretrained=True, num_classes=1)\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "#model_ft = mod_CNNModel()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "#optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "optimizer_ft =  optim.AdaBound(\n",
        "    model_ft.parameters(),\n",
        "    lr= 1e-3,\n",
        "    betas= (0.9, 0.999),\n",
        "    final_lr = 0.1,\n",
        "    gamma=1e-3,\n",
        "    eps= 1e-8,\n",
        "    weight_decay=5e-4,\n",
        "    amsbound=False,\n",
        ")"
      ],
      "metadata": {
        "id": "-VvPVPdJWIER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bcb424-1d0e-481e-8cc6-f1efb5fcf74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ranger_adabelief in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from ranger_adabelief) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->ranger_adabelief) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->ranger_adabelief) (1.3.0)\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)"
      ],
      "metadata": {
        "id": "o46zFFxQ7zZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Draw Learning Curves"
      ],
      "metadata": {
        "id": "nGCIPkQbd8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1\n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "HRqX-uCLWIIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation using testset"
      ],
      "metadata": {
        "id": "HFAFvtSxeBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in val_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "id": "gcZBDepNWIKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55297e7-906c-4bc7-e500-a98f7ffd0104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: -0.19647376860805493\n",
            "StdError: 1.7861947268910445\n",
            "AveAbsError: 1.3963950938315848\n",
            "StdAbsError: 1.1288754956800753\n",
            "\n",
            "Corrected_AveAbsError: 1.401375062314974\n",
            "Corrected_StdAbsError: 1.1053021335598956\n",
            "Round_Corrected_AveAbsError: 1.3869346733668342\n",
            "Round_Corrected_StdAbsError: 1.1881150114178518\n",
            "Probability of AbsError <= 1: 0.4371859296482412\n",
            "Probability of AbsError <= 2: 0.7562814070351759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using test dataset\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #平均誤差\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #誤差標準偏差\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #平均絶対誤差\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #絶対誤差標準偏差\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #平均絶対誤差(四捨五入)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #絶対誤差標準偏差(四捨五入)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)\n"
      ],
      "metadata": {
        "id": "8p5vrzYkJQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "qa1NMrMQJQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bland-Altman-Plot\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fs7DIPTJeVpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#線形近似式算出\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# 説明変数xに \"x1\"のデータを使用\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# 目的変数yに \"x2\"のデータを使用\n",
        "y = target.values\n",
        "\n",
        "# 予測モデルを作成（単回帰）\n",
        "clf.fit(x, y)\n",
        "\n",
        "# パラメータ（回帰係数、切片）を抽出\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# パラメータの表示\n",
        "print(\"回帰係数:\", a)\n",
        "print(\"切片:\", b)\n",
        "print(\"決定係数:\", clf.score(x, y))\n",
        "\n",
        "#平均値により補正した値\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#回帰直線により補正した値\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#残差\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#残差\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]"
      ],
      "metadata": {
        "id": "_1MEuRIReVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "heTFISEt9D_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#平均近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "\n",
        "# print(\"\")\n",
        "# print('Error<-1 and Error>-1: ' + str(sum((-1 < i < 1 for i in df['Residual_error_2']))))\n",
        "# print('Error<-2 and Error>2: ' + str(sum((-2 < i < 2 for i in df['Residual_error_2']))))\n",
        "# print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "# print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "total_errors = len(df['Residual_error_2'])\n",
        "\n",
        "error_minus1_to_1_count = sum((-1 < i < 1 for i in df['Residual_error_2']))\n",
        "error_minus1_to_1_percentage = (error_minus1_to_1_count / total_errors) * 100\n",
        "\n",
        "error_minus2_to_2_count = sum((-2 < i < 2 for i in df['Residual_error_2']))\n",
        "error_minus2_to_2_percentage = (error_minus2_to_2_count / total_errors) * 100\n",
        "\n",
        "error_less_equal_minus2_count = sum((i <= -2 for i in df['Residual_error_2']))\n",
        "error_less_equal_minus2_percentage = (error_less_equal_minus2_count / total_errors) * 100\n",
        "\n",
        "error_greater_equal_2_count = sum((i >= 2 for i in df['Residual_error_2']))\n",
        "error_greater_equal_2_percentage = (error_greater_equal_2_count / total_errors) * 100\n",
        "\n",
        "print(\"\")\n",
        "print(f'Error<-1 and Error>-1: {error_minus1_to_1_count}/{total_errors} ({error_minus1_to_1_percentage:.2f}%)')\n",
        "print(f'Error<-2 and Error>2: {error_minus2_to_2_count}/{total_errors} ({error_minus2_to_2_percentage:.2f}%)')\n",
        "print(f'Error<=-2: {error_less_equal_minus2_count}/{total_errors} ({error_less_equal_minus2_percentage:.2f}%)')\n",
        "print(f'Error>=2: {error_greater_equal_2_count}/{total_errors} ({error_greater_equal_2_percentage:.2f}%)')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1\n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1\n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')"
      ],
      "metadata": {
        "id": "cHjgGcbk9EBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference pipeline YOLOv5→MobileNetv3**"
      ],
      "metadata": {
        "id": "450sY_r6ZvZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_dir = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/HerteL_images_TED\"\n",
        "#imgs_dir = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/HerteL_images_TUMOR\"\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Img_Hertel_list_TED.csv\"\n",
        "# csv_path = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Img_Hertel_list_TUMOR.csv\"\n",
        "\n",
        "yolo_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "mobilenet_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_0.pth\""
      ],
      "metadata": {
        "id": "Cs3KMCqreVvN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List comprehension to create a list of file paths\n",
        "imgs_list = [os.path.join(imgs_dir, path) for path in os.listdir(imgs_dir)]\n",
        "img_paths = imgs_list[0:]  # Displaying the first five file paths"
      ],
      "metadata": {
        "id": "2NS0xdWJndml"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# 結果を格納するためのリスト\n",
        "labels_list = []\n",
        "\n",
        "# img_pathの各パスに対して\n",
        "for path in img_paths:\n",
        "    # パスからベースネーム（ファイル名）を抽出\n",
        "    base_name = os.path.basename(path)\n",
        "\n",
        "    # ベースネームに対応する行を見つける\n",
        "    row = df[df['file_name'] == base_name]\n",
        "\n",
        "    # \"Hertel_R\" と \"Hertel_L\" の値を取得し、リストに追加\n",
        "    if not row.empty:\n",
        "        labels_list.append(row[['file_name','Hertel_R', 'Hertel_L']].values[0].tolist())\n",
        "    else:\n",
        "        # 対応する行がない場合は、None または適切な値を追加\n",
        "        labels_list.append(None)\n",
        "        print(f\"not found in csv file, {base_name}\")\n",
        "\n",
        "# 新しいデータフレームを作成\n",
        "new_df = pd.DataFrame(labels_list, columns=['file_name', 'Hertel_R', 'Hertel_L'])\n",
        "\n",
        "# 予測値記録用にデータフレームに欄を作っておく\n",
        "new_df['pred_R'] = pd.NA\n",
        "new_df['pred_L'] = pd.NA\n",
        "\n",
        "# 新しいデータフレームを表示\n",
        "#print(new_df)"
      ],
      "metadata": {
        "id": "rj2aAK7YYvTu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "id": "1eDSSdSeTxCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Y8ElaId4Ct",
        "outputId": "ee286ca0-00d2-49ef-fa0a-7680fcfa0990"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "# state_dict をロードしてモデルに適用\n",
        "model_ft.load_state_dict(torch.load(mobilenet_weight))\n",
        "model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "metadata": {
        "id": "zEDchCiOttlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image作成、リサイズ\n",
        "    # 元の画像のサイズを取得\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # 正方形のサイズを決定（元の画像の長辺に合わせる）\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # 正方形のキャンバスを作成（背景は黒）\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # 元の画像を正方形の画像の中央に配置するための開始点（x,y）を計算\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # 元の画像を正方形のキャンバスにコピーする\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    return letterbox_img_resized\n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image):\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(250),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.1,saturation=0.1, hue=0.2),\n",
        "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#MobileNetv3の前処理\n",
        "def image_eval(image_tensor):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    pred = torch.round(output).int().item()\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)"
      ],
      "metadata": {
        "id": "tMyb62rOoD2F"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display  # For displaying images in Colab\n",
        "\n",
        "# Assuming yolo_weight and other necessary variables are defined elsewhere\n",
        "\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "def process_images(image_paths, labels_llist):\n",
        "    for i, (img_path, label) in enumerate(zip(image_paths, labels_list)):\n",
        "        pred = inference(img_path, yolo_weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "        img_cv2 = cv2.imread(img_path)\n",
        "        img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "        if len(pred[0]) != 2:\n",
        "            print(\"Number of eyes are not 2 in image:\", img_path)\n",
        "\n",
        "        for bbox in pred[0]:\n",
        "            x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "            # Adjust bounding box coordinates\n",
        "            img_height, img_width, _ = img_cv2_resized.shape\n",
        "            padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "            padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "            x1 = max(x1 - padding_x, 0)\n",
        "            y1 = max(y1 - padding_y, 0)\n",
        "            x2 = min(x2 - padding_x, img_width)\n",
        "            y2 = min(y2 - padding_y, img_height)\n",
        "\n",
        "            # Crop and resize logic\n",
        "            mag = 640 / img_cv2.shape[1]\n",
        "            cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "            #resized_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "            resized_img = cv2.resize(cropped_img,(250,250))\n",
        "\n",
        "            # Display image\n",
        "            #cv2_imshow(resized_img)\n",
        "\n",
        "            #MobileNet用の前処理\n",
        "            resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
        "            resized_img= Image.fromarray(resized_img)\n",
        "            image_tensor = image_transform(resized_img)\n",
        "            pred = image_eval(image_tensor)\n",
        "            if class_num == 0: #右眼\n",
        "                print(f\"file_name: {label[0]}, side: R, pred: {pred}, label: {label[1]}\")\n",
        "                new_df.at[i, \"pred_R\"] = pred\n",
        "            elif class_num == 1: #左眼\n",
        "                print(f\"file_name: {label[0]}, side: R, pred: {pred}, label: {label[2]}\")\n",
        "                new_df.at[i, \"pred_L\"] = pred\n",
        "# Example usage\n",
        "process_images(img_paths, labels_list)\n",
        "new_df"
      ],
      "metadata": {
        "id": "wQazEg8Tcivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# NAのある行を削除\n",
        "new_df = new_df.dropna()\n",
        "\n",
        "# scatteplot\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(data=df, x=\"Hertel_R\", y=\"pred_R\")\n",
        "# plt.title(\"Comparison of Hertel and Predicted Values\")\n",
        "# plt.xlabel(\"Hertel Value\")\n",
        "# plt.ylabel(\"Predicted Value\")\n",
        "# plt.show()\n",
        "\n",
        "# Adjusting the plot with specified axis limits and increments\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=combined_df, x=\"Hertel\", y=\"Predicted\", hue=\"Eye\")\n",
        "\n",
        "# Setting the axis limits\n",
        "plt.xlim(8, 25)\n",
        "plt.ylim(8, 25)\n",
        "\n",
        "# Setting the axis ticks\n",
        "plt.xticks(range(8, 26, 1))\n",
        "plt.yticks(range(8, 26, 1))\n",
        "\n",
        "# Setting the title and labels\n",
        "plt.title(\"Combined Analysis of Right and Left Eye Values\")\n",
        "plt.xlabel(\"Hertel Value\")\n",
        "plt.ylabel(\"Predicted Value\")\n",
        "plt.legend(title=\"Eye\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O1wDeiR-d4GU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "d407a46a-7c38-454b-f136-11ac6845062f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data provided by the user\n",
        "data = {\n",
        "    \"file_name\": [\n",
        "        \"129-20191002-76-162415_118f83a916863e06de64edf...\",\n",
        "        \"9129-20191002-49-140906_ac8b709121a742083607af...\",\n",
        "        \"6336-20190213-79-132839_e0f0869a867604753f5741...\",\n",
        "        \"818-20181130-35-090753_614e05ca1e8e501ca739873...\",\n",
        "        \"1910-20181114-49-103737_71dcc2f31d839f0c77b9f2...\"\n",
        "    ],\n",
        "    \"Hertel_R\": [18, 18, 19, 16, 14],\n",
        "    \"Hertel_L\": [19, 19, 17, 15, 14],\n",
        "    \"pred_R\": [14, 15, 15, 15, 14],\n",
        "    \"pred_L\": [12, 14, 13, 15, 14]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# # Plotting with seaborn\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(data=df, x=\"Hertel_R\", y=\"pred_R\")\n",
        "# plt.title(\"Comparison of Hertel and Predicted Values\")\n",
        "# plt.xlabel(\"Hertel Value\")\n",
        "# plt.ylabel(\"Predicted Value\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "FzEwxwJ4kfP7",
        "outputId": "0cfe324a-98fe-4bf7-8c5f-b3f90b102e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIlCAYAAAAjY+IAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTJ0lEQVR4nO3deVyU9f7//+cgi4CCIESgKGgquWelZam4p6nkVqLnoxzNsqzUrCN2CvWU0knNpUyt3E6pndKTpaKmhttRs8VTuYupUS6JbCoKyly/P/oxXycWZ5SrAX3cb7e53eB9va9rXnPNm3GeXtf1viyGYRgCAAAAAJQqN1cXAAAAAAA3I8IWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAlIKIiAhFR0e7ugxTpKWlaeDAgQoLC5PFYrlpX2dRLBaL4uLiXF1GscaPHy+LxaJjx465uhSnFfU3U9b/jsrC/j527JgsFovGjx/vshoAOI6wBcBhubm5mjt3rjp06KDg4GB5eHgoMDBQrVq10htvvKGMjAxXlwgTjB49Wv/+9781bNgwffDBB/r73/9eYn+LxaIOHToUuzwuLk4Wi0W//PJLaZcqSdq0aZPGjx+vzMxMU7ZfXhXs94JHhQoVFBQUpM6dO2vNmjWuLu+GrVixokwEkPT0dFWsWFFRUVEl9svIyJC3t/c1+wEo39xdXQCA8uHnn39W9+7d9cMPP+jBBx/UqFGjFBoaqszMTG3fvl2vvPKKli9frq+++srVpbrEwYMHZbFYXF2GKdavX6/OnTsrISHB1aU4ZNOmTZowYYLi4uJUpUoVV5dT5sycOVMBAQG6fPmyDh48qHfffVddu3bVkiVLFBsb69LabuTvaMWKFVq0aJHLA1dgYKB69+6tJUuWaNu2bXrwwQeL7Pfhhx/q0qVLevzxx//kCgH8mQhbAK4pNzdX3bp10759+7R48WL179/fbvmoUaP0yy+/6O2333ZRha6Rl5cnq9WqihUrysvLy9XlmObUqVMKDAx0dRnXdO7cOVWuXNnVZZR5PXv2VPXq1W2/9+7dW82bN9drr71WYtjKzs6Wn5+fqbXdLH9HQ4cO1ZIlS/T+++8XG7bmzZsnDw8PDRo06E+uDsCfidMIAVzT/Pnz9eOPP2rkyJGFglaB6tWr6/XXX7drO3DggPr166eQkBB5eXmpVq1aeuGFF5SdnW3Xb+HChbJYLNq4caMmTZqkWrVqqWLFimrSpInt9KZ9+/apW7du8vf3V5UqVRQXF6fz58/bbafgeop9+/bp+eefV7Vq1Wzb+eijjwrV/MUXXyg2Nla1a9eWt7e3/Pz81Lp1a61cubJQ34JTsM6ePasnnnhCoaGh8vb21s6dOyUVfa3Jzp071b17d4WFhcnLy0uhoaFq27atVqxYYdcvMzNTzz//vCIjI+Xl5aWQkBDFxsbq8OHDdv2uvlZjzZo1uu++++Tt7a3g4GA9+eSTunDhQpHvTVEceW8KXrNhGFq0aJHt9LOFCxc6/DzOOHfunP7+97+rXr168vLyUmBgoB555BH98MMPdv02bdpkq2Pu3Llq3LixKlasqGeffVbR0dGaMGGCJCkyMtJW89VHO/Ly8vTGG2+ocePGtve9Q4cO2rJlyw3VP3v2bHXu3FnVq1eXp6enbrvtNvXu3Vt79uwp1LdgvBw6dEgxMTHy9/dXpUqV1LVrV6WkpBS5b0aMGGEbd82aNdMnn3xyQ/UWuPfee1W1alXb8149zpYvX67mzZvLx8dHPXr0sK2ze/du9enTR7fddps8PT1Vq1YtxcfHKycnp9D2v/32W3Xo0EG+vr4KCAhQ7969i73mqbhrtn744QfFxsYqLCxMnp6eqlatmmJiYvTtt9/a1lu0aJEk2Z0qefVYdXR8FfS9kf0dHR2tOnXq6JNPPin0eSdJ33zzjb7//nvFxMQoODhYJ06c0AsvvKBmzZopMDBQXl5eqlu3rv7+97/r4sWL13y+q/8m/qi468xOnz6tZ599VhEREfL09FRISIj+8pe/FOqXm5urV199VfXr15evr6/8/PxUr149DR482KHagFsdR7YAXNPHH38sSRo2bJjD6/zvf/9T69atdeXKFT399NOqVauWtm3bpqlTp2rjxo3673//Kx8fH7t1xo4dq9zcXD311FOqUKGCZsyYoZiYGC1btkxDhgzRo48+qu7du2vHjh1atGiRvLy8NHfu3ELPPXDgQBmGoeeff165ublauHChYmNjdf78ebtTdhYuXKjTp0/rL3/5i6pXr64zZ85o0aJF6tGjhz766CM99thjhbbdoUMHVa1aVfHx8bJarbr99tuLfP2HDh1S+/btddttt+npp59WWFiY0tLS9O2332rHjh165JFHJP3+pe6BBx7Qvn37FBsbqwcffFBHjhzRO++8o7Vr1+q///2v6tevb7ftNWvW6O2339aTTz6puLg4bdy4Ue+++64sFovmzJlTau/Nk08+qQ4dOuj//u//1KpVKz3xxBOSpJYtW17zOS5fvqy0tLQil+Xm5hZqy87O1oMPPqiUlBQNGjRITZo0UUZGht577z3df//92rp1q5o1a2a3zowZM3T69GkNHTpU1atXV+XKlVW1alUFBgbq008/1bRp0xQUFCRJaty4sSTpypUr6tq1qzZv3qzY2FgNGzZMOTk5+vDDD9WuXTutWLFC3bp1u+brK8obb7yhFi1aaPjw4QoKCtLhw4f1/vvva/369dq9e7dq165t1//XX39V69at1aNHD/3zn//U4cOH9dZbbykmJkY//vij3NzcbDV36dJF//3vf9WzZ0+1b99eP//8swYPHqy6deteV61XO3PmjDIyMhQaGmrX/tlnn2n69OkaNmyYhg4dKsMwJElr167VI488ovDwcD377LMKCQnR999/rzfffFP//e9/lZycLHf3379efPfdd2rdurUqVKigZ599VuHh4friiy8UHR3t8H8OrFmzRj179pSnp6eGDBmiqKgonT17Vps3b9b27dt19913a/r06XrzzTe1detWffDBB7Z1C8aqM+OrtPb3448/rjFjxmjp0qV68skn7Za9//77kn4/Aib9HiaXLVumRx55RIMHD5ZhGNq0aZMSExO1e/duJSUlOfy8jkhNTVXLli11/vx5DRkyRHXr1tWvv/6q2bNn64svvtA333yjGjVqSJKeeeYZvf/++xowYICee+45SdLRo0e1atUqXbhwQd7e3qVaG3DTMQDgGqpWrWpUrlzZqXVatWplWCwWY9u2bXbtEyZMMCQZr776qq1twYIFhiSjSZMmxqVLl2ztu3fvNiQZFovF+Pe//223nZiYGMPDw8M4d+6crW3cuHGGJOPuu++2205mZqZRo0YNo3LlykZWVpat/fz584XqvnDhglGnTh2jfv36du2DBg0yJBn9+vUzrFZrofVq1qxptGnTxvb7jBkzDEnGzp07i9tFhmEYxiuvvGJIMiZOnGjXvmnTJkOS0b59e1vb0aNHDUmGt7e3ceTIEbv+nTt3Njw8PIp8TX/kzHtjGIYhyRg0aNA1t3t1f0ceqamptnVGjhxpeHh4FNpfGRkZRvXq1Y3o6GhbW3JysiHJqFKlinHy5MlCz18wDo4ePVpo2fTp0w1Jxn/+8x+79ry8POOuu+4yIiMjr/u1F7Xv9+zZY3h4eBhPP/20XXvNmjUNScaSJUvs2hMTEw1Jxrp162xt8+bNMyQZI0aMsOu7fft2w2KxFPta/6hgDP/www/GmTNnjBMnThjJycnG/fffb0gyXnrpJcMw/t84c3d3N3788Ue7bVy8eNG4/fbbjebNm9v9jRmGYSxbtsyQZCxcuNDW1qpVK8PNzc345ptv7Po++eSThiS7v5mC/XJ124ULF4zg4GDD39+/0Jg3DMPIz88v9PqK4sz4Kq39ffr0acPDw8O499577dovXLhg+Pn5GTVr1rTVn5OTY/daCvz97383JBm7du2ytRW8P+PGjbO1FfxNLFiwoNA2ivp7eOSRR4yAgIBC+/To0aNGpUqVjLi4OFtbQECA8dBDD13z9QIoGqcRArimrKwsp67VOHPmjLZu3aqOHTvqgQcesFv2wgsvyNfXV8uXLy+03vDhw+2u2WjatKn8/PwUGhqqRx991K5vmzZtdPny5SJPRxo9erTddvz9/TV8+HCdO3dO69evt7X7+vrafr5w4YLOnj2rnJwctWvXTvv27dO5c+cKbXvMmDEOXcBfMDHDihUrSjzVZvny5fLz89Pzzz9f6PW1bdtWX375ZaFZHnv27KlatWrZtXXs2FGXL1/W0aNHS6zret8bZ911111av359kY9OnTrZ9TUMQx9++KHuv/9+1a5dW2lpabbHlStX1KlTJ23durXQfhw0aFCxRxaL88EHHygiIkKtWrWye56srCz16NFDR48e1aFDh67rNReMJ8MwlJ2drbS0NIWEhKhevXpFThwTFhZW6Bqpjh07SpJdDQXvx0svvWTX9/7771f79u2drrNx48YKDg5WWFiY2rZtqx9//FEvvvii/vGPf9j1e/jhh9WwYUO7tg0bNujUqVOKi4vTuXPn7PZh69at5ePjo3Xr1kn6f2PtoYce0t133223nVdeecWhWr/44gudOXNGI0eOLDTmJdmO/pXE2fFVWvv7tttuU48ePfT111/rxx9/tLUXnFo4ZMgQW/3e3t62ny9fvqz09HSlpaXZxkNpTjyUlZWlzz//XF27dpWfn5/d/qhUqZLuu+8+23so/f5ZtnfvXn3//felVgNwK+E0QgDX5O/vX2TwKM5PP/0kSWrUqFGhZT4+Pqpdu7aOHDlSaFlRX6YCAgIUHh5eZLsknT17ttCyP552d3Xb1dfDHDt2TK+88oqSkpKUnp5eaJ2MjIxCEy44ehpRv379tHTpUr3++uuaNm2amjdvrtatW6tfv352X2B/+uknNWjQQBUrViy0jUaNGik5OVlHjx61vV6p6P1UtWpVSUXvj6td73vjrMDAwGKnf//www/tfi/4ordlyxYFBwcXu820tDS7sXA9p9Dt379fOTk5JT7P6dOnr2vbW7Zs0T/+8Q9t3769UDCMjIws1N/R9/HIkSMKCgrSbbfdVqh/gwYNtGHDBqfqXLp0qYKCglShQgVVqVJF9evXL3JiiqL2wf79+yVJTz/9tJ5++ukit3/69Glb3VLRf4/VqlWTv7//NWstCJ1/PIXUGc6Or9Lc30OHDtXy5cv1/vvva8aMGZJ+nxijQoUK+utf/2rrl5+frylTpmjhwoU6dOiQrFar3XaK+ny6XgXbX7x4sRYvXlxkn6tD7IwZM/R///d/atq0qWrUqKFWrVqpc+fO6tu3b5GfWwDsEbYAXFOjRo20adMmpaSk6I477jDteSpUqOBUuyTbdSTOOn/+vFq3bq2srCyNGDFCjRs3lp+fn9zc3DR//nwtXbq00BceSYWuMyuOp6en1qxZo++++07r1q3Ttm3bNG3aNE2aNEmTJ0/W6NGjr6tuyZz94UoF+7l169YlHvH44xdlR9+LPz5XvXr1Spw5849Hcxzx7bffqn379qpVq5YmTpyoWrVqycfHRxaLRSNGjCjy+iRXvY8PPvig3WyExSlq/xa8VxMnTlTz5s2LXO/q/xgoC653fJWGjh07qmbNmvrwww/1xhtv6Pjx49q6dasefvhhu/fghRde0PTp09WnTx+NGTPGNvHIr7/+qri4uCI/i65W0tH2K1eu2P1esK1HH33Uds1YSbp3765jx45p3bp12rRpkzZt2qTFixdrwoQJ2rFjhyn7DbiZELYAXFPfvn21adMmvfvuu3rjjTeu2b/gf+z37t1baNnFixf1008/mRra9u3bpyZNmhRqk2R73i+//FKpqamaN2+eBg8ebNf3vffeK7VamjVrZvtf+YyMDLVs2VIvvfSSnn32WXl6eqp27dpKSUlRbm5uoaMLe/bskcViKfKoyPVy9XtTlODgYFWpUkUZGRkl3gzZUSV98axbt65SU1MVHR1tm8ShNCxevFhXrlzRmjVrCh2xOnv27A0dAahdu7YOHjyo3377rdDRlqLeRzMVHO2qWLHiNd+rgglBCv72rvbrr78qKyvL4efbvXu33WyIRSnufXd2fJXm/nZzc9OQIUOUkJCgTz/9VLt375akQiFn0aJFatWqVaEZDx292XTBrRmKOgJWcDS7wB133CE3NzddvHjR4b+3KlWq6LHHHrNNGjRnzhw99dRTmjVrlsvvawaUdVyzBeCahgwZooYNG+rNN9/Uv//97yL7/Prrr4qPj5f0+5ebVq1aad26ddq1a5ddv6lTp+r8+fPq3bu3afVOnTrVbsa7rKwszZo1S5UqVbJdA1FwVOGPRxB++OGHQlOzX4+iZuILCAhQrVq1lJeXZzsts1evXsrKytJbb71l13fr1q368ssv1a5du1I9UuDq96Yobm5u+stf/qIff/zRNn33HxWcmuaISpUqSSr6i+fAgQOVkZGhiRMn3vDzXK248TRnzpzr3maBXr16SZImTZpk175jxw5t3LjxhrbtrM6dOyskJESTJ0/WqVOnCi2/cuWKbb8HBwfrwQcf1Nq1a/Xdd9/Z9Xvttdccer5OnTopODhY06dPL/L6zKuP+BT3vjs7vkp7fw8ePFgVKlTQnDlz9K9//UuhoaF6+OGH7fpUqFCh0Ni5fPmyEhMTHXqOyMhIeXh4FDrF8fDhw/r000/t2qpWraquXbtq9erVSk5OLnJ7BfsjPz+/0DWjkmzX4F3rtGUAHNkC4AAvLy+tXr1a3bp1U79+/fTOO++oS5cuCgkJUXZ2tnbs2KEVK1aoadOmtnVmzpyp1q1bq127dnrqqads04svWbJETZo0KTQhRGlr2bKl+vfvr7y8PC1YsEA///yz5syZY5vo44EHHlBoaKhGjx6tn376SREREdq/f7/ee+89NWrUyHb/nuv12muvae3aterWrZsiIyPl7u6uzZs3KykpSd26dbNdm/Piiy9q+fLlevHFF/X999+rZcuWtqnf/f39NXPmzBveF3/k6vemKBMnTtT27dsVFxenFStWqFWrVvL19dXPP/+sjRs3ytvbu9gvhn903333Sfp9MpMBAwaoYsWKatiwoRo2bKgRI0Zo48aNGj9+vLZs2aJOnTopMDBQqamp2r59u3766adCRwIc0atXL7355pvq0qWLnnjiCfn4+Gjbtm1at26dateuXehULmcMGjRI8+bN04wZM5SammqbinzWrFm66667CgUZM/n4+OiDDz5QTEyM7rzzTv31r39VVFSUzp07pyNHjug///mPXn/9dcXFxUmSpk2bptatWys6OlrDhw+3Tf2+e/du27T813q+BQsWqFevXmrSpIlt6veMjAxt3rxZXbp00bPPPivp9/f97bff1tNPP62HH35YHh4eatGihSIjI50aX6W9v6tVq6YuXbpo1apVkn6/xcUfj6r27dtXs2fPVp8+fdSpUyelp6dr8eLFDk+rXqlSJQ0ePFhz587VY489pnbt2tk+8xo3blzoP1bmzJmjBx98UB07dlT//v117733ys3NTcePH1dSUpLuueceLVy4UOfOnVNoaKi6d++upk2bKjQ0VCdOnNB7770nd3d3DRgwwKl9AdySXDYPIoBy59KlS8bs2bONtm3bGlWrVjXc3d2NgIAAo1WrVsaUKVOMzMxMu/779u0zHn30USMoKMjw8PAwatasaTz//POF+hVM/Z6cnFzoOf84FXRJ6xRMcbx3715j1KhRRmhoqOHp6Wk0atTIWLx4caFt/Pjjj0bXrl2NgIAAw8fHx7jvvvuMzz77rMipkkuaVrqoOpOTk43HHnvMiIiIMLy9vQ0/Pz+jcePGxj//+U8jJyfHbt309HRj5MiRRs2aNQ0PDw8jKCjI6Nevn3Hw4EG7fkVN+VzS/iiJo++NYVzf1O9XT1n/RwX78uqp3w3j9+mvJ02aZDRp0sTw9vY2fH19jTvuuMMYMGCA3VToJU1zXeCf//ynERkZabi7uxfaZ1euXDHeeecdo0WLFkalSpWMihUrGhEREUavXr0K3WLAmdf++eefG/fcc4/h4+NjBAQEGN27dzf27t1rtGnTxqhZs6Zd3+LGdXHvcVZWlvHMM88YISEhhpeXl9G0aVPj448/LnGa+z8qbr87WsPV9u/fbwwaNMioXr26bczefffdxtixY42ff/7Zru+uXbuMtm3bGj4+Poa/v7/Rq1cv4+jRo0Xug+L2y7fffmv07t3bCA4ONjw8PIywsDCjZ8+exrfffmvrk5+fb4wePdqoVq2a4ebmVmiMODq+DKN09vfVPvvsM9ttLFJSUgotz8nJMcaMGWPUrFnT8PT0NCIiIoyxY8ca+/fvL/ReFPf+nD9/3hg2bJgRFBRkVKxY0bjnnnuMlStXFltzenq6ER8fb0RFRRleXl5G5cqVjaioKGPo0KG2KfJzc3ONsWPHGi1atDCCgoIMT09Po3r16kafPn2Mr776yql9ANyqLIZRDq+mBoAijB8/XhMmTNDRo0cVERHh6nIAAMAtjmu2AAAAAMAEhC0AAAAAMAFhCwAAAABMwDVbAAAAAGACjmwBAAAAgAkIWwAAAABgAm5q7ACr1aoTJ06ocuXKslgsri4HAAAAgIsYhqFz584pLCxMbm4lH7sibDngxIkTCg8Pd3UZAAAAAMqI1NRUVa9evcQ+hC0HVK5cWdLvO9TPz8/F1QAAAABwlezsbIWHh9syQkkIWw4oOHXQz8+PsAUAAADAocuLmCADAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwgcvDVkpKioYNG6amTZvK3d1dDRs2LNQnOjpaFoul0OPAgQPX3P6JEyfUu3dvVa5cWYGBgXr88ceVnZ1txksBAAAAABt3Vxewd+9erV69Wi1atJDVapXVai2y3wMPPKApU6bYtUVERJS47cuXL6tz586SpCVLlignJ0cvvPCC+vfvr1WrVpVK/QAAAABQFJeHre7duysmJkaSFBcXp2+++abIflWqVNF9993n1LaXLVumvXv3av/+/apXr54kKSAgQJ07d9auXbvUvHnzGyseAG4yWTl5Sjufp+xLl+Xn7aEgX0/5+3i6uiwANxE+Z+Cs8jxmXB623NzMO5NxzZo1aty4sS1oSVLHjh0VGBiopKQkwhYAXOVE5kWNWf6Dth5Os7W1rhOk13s3VlgVbxdWBuBmwecMnFXex4zLr9ly1ObNm+Xr66uKFSuqTZs22rJlyzXXOXDggKKiouzaLBaLoqKiHLreCwBuFVk5eYX+MZOkLYfTFL/8B2Xl5LmoMgA3Cz5n4KybYcyUi7DVpk0bzZgxQ2vXrtWiRYuUk5OjDh06aMeOHSWul5GRoSpVqhRqDwgIUHp6erHr5ebmKjs72+4BADeztPN5hf4xK7DlcJrSzpf9f9AAlG18zsBZN8OYcflphI6YMGGC3e/dunVTgwYN9OqrryopKanUny8xMbHQcwLAzSz70uUSl5+7xnIAuBY+Z+Csm2HMlIsjW3/k6+urhx9+WN9++22J/QICApSVlVWoPSMjQ4GBgcWuN3bsWGVlZdkeqampN1wzAJRlfhU9Slxe+RrLAeBa+JyBs26GMVMuw5ajiro2yzAMHTx4sNC1XFfz8vKSn5+f3QMAbmZBlTzVuk5Qkcta1wlSUKXyMesTgLKLzxk462YYM+UybF24cEGrVq3SvffeW2K/Ll266Pvvv9fhw4dtbRs3btTZs2fVtWtXs8sEgHLD38dTr/duXOgftdZ1gvTP3o3LzRS7AMouPmfgrJthzFgMwzBcWUBOTo7tuqtZs2bpyJEjevPNNyX9PjHGgQMHNHnyZPXs2VMRERE6ceKEpk6dqr1792rr1q226duPHz+u2rVrKyEhQQkJCZJ+v6lxs2bNZLFYNGnSJNtNjRs3buzUTY2zs7Pl7++vrKwsjnIBuKkV3Mvk3KXLqlzRQ0GVys+9TACUD3zOwFllbcw4kw1cPkHGb7/9pr59+9q1FfyenJys6tWrKy8vTy+99JLOnj0rX19ftWzZUnPmzLG7T5ZhGMrPz5fVarW1eXh4aO3atXruuecUGxsrd3d39erVS9OmTftzXhwAlDP+PnzpAWAuPmfgrPI8Zlx+ZKs84MgWAAAAAMm5bFAur9kCAAAAgLKOsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAlcHrZSUlI0bNgwNW3aVO7u7mrYsGGJ/VesWCGLxXLNfgW2bdumtm3bKiAgQEFBQerSpYv+97//lULlAAAAAFA8l4etvXv3avXq1brjjjtUv379EvtevHhRo0aNUkhIiEPbPnjwoDp16iRfX18tXbpU8+bNU3p6utq3b69Tp06VRvkAAAAAUCSXh63u3bsrNTVVy5YtU7NmzUrsm5iYqBo1auihhx5yaNuffvqpDMPQJ598ooceekgxMTH66KOPlJ6ervXr15dG+QAAAABQJJeHLTc3x0o4cuSIpk6dqpkzZzq87cuXL8vLy0sVK1a0tfn7+0uSDMNwrlAAAAAAcILLw5ajRowYoYEDB6pJkyYOr9OvXz9duXJFL7/8ss6ePasTJ05o1KhRCg8PV0xMjInVAgAAALjVubu6AEesXLlS27dv16FDh5xar06dOtq4caNiYmI0adIkSVJERIQ2bNhgO8JVlNzcXOXm5tp+z87Ovr7CAQAAANyyyvyRrUuXLmnkyJGaMGGCgoKCnFr30KFD6t27tzp16qT169dr5cqVqlmzprp06aLTp08Xu15iYqL8/f1tj/Dw8Bt9GQAAAABuMWU+bE2fPl1ubm6KjY1VZmamMjMzlZeXJ6vVavu5OC+99JJuv/12/etf/1KHDh3UrVs3rVq1ShkZGZoxY0ax640dO1ZZWVm2R2pqqhkvDQAAAMBNrMyfRnjgwAGlpKQoODi40LKAgADNnj1bw4YNK3Ldffv26f7777drq1Spku644w4dOXKk2Of08vKSl5fXjRUOAAAA4JZW5sNWfHy84uLi7Npef/11HTx4UAsWLFDdunWLXbdmzZravXu3DMOQxWKR9Pv1V4cPH1bbtm3NLBsAAADALc7lYSsnJ0dJSUmSpOPHjys7O1vLli2TJLVp00ZRUVGKioqyW2fhwoX65ZdfFB0dbWs7fvy4ateurYSEBCUkJEiShg0bpkceeUQDBgzQwIEDdenSJU2dOlW5ubl6/PHH/5wXCAAAAOCW5PKw9dtvv6lv3752bQW/Jycn2wWqkhiGofz8fFmtVltbTEyMPv74Y02ePFmPPfaYPD09dddddyk5OVl16tQptdcAAAAAAH9kMbi77zVlZ2fL399fWVlZ8vPzc3U5AAAAAFzEmWxQ5mcjBAAAAIDyiLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACY4IbCVmpqqrZv364LFy6UVj0AAAAAcFO4rrD17rvvqlq1aoqIiFCrVq108OBBSVLPnj01Y8aMUi0QAAAAAMojp8PW9OnT9eyzz2rgwIFat26dDMOwLYuOjtYnn3xSqgUCAAAAQHnk7uwKb731ll555RW9/PLLys/Pt1tWr14921EuAAAAALiVOX1k69dff1XLli2LXObh4aHz58/fcFEAAAAAUN45HbZq1qypXbt2Fbnsq6++Ut26dW+4KAAAAAAo75wOW0OHDtVrr72mefPmKTs7W5J0+fJlrV69WpMnT9aTTz5Z6kUCAAAAQHljMa6e4cJBzz33nGbNmiWLxSKr1So3t98z29NPP62ZM2eWepGulp2dLX9/f2VlZcnPz8/V5QAAAABwEWeywXWFLUn66aeftH79ep09e1aBgYFq37696tSpc10Fl3WELQAAAADSnxS2biWELQAAAACSc9nA6anft2zZcs0+rVu3dnh7KSkpmjJlinbu3Kk9e/YoKipKe/bsKbb/ihUr1LNnTzVo0KDEfldbvXq1Jk6cqO+//16enp5q2rSpPvjgA1WvXt3hOgEAAADAGU6HrejoaFksFrubGVssFrs+f7z/Vkn27t2r1atXq0WLFrJarbJarcX2vXjxokaNGqWQkBCHt//hhx9qyJAhGj16tCZOnKhz585p69atunTpksPbAAAAAABnOR22du/eXagtIyND69at0/LlyzV37lyntte9e3fFxMRIkuLi4vTNN98U2zcxMVE1atRQZGRkif0KpKena/jw4Zo+fbqeeuopW3uPHj2cqhEAAAAAnOV02GrSpEmR7dHR0fLx8dHcuXPVtm1bh7dXMJPhtRw5ckRTp07V9u3bNW3aNIfW+fjjj5Wfn68hQ4Y4XA8AAAAAlAan77NVkpYtWyopKak0N2kzYsQIDRw4sNiwV5SdO3cqKipKixYtUs2aNeXu7q6mTZtqzZo1ptQIAAAAAAWcPrJVkhUrVigwMLA0NylJWrlypbZv365Dhw45td6pU6d08OBBvfLKK3rjjTcUGhqqWbNmqUePHvrf//6nBg0aFLlebm6ucnNzbb8X3LwZAAAAABzldNgq6nqnvLw8HTx4UD///LPeeOONUimswKVLlzRy5EhNmDBBQUFBTq1rtVp1/vx5LV682FZ3dHS06tatq3/+85/617/+VeR6iYmJmjBhwg3XDgAAAODW5fRphNnZ2Tp37pzdw2KxqEOHDkpKStLo0aNLtcDp06fLzc1NsbGxyszMVGZmpvLy8mS1Wm0/FycgIECS1K5dO1ubh4eHWrdurb179xa73tixY5WVlWV7pKamlt4LAgAAAHBLcPrI1qZNm0woo3gHDhxQSkqKgoODCy0LCAjQ7NmzNWzYsCLXLe40QUklTv3u5eUlLy8v54sFAAAAgP9fqU6QYYb4+HglJyfbPTp37qyIiAglJyeXOI17t27dJEkbNmywteXl5Wnz5s26++67Ta8dAAAAwK3LoSNbzz33nMMbtFgsmjFjhsP9c3JybDMYHj9+XNnZ2Vq2bJkkqU2bNoqKilJUVJTdOgsXLtQvv/yi6OhoW9vx48dVu3ZtJSQkKCEhQZLUrFkz9e7dW0888YTS09NtE2ScPn1aL774osM1AgAAAICzHApbK1eudHiDzoat3377TX379rVrK/g9OTnZLlCVxDAM5efny2q12rUvWrRIY8eOVXx8vLKzs3X33Xdrw4YNatSokcM1AgAAAICzLIZhGK4uoqzLzs6Wv7+/srKy5Ofn5+pyAAAAALiIM9mgzF+zBQAAAADl0XXf1DglJUWHDh0qcla/Xr163VBRAAAAAFDeOR22srOz1bNnT9sU8AVnIVosFluf/Pz80qkOAAAAAMopp08jHDNmjE6dOqWtW7fKMAx9+umn2rRpk4YMGaLIyEjt3LnTjDoBAAAAoFxxOmytXbtWf//739WiRQtJUlhYmFq3bq13331XMTExmjp1aqkXCQAAAADljdNh67ffflN4eLgqVKggX19fnT171rasa9euWrt2bakWCAAAAADlkdNhKzw8XGlpaZKkOnXq6PPPP7ct27FjhypWrFh61QEAAABAOeX0BBkdO3bUhg0b1LNnT40aNUqDBg3SV199JU9PT+3atUujR482o04AAAAAKFccuqnxpk2bFB0dLUnKyclRTk6OgoKCJEmffvqpli1bposXL6pjx4568skn5eZ2c92+i5saAwAAAJCcywYOhS03NzdVq1ZNsbGxGjBggJo0aVJqxZYHhC0AAAAAknPZwKFDUJ999platWql2bNnq1mzZmrQoIEmTZqko0ePlkrBAAAAAHCzcejIVoGcnBx99tlnWrJkib744gtduXJF9913nwYMGKBHH33UdmrhzYYjWwAAAAAkE45sFfDx8VFsbKxWrlypU6dOac6cOfLy8tJzzz2nsLAwPfzww1qyZMkNFQ8AAAAANwOnjmwV5+TJk0pMTNSsWbMkSfn5+TdcWFnCkS0AAAAAknPZwOmp36925swZ/fvf/9bSpUu1Y8cOeXh4qEuXLjeySQAAAAC4KTgdts6dO6fly5dr6dKlSk5OVn5+vlq3bq25c+eqT58+CggIMKNOAAAAAChXHApbubm5WrVqlZYsWaI1a9bo0qVLatq0qSZNmqTY2FhVq1bN7DoBAAAAoFxxKGzddtttOn/+vCIjIzV69Gj1799fd955p9m1AQAAAEC55VDYGjRokPr376/77rvP7HoAAAAA4KbgUNiaOXOm2XUAAAAAwE3FqftsAQAAAAAcQ9gCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATODQbISRkZGyWCwOb/Snn3667oIAAAAA4GbgUNiKiYmxC1vLli1Tdna2OnTooJCQEJ0+fVobNmyQv7+/+vTpY1qxAAAAAFBeOBS2pk+fbvt58uTJCg8P19q1a+Xn52drz8rKUpcuXRQSElLqRQIAAABAeeP0NVszZ87U2LFj7YKWJPn7+ys+Pl5vvfVWqRUHAAAAAOWV02ErPT1dWVlZRS7LyspSRkbGDRcFAAAAAOWd02Grffv2GjNmjDZv3mzXvmnTJsXHx6t9+/alVhwAAAAAlFdOh625c+cqLCxM7dq1U2BgoOrVq6fAwEC1b99eoaGhmjNnjhl1AgAAAEC54tAEGVcLDQ3V119/rbVr12rXrl06efKkQkND1bx5cz300ENm1AgAAAAA5Y7FMAzD1UWUddnZ2fL391dWVlahiUEAAAAA3DqcyQZOn0ZYYO3atXr11Vf1xBNP6Oeff5YkbdmyRSdOnLjeTQIAAADATcPp0wjPnDmjRx55RDt37lR4eLhSU1M1bNgw1ahRQ/Pnz5evr69mzZplRq0AAAAAUG44fWRr5MiROnPmjPbs2aOUlBRdfRZihw4dtHHjxlItEAAAAADKI6ePbK1evVrvvfee7rzzTuXn59stCw8P1y+//FJqxQEAAABAeeX0ka0rV67I19e3yGUZGRny9PS84aIAAAAAoLxzOmy1aNFC8+fPL3LZRx99pAceeOCGiwIAAACA8s7p0whfe+01tW3bVq1bt1afPn1ksVi0YsUKJSYmavXq1dq2bZsZdQIAAABAueL0ka37779fycnJslgsGj16tAzD0MSJE3Xy5Elt3LhRzZo1M6NOAAAAAChXbuimxhcvXlRGRoaqVKkiHx+f0qyrTOGmxgAAAAAkk29qPHjwYB09elSS5O3trbCwMFvQOn78uAYPHnwdJQMAAADAzcXpsLVw4UKdOXOmyGVpaWlatGjRDRcFAAAAAOWd02FLkiwWS5Hthw8fVtWqVW+oIAAAAAC4GTg0G+Hs2bM1e/ZsSb8Hrf79+8vb29uuz6VLl3Ts2DH17du39KsEAAAAgHLGobAVFhamu+++W5K0Z88e1atXT8HBwXZ9PD09deedd2rIkCGlXyUAAAAAlDMOha2YmBjFxMTYfn/llVdUq1Yt04oCAAAAgPLO6ZsaL1iwwIw6AAAAAOCmcl1Tvz/22GNFLuvXr5+eeOKJGy4KAAAAAMo7p8PW+vXr1atXryKX9e7dW+vWrbvhogAAAACgvHM6bJ05c6bQ5BgFqlatqtOnT99wUQAAAABQ3jkdtqpVq6avvvqqyGVfffWVQkNDb7goAAAAACjvnA5bsbGxmjhxoj7++GO79k8++USTJk1S//79S604AAAAACivLIZhGM6skJeXp169eikpKUm+vr4KDQ3VyZMnlZOToy5duug///mPPD09zarXJbKzs+Xv76+srCz5+fm5uhwAAAAALuJMNnB66ndPT0+tWrVK69ev15dffqmzZ8+qatWq6tChg9q3b3/dRQMAAADAzcTpI1u3Io5sAQAAAJBMOLKVnp6uKlWqyM3NTenp6dfsHxgY6FilAAAAAHCTcihsBQcHa8eOHWrevLmCgoJksVhK7J+fn18qxQEAAABAeeVQ2Jo/f75q165t+/laYQsAAAAAbnVcs+UArtkCAAAAIDmXDZy+zxYAAAAA4NocOo0wMjLSqVMHf/rpp+suCAAAAABuBg6FrZiYGLuwtWzZMmVnZ6tDhw4KCQnR6dOntWHDBvn7+6tPnz6mFQsAAAAA5YVDYWv69Om2nydPnqzw8HCtXbvW7hzFrKwsdenSRSEhIaVeJAAAAACUN05fszVz5kyNHTu20MVg/v7+io+P11tvvVVqxQEAAABAeeV02EpPT1dWVlaRy7KyspSRkXHDRQEAAABAeed02Grfvr3GjBmjzZs327Vv2rRJ8fHxat++fakVBwAAAADlldNha+7cuQoLC1O7du0UGBioevXqKTAwUO3bt1doaKjmzJljRp0AAAAAUK44NEHG1UJDQ/X1119r7dq12rVrl06ePKnQ0FA1b95cDz30kBk1AgAAAEC5YzEMw3BlASkpKZoyZYp27typPXv2KCoqSnv27Cm2/4oVK9SzZ081aNCgxH5/ZLVade+99+q7777TJ5984tQU9c7cJRoAAADAzcuZbOD0ka0Ca9eu1ddff63U1FS9/PLLqlGjhrZs2aI77rhDYWFhDm9n7969Wr16tVq0aCGr1Sqr1Vps34sXL2rUqFHXNb383Llz9euvvzq9HgAAAABcD6ev2Tpz5oweeOABPfzww5o3b57mzZuntLQ0SdL8+fM1ceJEp7bXvXt3paamatmyZWrWrFmJfRMTE1WjRg2nT1dMS0vTyy+/rMTERKfWAwAAAIDr5XTYGjlypM6cOaM9e/YoJSVFV5+F2KFDB23cuNG5AtwcK+HIkSOaOnWqZs6c6dT2JWns2LFq27at2rZt6/S6AAAAAHA9nD6NcPXq1Xrvvfd05513Kj8/325ZeHi4fvnll1Ir7mojRozQwIED1aRJE6fW27Vrl5YsWaK9e/eaUhcAAAAAFMXpsHXlyhX5+voWuSwjI0Oenp43XNQfrVy5Utu3b9ehQ4ecWs9qtWr48OEaPXq0IiIidOzYMYfWy83NVW5uru337Oxsp54XAAAAAJw+jbBFixaaP39+kcs++ugjPfDAAzdc1NUuXbqkkSNHasKECQoKCnJq3ffff1+nTp1SfHy8U+slJibK39/f9ggPD3dqfQAAAABwOmy99tprWrVqlVq3bq1Zs2bJYrFoxYoV6tu3rz7//HNNmDChVAucPn263NzcFBsbq8zMTGVmZiovL09Wq9X2c1HOnz+vl156SS+//LLy8vKUmZlpO0KVk5NT4tGqsWPHKisry/ZITU0t1dcEAAAA4OZ3XffZ2rFjh+Lj47V9+3bl5+fLYrHo/vvv1+TJk3X//fdfdzFxcXH65ptv7O6fFRcXp0WLFhW7zuzZszVs2LBC7ceOHVNkZGSx64WEhOjUqVMO1cV9tgAAAABIJt5nKy8vT6tWrVLTpk21efNmXbx4URkZGapSpYp8fHxuqOjixMfHKy4uzq7t9ddf18GDB7VgwQLVrVu3yPVuv/12JScn27WdOnVKsbGxGj9+vDp27GhKvQAAAAAgORm2PD091b9/f61du1a1atWSt7e3vL29b6iAnJwcJSUlSZKOHz+u7OxsLVu2TJLUpk0bRUVFKSoqym6dhQsX6pdfflF0dLSt7fjx46pdu7YSEhKUkJCgihUr2i2XZJsgo0GDBmrZsuUN1Q0AAAAAJXF6NsKoqCj9/PPPpVbAb7/9pr59+9q1FfyenJxcKDAVxzAM5efny2q1llptAAAAAHC9nL5ma82aNRoxYoSWLFmie+65x6y6yhSu2QIAAAAgmXjNliT97W9/09mzZ9WiRQtVrVpVISEhslgstuUWi0Xff/+981UDAAAAwE3E6bB199133zJHtAAAAADgejkdthYuXGhCGQAAAABwc3E4bO3bt09z5szR0aNHVa1aNfXp00cdOnQwszYAAAAAKLccmiBj27Zt6tChgy5fvqzg4GCdPXtWVqtVs2bNKvKGwjcbJsgAAAAAIDmXDdwc2eC4ceMUFRWlY8eO6dSpUzp79qweeeQRvfzyy6VSMAAAAADcbBwKWz/++KMSEhIUHh4uSfLz89PUqVOVnp6u1NRUUwsEAAAAgPLIobCVlpam6tWr27UVBK+0tLTSrwoAAAAAyjmHwpYku3tpAQAAAABK5vBshG3btpWbW+Fs1qpVK7t2i8WirKys0qkOAAAAAMoph8LWuHHjzK4DAAAAAG4qDk39fqtj6ncAAAAAkglTvwMAAAAAnEPYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABC4PWykpKRo2bJiaNm0qd3d3NWzYsMT+K1askMViuWY/SdqwYYP69euniIgI+fj4qH79+po8ebIuX75cWuUDAAAAQJHcXV3A3r17tXr1arVo0UJWq1VWq7XYvhcvXtSoUaMUEhLi0Lbnzp2rnJwc/eMf/1CNGjW0c+dOjRs3Tvv27dOCBQtK6yUAAAAAQCEWwzAMVxZgtVrl5vb7Aba4uDh988032rNnT5F9ExIStHnzZkVGRpbYr0BaWpqCgoLs2iZNmqSXX35Zv/32W6FlxcnOzpa/v7+ysrLk5+fn0DoAAAAAbj7OZAOXn0ZYELSu5ciRI5o6dapmzpzp8LaLClN33XWXDMPQyZMnHd4OAAAAADjL5WHLUSNGjNDAgQPVpEmTG9rOtm3b5OXlpcjIyFKqDAAAAAAKc/k1W45YuXKltm/frkOHDt3Qdg4fPqwZM2Zo2LBhqlSpUrH9cnNzlZuba/s9Ozv7hp4XAAAAwK2nzB/ZunTpkkaOHKkJEyY4fI1VUbKzs9WrVy9FRkZq4sSJJfZNTEyUv7+/7REeHn7dzwsAAADg1lTmw9b06dPl5uam2NhYZWZmKjMzU3l5ebJarbafryUvL089e/ZURkaGkpKS5OvrW2L/sWPHKisry/ZITU0trZcDAAAA4BZR5k8jPHDggFJSUhQcHFxoWUBAgGbPnq1hw4YVu77VatWAAQP07bffauvWrQ4dpfLy8pKXl9cN1Q0AAADg1lbmw1Z8fLzi4uLs2l5//XUdPHhQCxYsUN26dUtcf/jw4Vq5cqXWrVunRo0amVgpAAAAAPw/Lg9bOTk5SkpKkiQdP35c2dnZWrZsmSSpTZs2ioqKUlRUlN06Cxcu1C+//KLo6Ghb2/Hjx1W7dm0lJCQoISFB0u/31JozZ45efPFFeXl5aefOnbb+9evX555ZAAAAAEzj8rD122+/qW/fvnZtBb8nJyfbBaqSGIah/Px8Wa1WW9sXX3whSZo8ebImT55s19+ZbQMAAACAsyyGYRiuLqKsc+Yu0QAAAABuXs5kgzI/GyEAAAAAlEeELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATODysJWSkqJhw4apadOmcnd3V8OGDUvsv2LFClkslmv2K3DixAn17t1blStXVmBgoB5//HFlZ2eXRukAAAAAUCx3Vxewd+9erV69Wi1atJDVapXVai2278WLFzVq1CiFhIQ4tO3Lly+rc+fOkqQlS5YoJydHL7zwgvr3769Vq1aVSv0AAAAAUBSXh63u3bsrJiZGkhQXF6dvvvmm2L6JiYmqUaOGIiMjS+xXYNmyZdq7d6/279+vevXqSZICAgLUuXNn7dq1S82bNy+dF/EnycrJU9r5PGVfuiw/bw8F+XrK38fT1WUBAAA4jO8zuJW4PGy5uTl2JuORI0c0depUbd++XdOmTXNonTVr1qhx48a2oCVJHTt2VGBgoJKSkspV2DqReVFjlv+grYfTbG2t6wTp9d6NFVbF24WVAQAAOIbvM7jVuPyaLUeNGDFCAwcOVJMmTRxe58CBA4qKirJrs1gsioqK0oEDB0q7RNNk5eQV+mCSpC2H0xS//Adl5eS5qDIAAADH8H0GtyKXH9lyxMqVK7V9+3YdOnTIqfUyMjJUpUqVQu0BAQFKT08vdr3c3Fzl5ubafnf1hBpp5/MKfTAV2HI4TWnn8zj8DgAAyjS+z+BWVOaPbF26dEkjR47UhAkTFBQU9Kc8Z2Jiovz9/W2P8PDwP+V5i5N96XKJy89dYzkAAICr8X0Gt6IyH7amT58uNzc3xcbGKjMzU5mZmcrLy5PVarX9XJyAgABlZWUVas/IyFBgYGCx640dO1ZZWVm2R2pqaqm8luvlV9GjxOWVr7EcAADA1fg+g1tRmQ9bBw4cUEpKioKDgxUQEKCAgAAtXbpU+/fvV0BAgObPn1/sukVdm2UYhg4ePFjoWq6reXl5yc/Pz+7hSkGVPNW6TtFH9VrXCVJQJQ65AwCAso3vM7gVlfmwFR8fr+TkZLtH586dFRERoeTkZPXo0aPYdbt06aLvv/9ehw8ftrVt3LhRZ8+eVdeuXf+M8kuFv4+nXu/duNAHVOs6Qfpn78ac3wwAAMo8vs/gVmQxDMNwZQE5OTlKSkqSJM2aNUtHjhzRm2++KUlq06aNgoODC61TcD+uPXv22NqOHz+u2rVrKyEhQQkJCZJ+v6lxs2bNZLFYNGnSJNtNjRs3buzUTY2zs7Pl7++vrKwslx7lKrgvxblLl1W5ooeCKnFfCgAAUL7wfQblnTPZwOWzEf7222/q27evXVvB78nJyYqOjnZoO4ZhKD8/X1ar1dbm4eGhtWvX6rnnnlNsbKzc3d3Vq1cvh+/TVdb4+/BhBAAAyje+z+BW4vIjW+VBWTmyBQAAAMC1nMkGZf6aLQAAAAAojwhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACdxdXUB5YBiGJCk7O9vFlQAAAABwpYJMUJARSkLYcsC5c+ckSeHh4S6uBAAAAEBZcO7cOfn7+5fYx2I4EslucVarVSdOnFDlypVlsVhcWkt2drbCw8OVmpoqPz8/l9aC8oExA2cxZuAsxgycxZiBs8rSmDEMQ+fOnVNYWJjc3Eq+KosjWw5wc3NT9erVXV2GHT8/P5cPNJQvjBk4izEDZzFm4CzGDJxVVsbMtY5oFWCCDAAAAAAwAWELAAAAAExA2CpnvLy8NG7cOHl5ebm6FJQTjBk4izEDZzFm4CzGDJxVXscME2QAAAAAgAk4sgUAAAAAJiBsAQAAAIAJCFsAAAAAYALCVhmUkpKiYcOGqWnTpnJ3d1fDhg1L7L9ixQpZLJZr9sPNy5ExEx0dLYvFUuhx4MABF1QMV3P0cyYzM1PPPfecwsLCVLFiRdWuXVtTp079k6tFWXCtMXPs2LEiP2MsFosqVqzooqrhSo58zuTk5Gjs2LGqVauWfHx8VLduXU2aNElXrlxxQcVwNUfGTF5ensaMGaOwsDB5e3urefPm2rhxowuqdQw3NS6D9u7dq9WrV6tFixayWq2yWq3F9r148aJGjRqlkJCQP7FClDWOjpkHHnhAU6ZMsWuLiIj4EypEWePImLlw4YKio6Pl7u6uadOmKSQkRIcOHVJ2drYLKoarXWvMhIaGaseOHXZthmHooYceUrt27f7MUlFGOPI588wzz2j58uWaNGmS6tevrx07dighIUEXLlzQxIkTXVA1XMmRMTNy5Ej961//0sSJE1WvXj0tWLBAXbt21Y4dO9SsWTMXVF0yZiMsg6xWq9zcfj/oGBcXp2+++UZ79uwpsm9CQoI2b96syMjIEvvh5ubImImOjlalSpW0atUqV5SIMsaRMfPKK69oyZIl+uGHH+Tr6+uKMlGGOPNvU4FNmzapbdu2+vjjj9W3b98/o0yUIdcaM1arVZUrV9aLL76o8ePH29oHDRqkbdu26ciRI392yXCxa42ZX3/9VTVr1tS0adP07LPPSvr9P3WaNGmiyMhIffbZZy6puyScRlgGFQyyazly5IimTp2qmTNnmlwRyjpHxwxQwJEx8/7772vw4MEELUi6vs+ZJUuWyM/PT927dzehIpR11xozhmHoypUr8vf3t2v39/cXxwJuTdcaMz/88IPy8/PVqVMnW5vFYlGnTp20bt065eXlmV2i0/iGVo6NGDFCAwcOVJMmTVxdCsqJzZs3y9fXVxUrVlSbNm20ZcsWV5eEMurYsWM6deqUgoKC1KNHD3l5eSkwMFBDhw7V+fPnXV0eyoHLly9r+fLl6tmzJ9dsoUgVKlRQXFyc3n77bX399dc6f/68NmzYoA8++EDPPPOMq8tDGXTp0iVJKnRjYy8vL+Xm5uro0aOuKKtEXLNVTq1cuVLbt2/XoUOHXF0Kyok2bdpo4MCBqlOnjk6cOKEpU6aoQ4cO2rx5s+6//35Xl4cy5tSpU5KkF154Qb169VJSUpIOHz6s+Ph4nT9/XkuXLnVxhSjr1qxZo/T0dPXv39/VpaAMe+eddzRs2DA1b97c1jZ27Fg9//zzLqwKZVWdOnUkSbt27bK75nznzp2SpPT0dFeUVSLCVjl06dIljRw5UhMmTFBQUJCry0E5MWHCBLvfu3XrpgYNGujVV19VUlKSi6pCWVVwUXLdunW1aNEiSVL79u3l7u6uoUOHauLEiapVq5YrS0QZt3jxYoWEhKh9+/auLgVlWHx8vFavXq33339fderU0c6dOzVhwgQFBAToxRdfdHV5KGMaNmyoVq1aacyYMQoPD1fdunW1YMECbd68WdLvpxSWNYStcmj69Olyc3NTbGysMjMzJf0+DabValVmZqZ8fHzk6enp2iJR5vn6+urhhx/WsmXLXF0KyqCAgABJUtu2be3aC7447927l7CFYp0/f14rV67U0KFDVaFCBVeXgzJqz549mjJlij7//HPbdX2tW7fW5cuX9corr2jYsGGqXLmyi6tEWbNo0SI9+uijatmypSSpZs2aSkhI0Lhx4xQaGuri6grjmq1y6MCBA0pJSVFwcLACAgIUEBCgpUuXav/+/QoICND8+fNdXSKAcq527dqFzom/WsF580BRPv30U128eJFTCFGiffv2SZKaNm1q137XXXcpNzdXv/zyiwuqQlkXGRmpr7/+WkePHtXevXt15MgReXt7KzQ0VDVr1nR1eYVwZKscio+PV1xcnF3b66+/roMHD2rBggWqW7euawpDuXLhwgWtWrVK9957r6tLQRnk6empTp06FbpR5Pr16yWpTN7LBGXHkiVLVLt2bbVo0cLVpaAMK/hi/N133yk8PNzW/u2338pisZTJL84oOwqu2bp48aLmzZunxx9/3LUFFYOwVQbl5OTYrqE5fvy4srOzbad6tWnTRlFRUYqKirJbZ+HChfrll18UHR39Z5eLMuBaY+bAgQOaPHmyevbsqYiICJ04cUJTp07VqVOn9Mknn7iydLjItcZMcHCwxo0bp5YtW2rAgAEaNGiQDh8+rLFjx2rAgAGqXbu2K8uHCzgyZiTpzJkz2rBhg+Lj411WK8qGa42Ze+65R/fcc4+efPJJnT59WnfccYe++uorJSYmavDgwfLx8XFl+XABRz5n3n77bfn7+ys8PFzHjh3Tm2++qYoVK2rMmDGuLL14Bsqco0ePGpKKfCQnJxe5zqBBg4wGDRr8uYWizLjWmDl8+LDRuXNn4/bbbzc8PDyMKlWqGF27djW++uorV5cOF3H0c2bDhg3GPffcY3h5eRm33367MXr0aOPSpUuuKxwu4+iYefvttw1Jxr59+1xXLMoER8bMyZMnjccff9yoWbOm4e3tbdStW9cYN26ckZOT49ri4RKOjJkpU6YYtWrVMjw9PY3Q0FBj+PDhRnp6umsLL4HFMLhrHAAAAACUNibIAAAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCALjE+PHjValSJaeXXY9jx45p/PjxOnHixHWtHx0drW7duhW7vHv37qpTp06xy9966y1ZLBYdOXLEoeezWCyaMmWK03UCAMoWwhYA4KZ37NgxTZgw4brD1rX0799fKSkp+vrrr4tcvnTpUt13332qXbu2Kc8PACibCFsAgJuWYRjKzc01/XliYmJUqVIlLVmypNCyY8eOaceOHerfv7/pdQAAyhbCFgCgXMjNzdVLL72kmjVrysvLS3feeWehcBMXF6eGDRsqKSlJTZo0kZeXl1auXKm2bdtKku69915ZLBZZLBbbOpmZmXr66acVGhoqLy8v3X333friiy+cqs3Hx0cxMTH6+OOPZbVa7ZYtXbpUFSpU0GOPPaaTJ09q8ODBqlWrlry9vVWnTh299NJL1wyEEREReuaZZ+zaVqxYIYvFomPHjjm1jwAAfx53VxcAALi1XblypVDbHwOLJD366KPatm2bxo0bpzvvvFNJSUn6y1/+ooCAAHXp0sXW78SJE3ruuef08ssvq0aNGgoKCtKsWbM0fPhwLViwQFFRUba+eXl56tixo06fPq2JEyeqWrVq+vDDD/Xwww/ru+++U6NGjRx+Hf3799fixYu1adMmtWvXzta+ZMkSdezYUbfddpt+/PFHBQYG6s0331RAQIAOHTqk8ePH6+TJk1qwYIHDz1UcR/cRAODPQdgCALjMhQsX5OHhUeQyX19f28/Jycn6/PPPtW7dOnXq1EmS1LFjR508eVLjxo2zCxIZGRlas2aNWrRoYWtLT0+XJDVs2FD33HOPrX3x4sX63//+p++//17169eXJHXu3FmHDx/Wq6++qo8//tjh19KpUycFBwdr6dKltrC1Z88e7dmzR3/7298kSY0aNbKb+OKBBx6Qr6+vBg0apFmzZsnHx8fh5/sjZ/YRAODPwWmEAACX8fb21tdff13oMXToULt+X3zxhQIDA9WuXTtduXLF9ujYsaN2796t/Px8W9+qVavaBa2SfPHFF2rUqJHq1q1baLvFTXZRHHd3d/Xt21fLly9XXl6epN9PIfTx8VHPnj0l/X4N2fTp01W/fn15e3vLw8NDAwYM0JUrV/TTTz859XxFvRZH9xEA4M/BkS0AgMu4ubnZHWkqsGrVKrvf09LSlJ6eXuxRsJMnT6p69eqSpJCQEIefPy0tTbt37y5yuxUqVHB4OwX69++vd955R2vXrlWPHj20dOlS9ejRwzaN/fTp0/XCCy/ob3/7m9q2bauAgAB9/fXXGj58uC5duuT08/3xtTi6jwAAfw7CFgCgzAsMDFRwcLCSkpKKXH7bbbfZfr568gtHttu4cWPNmzfvhmuUpJYtWyoiIkJLly7VbbfdpqNHj2rGjBm25Z988ol69OihxMREW9u+ffuuud2KFSvajpYVyMjIsPvdmX0EAPhzELYAAGVehw4d9MYbb8jT01ONGzd2en1PT09JKnT0qEOHDkpKSlJYWJjCwsJuuE6LxaLY2FjNmDFDPj4+qlq1qh566CHb8osXL9pqKbB48eJrbrd69erav3+/XdsfZ0y80X0EACh9hC0AQJnXsWNHde/eXQ899JD+9re/qXHjxrpw4YL27t2rlJQUvf/++yWuX7duXVWoUEHz58+Xu7u73N3ddc8992jgwIGaO3euoqOj9cILL6hu3brKzMzU7t27lZeXZ3cEylH9+/dXYmKiFixYoCeffNLutL6OHTtqxowZevvtt1W3bl19+OGHSklJueY2+/Tpo6eeekoTJkxQy5YtlZSUpB07dpTqPgIAlD7CFgCgXFi2bJlef/11vfPOOzp+/Lj8/f3VsGFD/fWvf73mugXTv7/xxhv64IMPdOXKFRmGIS8vL3355ZcaP368Jk6cqJMnTyooKEh33XWXnn766euqs2HDhmrcuLF++OGHQjcyTkhI0JkzZ5SQkCDp9xA1c+ZMde/evcRtPv744zpy5Ihmz56tadOmqV+/fkpMTCy0/RvZRwCA0mcxDMNwdREAAAAAcLNh6ncAAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAE/x/LhVnh/Il/jAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0nD3hWRd4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTaiZLdCd4KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLp0xvu8ndtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "-BHc32Av1b2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 5-fold crossvalidation #\n",
        "####################\n",
        "\n",
        "model_name = 'repvgg_a2'\n",
        "# model_name = 'efficientnetv2_rw_m'\n",
        "# model_name = 'mobilenetv3_large_100'\n",
        "\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "# Initialize lists to store AbsErrors for each fold\n",
        "abs_errors_fold = [[] for _ in range(5)]\n",
        "\n",
        "\n",
        "for fold in [0,1,2,3,4]:\n",
        "    # Define dataset & dataloader\n",
        "    train_dataset = Create_Datasets(train_set[fold], CSV_PATH, train_data_transforms)\n",
        "    val_dataset = Create_Datasets(val_set[fold], CSV_PATH, val_data_transforms)\n",
        "    #test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "    #test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "    #Define Model\n",
        "    model_ft = timm.create_model(model_name = model_name, pretrained=True, num_classes=1)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    # !pip install ranger_adabelief\n",
        "    # from ranger_adabelief import RangerAdaBelief\n",
        "    # optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "    # optimizer_ft =  optim.AdaBound(\n",
        "    #     model_ft.parameters(),\n",
        "    #     lr= 1e-3,\n",
        "    #     betas= (0.9, 0.999),\n",
        "    #     final_lr = 0.1,\n",
        "    #     gamma=1e-3,\n",
        "    #     eps= 1e-8,\n",
        "    #     weight_decay=5e-4,\n",
        "    #     amsbound=False,\n",
        "    # )\n",
        "\n",
        "    # Train model\n",
        "    EPOCH = 1\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n",
        "\n",
        "    #evaluation using validation dataset\n",
        "    val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "\n",
        "    model_ft.eval() # prep model for evaluation\n",
        "\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in val_loader:\n",
        "        target = target.view(len(target), 1)\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        target = target.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "        outputs.append(output[0].item())\n",
        "        targets.append(target[0].item())\n",
        "        #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "        errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "    AbsError = [abs(i) for i in errors]\n",
        "\n",
        "    # Append AbsError from current fold to the corresponding list\n",
        "    abs_errors_fold[fold].extend(AbsError)\n",
        "\n",
        "\n",
        "\n",
        "    print('AveError: '+str(statistics.mean(errors)))\n",
        "    print('StdError: '+str(statistics.stdev(errors)))\n",
        "    print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "    print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "    #平均からの差分を補正\n",
        "    corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "    corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "    corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "    round_output = [my_round(i) for i in outputs]\n",
        "    round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "    print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "    print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "    print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "    print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "    # Calculate the probabilities\n",
        "    abs_error_np = np.array(AbsError)\n",
        "    prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "    prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "    print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "    print('Probability of AbsError <= 2:', prob_less_than_2)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df_abs_errors = pd.DataFrame({\n",
        "    f'Fold_{i}': errors for i, errors in enumerate(abs_errors_fold)\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Draw Box_plot\n",
        "# Melting the DataFrame to long format suitable for Seaborn\n",
        "df_long = df_abs_errors.melt(var_name='Fold', value_name='AbsError')\n",
        "\n",
        "# Create the boxplot\n",
        "\n",
        "sns.boxplot(x='Fold', y='AbsError', data=df_long)\n",
        "# Adding titles and labels\n",
        "plt.title('Boxplot of Absolute Errors for Each Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Absolute Error')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_abs_errors.to_csv(f'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_{model_name}.csv', index=False)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "todo:\n",
        "dfを作成する。画像のpath、label、fold毎の判定結果\n",
        "dfから、\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3uAaRHyehpQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9aad00a8-a1cf-4b27-e952-783fdc37364d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-34e82c59d329>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#evaluation using validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-68273327f946>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, batch_size, optimizer, patience, n_epochs, device, alpha)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;31m# convert batch-size labels to batch-size x 1 tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m#target = target.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0d6a5e19d841>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpilr_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtensor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilr_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhertel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EeuEYbSvne-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fxsq2b9nfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEQIJyYUnfE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFGlh1_7nfHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZ2vGUF4nfMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --include 'coreml' --nms --weights $weight_path"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --include 'coreml' --weights $weight_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3bGseObGPLg",
        "outputId": "2e53b1c9-d7b5-42af-a380-625f4e4f2ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['coreml']\n",
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt with output shape (1, 25200, 7) (3.7 MB)\n",
            "scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
            "XGBoost version 2.0.1 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "2023-11-11 06:39:03.361674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-11 06:39:03.361740: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-11 06:39:03.361772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "TensorFlow version 2.14.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 7.1...\n",
            "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_targer' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats\n",
            "Tuple detected at graph output. This will be flattened in the converted model.\n",
            "Converting PyTorch Frontend ==> MIL Ops: 100% 607/609 [00:00<00:00, 3968.51 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100% 5/5 [00:00<00:00, 230.02 passes/s]\n",
            "Running MIL default pipeline: 100% 71/71 [00:01<00:00, 44.08 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100% 12/12 [00:00<00:00, 373.03 passes/s]\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export failure ❌ 6.6s: For an ML Program, extension must be .mlpackage (not .mlmodel). Please see https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats to see the difference between neuralnetwork and mlprogram model types.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFoB1lpJGPNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkxZXJEZGPPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxnOT6leGS9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-EigenCAM**\n",
        "\n",
        "https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
      ],
      "metadata": {
        "id": "YXt1zi1TGm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "DM7nTgGnjwYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "0xDMsB0GkMyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "FzxYfIoemXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "id": "DKEgZ-Laojx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from PIL import Image\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "def parse_detections(results):\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    detections = detections.to_dict()\n",
        "    boxes, colors, names = [], [], []\n",
        "\n",
        "    for i in range(len(detections[\"xmin\"])):\n",
        "        confidence = detections[\"confidence\"][i]\n",
        "        if confidence < 0.2:\n",
        "            continue\n",
        "        xmin = int(detections[\"xmin\"][i])\n",
        "        ymin = int(detections[\"ymin\"][i])\n",
        "        xmax = int(detections[\"xmax\"][i])\n",
        "        ymax = int(detections[\"ymax\"][i])\n",
        "        name = detections[\"name\"][i]\n",
        "        category = int(detections[\"class\"][i])\n",
        "        color = COLORS[category]\n",
        "\n",
        "        boxes.append((xmin, ymin, xmax, ymax))\n",
        "        colors.append(color)\n",
        "        names.append(name)\n",
        "    return boxes, colors, names\n",
        "\n",
        "\n",
        "def draw_detections(boxes, colors, names, img):\n",
        "    for box, color, name in zip(boxes, colors, names):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (xmin, ymin),\n",
        "            (xmax, ymax),\n",
        "            color,\n",
        "            2)\n",
        "\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n",
        "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
        "img = cv2.resize(img, (640, 640))\n",
        "rgb_img = img.copy()\n",
        "img = np.float32(img) / 255\n",
        "transform = transforms.ToTensor()\n",
        "tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "target_layers = [model.model.model.model[-2]]\n",
        "\n",
        "results = model([rgb_img])\n",
        "boxes, colors, names = parse_detections(results)\n",
        "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
        "Image.fromarray(detections)"
      ],
      "metadata": {
        "id": "i_WLp1kGjvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "・外部のデータセット（Treated）を洗い出し\n",
        "\n",
        "・内部のデータセットをさらに水増し\n",
        "\n",
        "・内部および外部データセットより、test用各100枚（grav50枚、cont50枚）を抜き出しておき、合体する\n",
        "\n",
        "・既存のYOLOv5を用いてbounding boxを抜き出し、新たにトレーニングする"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}