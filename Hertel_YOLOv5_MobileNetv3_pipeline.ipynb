{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Hertel_YOLOv5_MobileNetv3_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation YOLOv5-MobileNetv3 pipeline**\n",
        "\n",
        "Train YOLOv5\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlib„ÅßÁõÆ„Åå2„Å§Ê§úÂá∫„Åï„Çå„Çã„ÇÇ„ÅÆ„ÇíÊäú„ÅçÂá∫„Åô\n",
        "YOLOv5„ÇíÁî®„ÅÑ„Å¶Â∑¶Âè≥„Å®„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíË™çË≠ò„Åï„Åõ„Çã\n",
        "Êäú„ÅçÂá∫„Åó„ÅüÁîªÂÉè„Å´„Å§„ÅÑ„Å¶MobileNetV3„ÅßÂõûÂ∏∞Ôºà5-fold ensembleÔºâ„ÇíË°å„ÅÜ\n",
        "„Çπ„Éû„Éõ„Å´ÂÆüË£Ö\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###„ÅØ„Åò„ÇÅ„ÅÆË®≠ÂÆö"
      ],
      "metadata": {
        "id": "TEGv1MI4DnFM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47d530c-5e2d-4e32-e09c-abdc27bd2060"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 13 08:11:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362557b3-9966-413c-a3cd-0d8b5474752b"
      },
      "source": [
        "#ÊÆã„ÇäÊôÇÈñìÁ¢∫Ë™ç\n",
        "!cat /proc/uptime | awk '{printf(\"ÊÆã„ÇäÊôÇÈñì : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÊÆã„ÇäÊôÇÈñì : 11.99"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd52f5a-fcd3-4da2-91ba-1e9e1305a696"
      },
      "source": [
        "'''\n",
        "Google Colab„Çí„Éû„Ç¶„É≥„Éà\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#„Éï„Ç©„É´„ÉÄË®≠ÂÆö\n",
        "#Ë¶™„Éï„Ç©„É´„ÉÄ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#ÂÖÉÁîªÂÉè„Éï„Ç©„É´„ÉÄ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#ÂÖÉÁîªÂÉè„Çí„Ç≥„Éî„Éº\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#Âàá„Çä„Å¨„ÅÑ„ÅüÁîªÂÉè„Çí‰øùÂ≠ò„Åô„Çã„Éï„Ç©„É´„ÉÄ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüYOLOv5„ÅßÂàá„ÇäÊäú„ÅÑ„ÅüÁîªÂÉè„Çí‰øùÂ≠ò„Åô„Çã„Éï„Ç©„É´„ÉÄ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSV„Éï„Ç°„Ç§„É´„ÅÆ„Éï„Ç©„É´„ÉÄ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Refresh folder (ÂÜÖÂÆπ„ÅåÂâäÈô§„Åï„Çå„Çã„ÅÆ„ÅßÊ≥®ÊÑèÔºÅÔºÅ) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# parent_dir„Åå„ÅÇ„Çå„Å∞ÂâäÈô§„Åô„Çã\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# Êñ∞„Åó„Åèparent_dir„Çí‰ΩúÊàê„Åô„Çã\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dir„ÇíÊñ∞Ë¶è„Å´‰ΩúÊàê„Åô„Çã\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dir„Å´dataset_dirÁõ¥‰∏ã„ÅÆ„Éï„Ç°„Ç§„É´„Çí„Åô„Åπ„Å¶„Ç≥„Éî„Éº„Åô„Çã\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"Âá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "EnV1hEgIu87W",
        "outputId": "9a63ef42-ae9d-4448-d096-002d30eebd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# parent_dir„Åå„ÅÇ„Çå„Å∞ÂâäÈô§„Åô„Çã\\nif os.path.exists(parent_dir):\\n    shutil.rmtree(parent_dir)\\n\\n# Êñ∞„Åó„Åèparent_dir„Çí‰ΩúÊàê„Åô„Çã\\nos.makedirs(parent_dir)\\n\\n# orig_dir, out_dir„ÇíÊñ∞Ë¶è„Å´‰ΩúÊàê„Åô„Çã\\nos.makedirs(orig_dir)\\nos.makedirs(out_dir)\\nos.makedirs(cropped_dir)\\n\\n# orig_dir„Å´dataset_dirÁõ¥‰∏ã„ÅÆ„Éï„Ç°„Ç§„É´„Çí„Åô„Åπ„Å¶„Ç≥„Éî„Éº„Åô„Çã\\nfile_list = os.listdir(dataset_dir)\\nfor filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\\n    src_path = os.path.join(dataset_dir, filename)\\n    dst_path = os.path.join(orig_dir, filename)\\n    shutil.copy(src_path, dst_path)\\n\\nprint(\"Âá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascade„ÇíÁî®„ÅÑ„Å¶ÁõÆ„ÇíÊ§úÂá∫**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# „Ç´„Çπ„Ç±„Éº„Éâ„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# „Ç´„Çπ„Ç±„Éº„ÉâÂàÜÈ°ûÂô®„ÅÆÁâπÂæ¥ÈáèÂèñÂæó\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ÁõÆ„Åå2„Å§‰ª•‰∏äÊ§úÂá∫„Åï„Çå„Åü„ÇÇ„ÅÆ„ÇíÊäú„ÅçÂá∫„Åô**\n",
        "\n",
        "dlib„ÅßÊ§úÂá∫„Åï„Çå„Åü„ÇÇ„ÅÆ„Åã„Çâ„ÄÅ‰∏ä‰∏ãÂ∑¶Âè≥„Å´0.1ÂÄç„Åö„Å§Êã°Â§ß„Åó„ÅüÁØÑÂõ≤„ÇíÊäú„ÅçÂá∫„Åó„Å¶„ÅÑ„Çã"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #„Éï„Ç©„É´„ÉÄÊï∞„ÅÆÂàÜ„Å†„Åë\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # ÁîªÂÉè„Ç∞„É¨„Éº„Çπ„Ç±„Éº„É´Âåñ\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix‰ª•‰∏ä„ÅÆ„ÇÇ„ÅÆ„ÅßÁõÆ„Å´Ë¶ã„Åà„Çã„ÇÇ„ÅÆ„ÇíÊäΩÂá∫\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # ÁúºÊ§úÂá∫Âà§ÂÆö\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('ÁõÆ„Åå' + str(len(eye_list)) +'ÂÄãÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #ÁîªÂÉè„ÅÆÂàá„ÇäÊäú„Åç„Å®‰øùÂ≠òÔºà2ÂÄã‰ª•‰∏äÊ§úÂá∫„ÅÆÊôÇ„Å´Èôê„ÇãÔºâ\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #Êú¨Êù•„ÅÆÂàá„ÇäÊäú„Åç„Çà„ÇäÂπÖ„ÅÆ0.1ÂÄç„Åö„Å§Ê∞¥Â¢ó„Åó„Åô„Çã\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #Ê®™ÂπÖ„ÅÆÂçäÂàÜ„Çà„ÇäÂ∑¶„Å´„ÅÇ„Çã„ÅÆ„ÅØÂè≥Áúº\n",
        "                      else:\n",
        "                          side = \"L\" #Ê®™ÂπÖ„ÅÆÂçäÂàÜ„Çà„Çä„Çà„ÇäÂè≥„Å´„ÅÇ„Çã„ÅÆ„ÅØÂ∑¶Áúº\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #ÂØæÂøúË°®„ÅÆ‰ΩúÊàê\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**„Åì„Åì„Åß„ÄÅÁõÆ‰ª•Â§ñ„ÅåË™§Ê§úÂá∫„Åï„Çå„Å¶„ÅÑ„Çã„ÇÇ„ÅÆ„ÇíÊâãÂãï„ÅßÊäú„ÅçÂá∫„Åó„Å¶ÂâäÈô§„Åô„Çã**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csv„Åã„Çâ„ÄÅÂâäÈô§„Åó„Å¶ÁîªÂÉè„ÅÆ„Éë„Çπ„ÅåÂ≠òÂú®„Åó„Å™„Åè„Å™„Å£„Å¶„ÅÑ„ÇãË°å„ÇíÂâäÈô§„Åô„Çã\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrame„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# Â≠òÂú®„Åó„Å™„ÅÑÁîªÂÉè„Éë„Çπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„ÄÅ„Åù„ÅÆ„É™„Çπ„Éà„Çí‰øùÊåÅ„Åô„Çã\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# Â≠òÂú®„Åó„Å™„ÅÑÁîªÂÉè„Éë„Çπ„ÇíË°®Á§∫\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # Â≠òÂú®„Åó„Å™„ÅÑÁîªÂÉè„Éë„Çπ„ÅÆË°å„ÇíÂâäÈô§\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # Êõ¥Êñ∞„Åï„Çå„ÅüDataFrame„Çí‰øùÂ≠ò„Åô„Çã\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "35178cc0-37a3-4148-b3fd-fa12e141342a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframe„ÅÆÊï¥ÁêÜ**\n",
        "\n",
        "„Éª hertel_df„ÇíÂèÇÁÖß„Åó„Å¶„ÄÅcoordinates_df„Å´„Éò„É´„ÉÜ„É´ÂÄ§„ÇíË®òÂÖ•„Åô„Çã\n",
        "\n",
        "„Éªid„Åå\"16_R, 16_L\"„Å®„ÅÑ„ÅÜÂΩ¢Âºè„Å´„Å™„Çã„Çà„ÅÜ„Å´„Éá„Éº„Çø„Éï„É¨„Éº„É†„ÇíÊï¥ÁêÜ„Åô„Çã"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6e553e1b-105b-4366-fd88-7dd9508f2c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-817e306f-678a-4ab3-a935-a6e022b87f2b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-817e306f-678a-4ab3-a935-a6e022b87f2b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-817e306f-678a-4ab3-a935-a6e022b87f2b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ÁîªÂÉè„Éë„Çπ„ÅÆÊäΩÂá∫ÔºàRL„Å®„ÇÇ„Å´ÊèÉ„Å£„Å¶„ÅÑ„Çã„ÇÇ„ÅÆÔºâ\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "ÁîªÂÉè„ÅÆÂàÜÂâ≤ train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "„Éï„Ç©„É´„ÉÄ„ÅÆ‰ΩúÊàê\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "ÁîªÂÉè„ÅÆ„Ç≥„Éî„Éº\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d78c062-ef26-47b7-b3b2-e8be204b050a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760/760 [00:09<00:00, 82.08it/s]\n",
            "Copying valid images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [00:02<00:00, 67.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\" {side} {cx} {cy} {w} {h}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSV„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# train„Å®valid„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„Éë„Çπ\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# train„Éá„Ç£„É¨„ÇØ„Éà„É™„Åß„É©„Éô„É´„Éï„Ç°„Ç§„É´„ÇíÁîüÊàê\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# valid„Éá„Ç£„É¨„ÇØ„Éà„É™„Åß„É©„Éô„É´„Éï„Ç°„Ç§„É´„ÇíÁîüÊàê\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e09a5a-a1ab-48cf-b03b-385a93882b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760/760 [00:05<00:00, 132.46it/s]\n",
            "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [00:01<00:00, 142.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆ„Çµ„É≥„Éó„É´ÊèèÁîª ##\n",
        "## („Åì„Çå„ÅØÂÆüË°å„Åó„Å™„Åè„Å¶ËâØ„ÅÑ)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíÊèèÁîª„Åô„ÇãÈñ¢Êï∞\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            class_id, cx, cy, bw, bh = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# „É©„Éô„É´„Éï„Ç°„Ç§„É´„Åã„Çâ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆ„É™„Çπ„Éà„ÇíÂèñÂæó„Åô„ÇãÈñ¢Êï∞\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# ÁîªÂÉè„Éë„Çπ„Å®„É©„Éô„É´„Éï„Ç°„Ç§„É´„Éë„Çπ\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# ÁîªÂÉè„ÅÆ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# „É©„Éô„É´„Éï„Ç°„Ç§„É´„Åã„Çâ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆ„É™„Çπ„Éà„ÇíÂèñÂæó\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíÊèèÁîª\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "id": "OhSI3Wc_-ylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "4237beaa-b891-4844-fbf8-9e1204bc6422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5**"
      ],
      "metadata": {
        "id": "473ybkfvE_4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup YOLOv5"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5_iFish --> train_batch„ÅÆÁ≤æÂ∫¶„Åå‰Ωé„ÅÑ„ÅÆ„Åß‰∏ÄÊó¶Âç¥‰∏ã„Å®„Åó„Åü\n",
        "# %cd $out_dir\n",
        "# !git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentation„ÇíÂÆüË£Ö„Åó„Åü„Éê„Éº„Ç∏„Éß„É≥\n",
        "# %cd yolov5-iFish\n",
        "# %pip install -qr requirements.txt\n",
        "\n",
        "# import torch\n",
        "# import utils\n",
        "# display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "x-33rbP1-iQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "%cd $out_dir\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "vdYFiF39egKw",
        "outputId": "d6748592-94d0-408c-a413-b6ed0a67e7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 27.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train YOLOv5##"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "id": "7iRcXa8I-iSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÈÄî‰∏≠„Åã„Çâ\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/data.yaml --resume $out_dir/yolov5/runs/train/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.py„Çírename„Åó„Å¶gdrive„Å´ÁßªÂãï„Åó„Å¶„Åä„Åè\n",
        "orig_pt = f\"{out_dir}/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)\n",
        "\n",
        "dst_pt = f\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2",
        "outputId": "50b709c2-ed57-4272-e70d-d3438424c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##YOLOv5 Inference original dataset"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d93b49-888a-4615-b755-12b658a234af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 27.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "#weight = f\"{out_dir}/dataset_uni/eyecrop-yolov5n-iFish_169epoch.pt\"\n",
        "shutil.copy(\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\", \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\")\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "\n",
        "# „ÇÇ„Å®„ÇÇ„Å®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà\n",
        "orig_dir = orig_dir #ÂÖÉÁîªÂÉè\n",
        "cropped_dir = cropped_dir #YOLOv5„ÅßÂàá„ÇäÊäú„ÅÑ„ÅüÁîªÂÉèÁî®\n",
        "\n",
        "# if os.path.exists(cropped_dir):\n",
        "#     shutil.rmtree(cropped_dir)\n",
        "# os.makedirs(cropped_dir)\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image‰ΩúÊàê„ÄÅ„É™„Çµ„Ç§„Ç∫\n",
        "    # ÂÖÉ„ÅÆÁîªÂÉè„ÅÆ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # Ê≠£ÊñπÂΩ¢„ÅÆ„Çµ„Ç§„Ç∫„ÇíÊ±∫ÂÆöÔºàÂÖÉ„ÅÆÁîªÂÉè„ÅÆÈï∑Ëæ∫„Å´Âêà„Çè„Åõ„ÇãÔºâ\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # Ê≠£ÊñπÂΩ¢„ÅÆ„Ç≠„É£„É≥„Éê„Çπ„Çí‰ΩúÊàêÔºàËÉåÊôØ„ÅØÈªíÔºâ\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # ÂÖÉ„ÅÆÁîªÂÉè„ÇíÊ≠£ÊñπÂΩ¢„ÅÆÁîªÂÉè„ÅÆ‰∏≠Â§Æ„Å´ÈÖçÁΩÆ„Åô„Çã„Åü„ÇÅ„ÅÆÈñãÂßãÁÇπÔºàx,yÔºâ„ÇíË®àÁÆó\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # ÂÖÉ„ÅÆÁîªÂÉè„ÇíÊ≠£ÊñπÂΩ¢„ÅÆ„Ç≠„É£„É≥„Éê„Çπ„Å´„Ç≥„Éî„Éº„Åô„Çã\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    # Â§âÊèõ„Åó„ÅüÁîªÂÉè„Çí‰øùÂ≠ò\n",
        "    return letterbox_img_resized"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ&Âàá„ÇäÊäú„Åç demo ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250pxÊ≠£ÊñπÂΩ¢„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    print(img)\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"Ë®∫Êñ≠„ÅØ {class_name}„ÄÅÁ¢∫Áéá„ÅØ{prob * 100:.1f}%„Åß„Åô„ÄÇ\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅåÂ∑¶„Å´Âàá„Çå„ÇãÂ†¥Âêà„ÅÆÂØæÂá¶\n",
        "            x1 = 0\n",
        "        if y2>img_width: #„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅåÂè≥„Å´Âàá„Çå„ÇãÂ†¥Âêà„ÅÆÂØæÂá¶\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "P-J5WiSyXGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# ## Âàá„ÇäÊäú„Åç + ‰øùÂ≠ò ##\n",
        "# #################\n",
        "# \"\"\"\n",
        "# Letterbox & 250pxÊ≠£ÊñπÂΩ¢„Å´„É™„Çµ„Ç§„Ç∫\n",
        "# cropped_dir„Å´‰øùÂ≠ò\n",
        "# \"\"\"\n",
        "\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "# start_index = 0\n",
        "# end_index = len(os.listdir(orig_dir))\n",
        "\n",
        "# class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# for i in range(start_index, end_index):\n",
        "#     img = image_path[i]\n",
        "#     pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "#     img_cv2 = cv2.imread(img)\n",
        "#     img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\n",
        "#     for bbox in pred[0]:\n",
        "#         x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "#         prob = bbox[4].item()\n",
        "#         class_name = class_names[bbox[5].item()]\n",
        "\n",
        "#         #print(f\"Ë®∫Êñ≠„ÅØ {class_name}„ÄÅÁ¢∫Áéá„ÅØ{prob * 100:.1f}%„Åß„Åô„ÄÇ\")\n",
        "\n",
        "#         # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "#         img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "#         #print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "#         padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "#         padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "#         x1 = x1 - padding_x\n",
        "#         y1 = y1 - padding_y\n",
        "#         x2 = x2 - padding_x\n",
        "#         y2 = y2 - padding_y\n",
        "#         #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "#         # Crop and save the image\n",
        "#         mag = 640 / img_cv2.shape[1]\n",
        "#         cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅßÂàá„ÇäÊäú„Åç\n",
        "#         letterbox_img = make_letterbox_image(cropped_img)\n",
        "#         #cv2_imshow(letterbox_img)\n",
        "\n",
        "#         base_name = os.path.splitext(os.path.basename(img))[0]\n",
        "#         cropped_img_path = os.path.join(f\"{cropped_dir}/cropped_images\", f\"{base_name}_{class_name}.png\")\n",
        "#         cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "#         print(f\"succefully saved, image {i}: {cropped_img_path}\")\n"
      ],
      "metadata": {
        "id": "iAelqChiXGne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Load image paths\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 0\n",
        "end_index = len(image_paths)\n",
        "\n",
        "# Define class names\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# List to hold images with incorrect detections\n",
        "incorrect_detections = []\n",
        "\n",
        "# Iterate over images\n",
        "for i in range(start_index, end_index):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "    # Check if the detections are not equal to 2\n",
        "    if len(pred[0]) != 2:\n",
        "        incorrect_detections.append((img_path, len(pred[0])))\n",
        "        continue  # Skip the rest of the loop and do not process this image\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅåÂ∑¶„Å´Âàá„Çå„ÇãÂ†¥Âêà„ÅÆÂØæÂá¶\n",
        "            x1 = 0\n",
        "        if y2>img_width: #„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅåÂè≥„Å´Âàá„Çå„ÇãÂ†¥Âêà„ÅÆÂØæÂá¶\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        # Save the cropped image\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        class_name = class_names[class_num]\n",
        "        cropped_img_path = os.path.join(f\"{cropped_dir}\", f\"{base_name}_{class_name}.png\")\n",
        "        cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "        print(f\"Successfully saved, image {i}: {cropped_img_path}\")\n",
        "\n",
        "# Output images with incorrect detections\n",
        "print(\"Images with incorrect detections:\")\n",
        "for img_path, num_detections in incorrect_detections:\n",
        "    print(f\"{img_path} - Number of detections: {num_detections}\")\n",
        "incorrect_paths = [path for path, _ in incorrect_detections]"
      ],
      "metadata": {
        "id": "WdowrfNNRhNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## „ÅÜ„Åæ„ÅèÂ∑¶Âè≥Áúº„ÇíÊ§úÂá∫„Åß„Åç„Å™„Åã„Å£„Åü‰æã„ÅÆÁ¢∫Ë™ç ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250pxÊ≠£ÊñπÂΩ¢„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = [\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig/92.JPG\"]\n",
        "#image_path = incorrect_paths\n",
        "start_index = 0\n",
        "end_index = len(image_path)\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"Ë®∫Êñ≠„ÅØ {class_name}„ÄÅÁ¢∫Áéá„ÅØ{prob * 100:.1f}%„Åß„Åô„ÄÇ\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img_cv2_resized, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅßÂàá„ÇäÊäú„Åç\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "PCBjJv1KlEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder\n",
        "%cd \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5\"\n",
        "folder_path = \"dataset_yolo_cropped\"\n",
        "\n",
        "# Zip the folder\n",
        "!zip -r /content/cropped_images.zip \"$folder_path\"\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('/content/cropped_images.zip')"
      ],
      "metadata": {
        "id": "vEepFKfW2HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To do\n",
        "\n",
        "„ÉªGroup 5-fold split\n",
        "„ÉªMobileNetV3„Åßcross validation --> Á≤æÂ∫¶„Éó„É≠„ÉÉ„Éà‰ΩúÊàê\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_estimation_crossvalidation_noTestset.ipynb\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_ensemble_quick.ipynb\n",
        "„ÉªÂΩìÈô¢„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßtest„Åô„Çã\n",
        "'''"
      ],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "def letterbox_image(image_path, output_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Calculate padding sizes\n",
        "    h, w = image.shape[:2]\n",
        "    max_side = max(h, w)\n",
        "    top = bottom = (max_side - h) // 2\n",
        "    left = right = (max_side - w) // 2\n",
        "\n",
        "    # Add black padding to make the image square\n",
        "    square_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "    # Resize the image to 640x640\n",
        "    resized_image = cv2.resize(square_image, (1080, 1080))\n",
        "\n",
        "    # Save the processed image\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "def select_and_process_files(input_directory, output_directory, num_files=5):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # List all files in the input directory\n",
        "    all_files = [f for f in os.listdir(input_directory) if os.path.isfile(os.path.join(input_directory, f))]\n",
        "\n",
        "    # Randomly select num_files files\n",
        "    selected_files = random.sample(all_files, num_files)\n",
        "\n",
        "    # Copy and process selected files\n",
        "    for file in selected_files:\n",
        "        input_path = os.path.join(input_directory, file)\n",
        "        output_path = os.path.join(output_directory, file)\n",
        "        shutil.copy(input_path, output_path)\n",
        "        letterbox_image(output_path, output_path)\n",
        "\n",
        "    return selected_files\n",
        "\n",
        "# Example usage\n",
        "input_directory = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig'\n",
        "output_directory = '/content/letterbox'  # Replace with your desired output directory path\n",
        "if os.path.exists(output_directory):\n",
        "    shutil.rmtree(output_directory)\n",
        "os.makedirs(output_directory)\n",
        "selected_files = select_and_process_files(input_directory, output_directory)\n",
        "print(\"Selected and processed files:\", selected_files)\n"
      ],
      "metadata": {
        "id": "pUYctpy0gNIN",
        "outputId": "fd06c8dc-747a-4d25-9bf2-a1e8c2c62dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected and processed files: ['917.JPG', '42.JPG', '333.JPG', '46.JPG', '530.JPG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3 using cropped images**\n",
        "\n",
        "„Éªdataset_uni_for_YOLOv5/dataset_yolo_cropped/cropped_images\n",
        "„ÅÆÁîªÂÉè„ÇíÊâãÂãï„ÅßÁ¢∫Ë™ç„ÄÅ‰∏çÈÅ©Âàá„Å™ÁîªÂÉè„ÇíÂâäÈô§\n",
        "\n",
        "„Éªhttps://tcd-theme.com/2019/12/mac-zip-compression.html\n",
        "\n",
        "„ÇíÂèÇËÄÉ„Å´„Åó„Å¶ÂúßÁ∏Æ\n",
        "\n",
        "„Éªdataset_uni_for_YOLOv5/dataset_cropped_for_MobileNet_training/cropped_images.zip„Å®„Åó„Å¶„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ"
      ],
      "metadata": {
        "id": "lvkfZ9aQ9S1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv5„ÅßÊäú„ÅçÂá∫„Åó„ÅüÁîªÂÉè„ÇíË¶èÂÆö„ÅÆ„Éï„Ç©„É´„ÉÄ„Å´ÁßªÂãï"
      ],
      "metadata": {
        "id": "rKFHyFisE2Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5„ÅßÊäú„ÅçÂá∫„Åó„ÅüÁîªÂÉè„ÇíË¶èÂÆö„ÅÆ„Éï„Ç©„É´„ÉÄ„Å´ÁßªÂãï\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# parent_folder = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training'\n",
        "\n",
        "# # zip„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/cropped_images.zip'\n",
        "\n",
        "# # zip„Éï„Ç°„Ç§„É´„ÇíËß£Âáç\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(parent_folder)\n"
      ],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###„Åì„Åì„Åã„Çâ"
      ],
      "metadata": {
        "id": "vdOpequeE781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer --q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install pingouin --q\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffle„ÅÆ„Ç∑„Éº„Éâ\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True"
      ],
      "metadata": {
        "id": "q7Zp_wf_JQjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7ef5ea-1306-4473-b810-f7e284c0d891"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Random Seed:  1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training\"\n",
        "# os.chdir(path)\n",
        "\n",
        "# contains train, val\n",
        "DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/dataset_yolo_cropped\"\n",
        "#DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_250px_uni_periocular\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_unilateral.csv\"\n",
        "LOG_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/log_multi.txt\"\n",
        "ROC_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/roc_multi.png\"\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# transforms param\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])])"
      ],
      "metadata": {
        "id": "8lzHTjGEL8eQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5-Fold„Å´ÂàÜÂâ≤"
      ],
      "metadata": {
        "id": "kW9iYtvtOPiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(DATASET_PATH)\n",
        "\n",
        "#„Åù„Çå„Åû„Çå„ÅÆÈ†ÖÁõÆÔºàpath, classes, IDÔºâ„Çí„É™„Çπ„ÉàÂåñ\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"patiend num: {}\".format(len(id)))"
      ],
      "metadata": {
        "id": "eMO4j991OO8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443c02c3-cccb-4c59-be7f-ee11c1a8c7b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patiend num: 1986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testset„Å™„Åó„ÄÇGroup K-fold„ÇíÁî®„ÅÑ„Å¶„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂàÜ„Åë\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#foldÊï∞„Å†„ÅëÁ©∫„ÅÆ„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=patient):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(path_list[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(path_list[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(train_set[0])[0]))"
      ],
      "metadata": {
        "id": "4QNmg_1gOO9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca46918a-1975-4d38-eea2-1ccd24fceb78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1588\n",
            "val_dataset: 398\n",
            "extracted_id (example): 870_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split Ôºã„ÄÄGroup K-fold„ÇíÁî®„ÅÑ„Å¶„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂàÜ„Åë\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#foldÊï∞„Å†„ÅëÁ©∫„ÅÆ„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9„ÅßÂàÜÂâ≤\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "groups = patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "zjQNXUD2OO_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da839bf8-332d-4e3f-e7e3-f5375160e456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1431\n",
            "val_dataset: 358\n",
            "test_dataset: 197\n",
            "\n",
            "extracted_id (example): 563_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#„Ç∑„É≥„Éó„É´„Å™K-fold (group_K_Fold„Åß„ÅØ„Å™„ÅÑ)\n",
        "\n",
        "num_folds = 5 #number of folds\n",
        "\n",
        "#foldÊï∞„Å†„ÅëÁ©∫„ÅÆ„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "#„Åæ„ÅöÂÖ®‰Ωì„ÅÆ1Ââ≤„Çí„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Å®„Åó„Å¶„Çà„Åë„Å¶„Åä„Åè\n",
        "remain_set, test_set = train_test_split(path_list, test_size=0.1, shuffle=True, random_state=random_seed)\n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_set):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "JatpoewI699u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3c4782-6ce1-4274-d689-6778e6ae47c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1429\n",
            "val_dataset: 358\n",
            "test_dataset: 199\n",
            "\n",
            "extracted_id (example): 830_L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Datasets"
      ],
      "metadata": {
        "id": "_hiIomNwbBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "\n",
        "    def __init__(self, img_list, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for image_path in img_list:\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0] #„Éï„Ç©„É´„ÉÄ„Çà„ÇäÁîªÂÉèÁï™Âè∑„ÇíÊäú„ÅçÂá∫„Åô\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV‰∏ä„Åß‰∏ÄËá¥„Åó„ÅüÁï™Âè∑„ÅÆÁîªÂÉè„Å´„Å§„ÅÑ„Å¶HertelÂÄ§„ÇíÊäú„ÅçÂá∫„Åô\n",
        "            self.item_paths.append([image_path, hertel]) #[path, hertel]„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„Çí„É™„Çπ„Éà„Å´ËøΩÂä†„Åô„Çã\n",
        "            item_paths = self.item_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.item_paths[index][0]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image).float()\n",
        "        hertel = self.item_paths[index][1]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "#test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "#test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))"
      ],
      "metadata": {
        "id": "IteKm-r5brwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013b260b-ebe0-4e4f-accd-ad64b4034f1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1588\n",
            "val_dataset_size: 398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test with early-stopping"
      ],
      "metadata": {
        "id": "JGWnnohkWBpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size,optimizer, patience, n_epochs, device,  alpha=0):\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "\n",
        "            #l2_normalization\n",
        "            l2 = torch.tensor(0., requires_grad=True)\n",
        "            for w in model.parameters():\n",
        "                l2 = l2 + torch.norm(w)**2\n",
        "            loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(n_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "\n",
        "        print(print_msg)\n",
        "\n",
        "\n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_loss„Åå‰∏ã„Åå„Çâ„Å™„Åë„Çå„Å∞Ê∏õË°∞\n",
        "\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "iJyYRN8SOPB0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ConvNet„ÅÆË™øÊï¥"
      ],
      "metadata": {
        "id": "E2WdAbf9WMMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###„Ç™„É™„Ç∏„Éä„É´RepVGG-A2‰ΩøÁî®\n"
      ],
      "metadata": {
        "id": "w4uM5uvHV8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc1 = nn.Linear(in_features=1408, out_features=512) #out_features„Çí1„Å´\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=1) #out_features„Çí1„Å´\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep_learning/666mai_dataset/RepVGG-A2-train.pth\"))\n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU‰ΩøÁî®\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#ÊêçÂ§±Èñ¢Êï∞„ÇíÂÆöÁæ©\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "xooKuBLyV_xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Timm‰ΩøÁî®„ÅÆÂ†¥Âêà"
      ],
      "metadata": {
        "id": "DLA4qqaoYeCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "efFP6EU1WH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc = nn.Linear(in_features=1280, out_features=1) #„É¢„Éá„É´„Å´Âøú„Åò„Å¶in_features„ÇíË™øÊï¥„ÄÅout_features„Çí1„Å´\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x) #dropout„Çí1Â±§ËøΩÂä†\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # fc layer 2„Å§„ÅÆ„Éê„Éº„Ç∏„Éß„É≥\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc1 = nn.Linear(in_features=1280, out_features=512) #„É¢„Éá„É´„Å´Âøú„Åò„Å¶in_features„ÇíË™øÊï¥\n",
        "#         self.fc2 = nn.Linear(in_features=512, out_features=1) #out_features„Çí1„Å´\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Batch_norm plus dropout (for RepVGG_A2: „Ç§„Éû„Ç§„ÉÅ)\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.bn = nn.BatchNorm2d(1408)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "#         self.dropout = nn.Dropout(0.25)\n",
        "#         self.fc = nn.Linear(in_features=12672, out_features=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = self.bn(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.maxpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "#model_ft = timm.create_model(model_name = 'efficientnetv2_rw_m', pretrained=True, num_classes=1)\n",
        "#model_ft = timm.create_model(model_name = 'repvgg_a2', pretrained=True, num_classes=1)\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "#model_ft = mod_CNNModel()\n",
        "\n",
        "#GPU‰ΩøÁî®\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#ÊêçÂ§±Èñ¢Êï∞„ÇíÂÆöÁæ©\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "#optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "optimizer_ft =  optim.AdaBound(\n",
        "    model_ft.parameters(),\n",
        "    lr= 1e-3,\n",
        "    betas= (0.9, 0.999),\n",
        "    final_lr = 0.1,\n",
        "    gamma=1e-3,\n",
        "    eps= 1e-8,\n",
        "    weight_decay=5e-4,\n",
        "    amsbound=False,\n",
        ")"
      ],
      "metadata": {
        "id": "-VvPVPdJWIER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bcb424-1d0e-481e-8cc6-f1efb5fcf74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ranger_adabelief in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from ranger_adabelief) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->ranger_adabelief) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->ranger_adabelief) (1.3.0)\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)"
      ],
      "metadata": {
        "id": "o46zFFxQ7zZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Draw Learning Curves"
      ],
      "metadata": {
        "id": "nGCIPkQbd8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1\n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "HRqX-uCLWIIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation using testset"
      ],
      "metadata": {
        "id": "HFAFvtSxeBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Êó¢Â≠ò„ÅÆ„É¢„Éá„É´„Çí„É≠„Éº„Éâ„Åô„ÇãÊôÇ\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_0.pth\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "# state_dict „Çí„É≠„Éº„Éâ„Åó„Å¶„É¢„Éá„É´„Å´ÈÅ©Áî®\n",
        "model_ft.load_state_dict(torch.load(model_weight))\n",
        "model_ft.to(device)"
      ],
      "metadata": {
        "id": "WqAWYvwtHbMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in val_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#Âπ≥Âùá„Åã„Çâ„ÅÆÂ∑ÆÂàÜ„ÇíË£úÊ≠£\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "id": "gcZBDepNWIKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fcd854-89cc-471c-ddea-ee3b997de261"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: -0.5527114029505744\n",
            "StdError: 1.7488486740381806\n",
            "AveAbsError: 1.423884748813495\n",
            "StdAbsError: 1.1541994525796486\n",
            "\n",
            "Corrected_AveAbsError: 1.3640384237223395\n",
            "Corrected_StdAbsError: 1.0923297173249142\n",
            "Round_Corrected_AveAbsError: 1.3165829145728642\n",
            "Round_Corrected_StdAbsError: 1.1577257223848767\n",
            "Probability of AbsError <= 1: 0.45979899497487436\n",
            "Probability of AbsError <= 2: 0.7412060301507538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using test dataset\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#Âπ≥Âùá„Åã„Çâ„ÅÆÂ∑ÆÂàÜ„ÇíË£úÊ≠£\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #Âπ≥ÂùáË™§Â∑Æ\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #Ë™§Â∑ÆÊ®ôÊ∫ñÂÅèÂ∑Æ\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #Áµ∂ÂØæË™§Â∑ÆÊ®ôÊ∫ñÂÅèÂ∑Æ\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ(ÂõõÊç®‰∫îÂÖ•)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #Áµ∂ÂØæË™§Â∑ÆÊ®ôÊ∫ñÂÅèÂ∑Æ(ÂõõÊç®‰∫îÂÖ•)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)\n"
      ],
      "metadata": {
        "id": "8p5vrzYkJQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw GraphsÔºà„ÇÇ„Å®„ÇÇ„Å®„ÅÆÊï£Â∏ÉÂõ≥\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "qa1NMrMQJQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bland-Altman-Plot\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fs7DIPTJeVpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Á∑öÂΩ¢Ëøë‰ººÂºèÁÆóÂá∫\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# Ë™¨ÊòéÂ§âÊï∞x„Å´ \"x1\"„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# ÁõÆÁöÑÂ§âÊï∞y„Å´ \"x2\"„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®\n",
        "y = target.values\n",
        "\n",
        "# ‰∫àÊ∏¨„É¢„Éá„É´„Çí‰ΩúÊàêÔºàÂçòÂõûÂ∏∞Ôºâ\n",
        "clf.fit(x, y)\n",
        "\n",
        "# „Éë„É©„É°„Éº„ÇøÔºàÂõûÂ∏∞‰øÇÊï∞„ÄÅÂàáÁâáÔºâ„ÇíÊäΩÂá∫\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# „Éë„É©„É°„Éº„Çø„ÅÆË°®Á§∫\n",
        "print(\"ÂõûÂ∏∞‰øÇÊï∞:\", a)\n",
        "print(\"ÂàáÁâá:\", b)\n",
        "print(\"Ê±∫ÂÆö‰øÇÊï∞:\", clf.score(x, y))\n",
        "\n",
        "#Âπ≥ÂùáÂÄ§„Å´„Çà„ÇäË£úÊ≠£„Åó„ÅüÂÄ§\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#ÂõûÂ∏∞Áõ¥Á∑ö„Å´„Çà„ÇäË£úÊ≠£„Åó„ÅüÂÄ§\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#ÊÆãÂ∑Æ\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#ÊÆãÂ∑Æ\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]"
      ],
      "metadata": {
        "id": "_1MEuRIReVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "heTFISEt9D_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Âπ≥ÂùáËøë‰ºº„Éê„Éº„Ç∏„Éß„É≥\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # Âá°‰æã„ÇíË°®Á§∫\n",
        "plt.show()   # „Éí„Çπ„Éà„Ç∞„É©„É†„ÇíË°®Á§∫\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "\n",
        "# print(\"\")\n",
        "# print('Error<-1 and Error>-1: ' + str(sum((-1 < i < 1 for i in df['Residual_error_2']))))\n",
        "# print('Error<-2 and Error>2: ' + str(sum((-2 < i < 2 for i in df['Residual_error_2']))))\n",
        "# print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "# print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "total_errors = len(df['Residual_error_2'])\n",
        "\n",
        "error_minus1_to_1_count = sum((-1 < i < 1 for i in df['Residual_error_2']))\n",
        "error_minus1_to_1_percentage = (error_minus1_to_1_count / total_errors) * 100\n",
        "\n",
        "error_minus2_to_2_count = sum((-2 < i < 2 for i in df['Residual_error_2']))\n",
        "error_minus2_to_2_percentage = (error_minus2_to_2_count / total_errors) * 100\n",
        "\n",
        "error_less_equal_minus2_count = sum((i <= -2 for i in df['Residual_error_2']))\n",
        "error_less_equal_minus2_percentage = (error_less_equal_minus2_count / total_errors) * 100\n",
        "\n",
        "error_greater_equal_2_count = sum((i >= 2 for i in df['Residual_error_2']))\n",
        "error_greater_equal_2_percentage = (error_greater_equal_2_count / total_errors) * 100\n",
        "\n",
        "print(\"\")\n",
        "print(f'Error<-1 and Error>-1: {error_minus1_to_1_count}/{total_errors} ({error_minus1_to_1_percentage:.2f}%)')\n",
        "print(f'Error<-2 and Error>2: {error_minus2_to_2_count}/{total_errors} ({error_minus2_to_2_percentage:.2f}%)')\n",
        "print(f'Error<=-2: {error_less_equal_minus2_count}/{total_errors} ({error_less_equal_minus2_percentage:.2f}%)')\n",
        "print(f'Error>=2: {error_greater_equal_2_count}/{total_errors} ({error_greater_equal_2_percentage:.2f}%)')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1\n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm‰ª•‰∏ä„ÅÆÊ§úÂá∫Á≤æÂ∫¶')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1\n",
        "\n",
        "print('')\n",
        "print('Êé®Ê∏¨18mm‰ª•‰∏ä„Å†„ÅåÂÆü„ÅØ16mmÊú™Ê∫Ä(ÈÅéÂâ∞): '+str(kajyou)+'‰æã')\n",
        "print('Êé®Ê∏¨16mmÊú™Ê∫Ä„Å†„ÅåÂÆü„ÅØ18mm‰ª•‰∏äÔºàË¶ãÈÄÉ„Åå„ÅóÔºâ: '+str(minogashi)+'‰æã')"
      ],
      "metadata": {
        "id": "cHjgGcbk9EBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference pipeline YOLOv5‚ÜíMobileNetv3**"
      ],
      "metadata": {
        "id": "450sY_r6ZvZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imgs_dir = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/HerteL_images_TED\"\n",
        "imgs_dir = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Hertel_images_TUMOR\"\n",
        "\n",
        "#csv_path = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Img_Hertel_list_TED.csv\"\n",
        "csv_path = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Img_Hertel_list_TUMOR.csv\"\n",
        "\n",
        "yolo_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "mobilenet_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_0.pth\""
      ],
      "metadata": {
        "id": "Cs3KMCqreVvN"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List comprehension to create a list of file paths\n",
        "imgs_list = [os.path.join(imgs_dir, path) for path in os.listdir(imgs_dir)]\n",
        "img_paths = imgs_list[0:]  # Displaying the first five file paths"
      ],
      "metadata": {
        "id": "2NS0xdWJndml"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# CSV„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# ÁµêÊûú„ÇíÊ†ºÁ¥ç„Åô„Çã„Åü„ÇÅ„ÅÆ„É™„Çπ„Éà\n",
        "labels_list = []\n",
        "\n",
        "# img_path„ÅÆÂêÑ„Éë„Çπ„Å´ÂØæ„Åó„Å¶\n",
        "for path in img_paths:\n",
        "    # „Éë„Çπ„Åã„Çâ„Éô„Éº„Çπ„Éç„Éº„É†Ôºà„Éï„Ç°„Ç§„É´ÂêçÔºâ„ÇíÊäΩÂá∫\n",
        "    base_name = os.path.basename(path)\n",
        "\n",
        "    # „Éô„Éº„Çπ„Éç„Éº„É†„Å´ÂØæÂøú„Åô„ÇãË°å„ÇíË¶ã„Å§„Åë„Çã\n",
        "    row = df[df['file_name'] == base_name]\n",
        "\n",
        "    # \"Hertel_R\" „Å® \"Hertel_L\" „ÅÆÂÄ§„ÇíÂèñÂæó„Åó„ÄÅ„É™„Çπ„Éà„Å´ËøΩÂä†\n",
        "    if not row.empty:\n",
        "        labels_list.append(row[['file_name','Hertel_R', 'Hertel_L']].values[0].tolist())\n",
        "    else:\n",
        "        # ÂØæÂøú„Åô„ÇãË°å„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅNone „Åæ„Åü„ÅØÈÅ©Âàá„Å™ÂÄ§„ÇíËøΩÂä†\n",
        "        labels_list.append(None)\n",
        "        print(f\"not found in csv file, {base_name}\")\n",
        "\n",
        "# Êñ∞„Åó„ÅÑ„Éá„Éº„Çø„Éï„É¨„Éº„É†„Çí‰ΩúÊàê\n",
        "new_df = pd.DataFrame(labels_list, columns=['file_name', 'Hertel_R', 'Hertel_L'])\n",
        "\n",
        "# ‰∫àÊ∏¨ÂÄ§Ë®òÈå≤Áî®„Å´„Éá„Éº„Çø„Éï„É¨„Éº„É†„Å´Ê¨Ñ„Çí‰Ωú„Å£„Å¶„Åä„Åè\n",
        "new_df['pred_R'] = pd.NA\n",
        "new_df['pred_L'] = pd.NA\n",
        "\n",
        "# Êñ∞„Åó„ÅÑ„Éá„Éº„Çø„Éï„É¨„Éº„É†„ÇíË°®Á§∫\n",
        "#print(new_df)"
      ],
      "metadata": {
        "id": "rj2aAK7YYvTu"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Y8ElaId4Ct",
        "outputId": "04d98909-02a5-4e75-e149-91c926325448"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 27.5/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "# state_dict „Çí„É≠„Éº„Éâ„Åó„Å¶„É¢„Éá„É´„Å´ÈÅ©Áî®\n",
        "model_ft.load_state_dict(torch.load(mobilenet_weight))\n",
        "model_ft.to(device)\n",
        "\n",
        "#Ë©ï‰æ°„É¢„Éº„Éâ„Å´„Åô„Çã\n",
        "model_ft.eval()"
      ],
      "metadata": {
        "id": "zEDchCiOttlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image‰ΩúÊàê„ÄÅ„É™„Çµ„Ç§„Ç∫\n",
        "    # ÂÖÉ„ÅÆÁîªÂÉè„ÅÆ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # Ê≠£ÊñπÂΩ¢„ÅÆ„Çµ„Ç§„Ç∫„ÇíÊ±∫ÂÆöÔºàÂÖÉ„ÅÆÁîªÂÉè„ÅÆÈï∑Ëæ∫„Å´Âêà„Çè„Åõ„ÇãÔºâ\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # Ê≠£ÊñπÂΩ¢„ÅÆ„Ç≠„É£„É≥„Éê„Çπ„Çí‰ΩúÊàêÔºàËÉåÊôØ„ÅØÈªíÔºâ\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # ÂÖÉ„ÅÆÁîªÂÉè„ÇíÊ≠£ÊñπÂΩ¢„ÅÆÁîªÂÉè„ÅÆ‰∏≠Â§Æ„Å´ÈÖçÁΩÆ„Åô„Çã„Åü„ÇÅ„ÅÆÈñãÂßãÁÇπÔºàx,yÔºâ„ÇíË®àÁÆó\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # ÂÖÉ„ÅÆÁîªÂÉè„ÇíÊ≠£ÊñπÂΩ¢„ÅÆ„Ç≠„É£„É≥„Éê„Çπ„Å´„Ç≥„Éî„Éº„Åô„Çã\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    return letterbox_img_resized\n",
        "\n",
        "\n",
        "#Ë©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆÁîªÂÉè‰∏ãÂá¶ÁêÜ\n",
        "def image_transform(image):\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(250),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.1,saturation=0.1, hue=0.2),\n",
        "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅÆÊ¨°ÂÖÉ„ÇíÂÖàÈ†≠„Å´ËøΩÂä†„Åó„Åü4D„ÉÜ„É≥„ÇΩ„É´„Å´Â§âÊèõ\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ft„ÇíGPU„Å´Ëºâ„Åõ„Çã\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#MobileNetv3„ÅÆÂâçÂá¶ÁêÜ\n",
        "def image_eval(image_tensor):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:„ÇØ„É©„ÇπÂêçÂâç„ÄÅprob:Á¢∫Áéá„ÄÅpred:„ÇØ„É©„ÇπÁï™Âè∑\n",
        "    pred = round(output.item(), 2)\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def showImage(image_path):\n",
        "    #ÁîªÂÉè„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #ÁîªÂÉè„ÅÆ„É™„Çµ„Ç§„Ç∫\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)"
      ],
      "metadata": {
        "id": "tMyb62rOoD2F"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display  # For displaying images in Colab\n",
        "\n",
        "# Assuming yolo_weight and other necessary variables are defined elsewhere\n",
        "\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "def process_images(image_paths, labels_llist):\n",
        "    for i, (img_path, label) in enumerate(zip(image_paths, labels_list)):\n",
        "        pred = inference(img_path, yolo_weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "        img_cv2 = cv2.imread(img_path)\n",
        "        img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "        if len(pred[0]) != 2:\n",
        "            print(\"Number of eyes are not 2 in image:\", img_path)\n",
        "\n",
        "        for bbox in pred[0]:\n",
        "            x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "            # Adjust bounding box coordinates\n",
        "            img_height, img_width, _ = img_cv2_resized.shape\n",
        "            padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "            padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "            x1 = max(x1 - padding_x, 0)\n",
        "            y1 = max(y1 - padding_y, 0)\n",
        "            x2 = min(x2 - padding_x, img_width)\n",
        "            y2 = min(y2 - padding_y, img_height)\n",
        "\n",
        "            # Crop and resize logic\n",
        "            mag = 640 / img_cv2.shape[1]\n",
        "            cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "            #resized_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "            resized_img = cv2.resize(cropped_img,(250,250))\n",
        "\n",
        "            # Display image\n",
        "            #cv2_imshow(resized_img)\n",
        "\n",
        "            #MobileNetÁî®„ÅÆÂâçÂá¶ÁêÜ\n",
        "            resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
        "            resized_img= Image.fromarray(resized_img)\n",
        "            image_tensor = image_transform(resized_img)\n",
        "            pred = image_eval(image_tensor)\n",
        "            if class_num == 0: #Âè≥Áúº\n",
        "                print(f\"file_name: {label[0]}, side: R, pred: {pred}, label: {label[1]}\")\n",
        "                new_df.at[i, \"pred_R\"] = pred\n",
        "            elif class_num == 1: #Â∑¶Áúº\n",
        "                print(f\"file_name: {label[0]}, side: R, pred: {pred}, label: {label[2]}\")\n",
        "                new_df.at[i, \"pred_L\"] = pred\n",
        "\n",
        "# Example usage\n",
        "process_images(img_paths, labels_list)\n",
        "new_df"
      ],
      "metadata": {
        "id": "wQazEg8Tcivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_targets = pd.concat([new_df[\"Hertel_R\"], new_df[\"Hertel_L\"]], ignore_index=True)\n",
        "df_outputs = pd.concat([new_df[\"pred_R\"], new_df[\"pred_L\"]], ignore_index=True)\n",
        "df_concat = pd.concat([df_targets, df_outputs], axis=1).dropna()\n",
        "targets = df_concat[0].tolist()\n",
        "outputs = df_concat[1].tolist()\n",
        "errors = (df_concat[0] - df_concat[1]).tolist()"
      ],
      "metadata": {
        "id": "DFDI3frKLj5E"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#Âπ≥Âùá„Åã„Çâ„ÅÆÂ∑ÆÂàÜ„ÇíË£úÊ≠£\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #Âπ≥ÂùáË™§Â∑Æ\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #Ë™§Â∑ÆÊ®ôÊ∫ñÂÅèÂ∑Æ\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #Áµ∂ÂØæË™§Â∑ÆÊ®ôÊ∫ñÂÅèÂ∑Æ\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #Âπ≥ÂùáÁµ∂ÂØæË™§Â∑Æ(ÂõõÊç®‰∫îÂÖ•)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #Áµ∂ÂØæË™§Â∑ÆÊ®ôÊ∫ñÂÅèÂ∑Æ(ÂõõÊç®‰∫îÂÖ•)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jac7UNZ9Jf4r",
        "outputId": "ae6ff47e-0e4a-4d0e-bfff-e6cab616de3b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: 0.7042749529190208\n",
            "StdError: 3.6201429128093703\n",
            "AveAbsError: 2.8783239171374766\n",
            "StdAbsError: 2.3026058283041193\n",
            "\n",
            "Corrected_AveError: -1.4085499058380406\n",
            "Corrected_StdError: 3.6201429128093703\n",
            "Corrected_AveAbsError: 2.979513124155468\n",
            "Corrected_StdAbsError: 2.4897675619913793\n",
            "Round_Corrected_AveAbsError: 2.983050847457627\n",
            "Round_Corrected_StdAbsError: 2.516679250193209\n",
            "\n",
            "Probability of AbsError <= 1: 0.24858757062146894\n",
            "Probability of AbsError <= 2: 0.416195856873823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw GraphsÔºà„ÇÇ„Å®„ÇÇ„Å®„ÅÆÊï£Â∏ÉÂõ≥\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "fcNiZ2NIQKMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data provided by the user\n",
        "data = {\n",
        "    \"file_name\": [\n",
        "        \"129-20191002-76-162415_118f83a916863e06de64edf...\",\n",
        "        \"9129-20191002-49-140906_ac8b709121a742083607af...\",\n",
        "        \"6336-20190213-79-132839_e0f0869a867604753f5741...\",\n",
        "        \"818-20181130-35-090753_614e05ca1e8e501ca739873...\",\n",
        "        \"1910-20181114-49-103737_71dcc2f31d839f0c77b9f2...\"\n",
        "    ],\n",
        "    \"Hertel_R\": [18, 18, 19, 16, 14],\n",
        "    \"Hertel_L\": [19, 19, 17, 15, 14],\n",
        "    \"pred_R\": [14, 15, 15, 15, 14],\n",
        "    \"pred_L\": [12, 14, 13, 15, 14]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# # Plotting with seaborn\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(data=df, x=\"Hertel_R\", y=\"pred_R\")\n",
        "# plt.title(\"Comparison of Hertel and Predicted Values\")\n",
        "# plt.xlabel(\"Hertel Value\")\n",
        "# plt.ylabel(\"Predicted Value\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "FzEwxwJ4kfP7",
        "outputId": "0cfe324a-98fe-4bf7-8c5f-b3f90b102e53"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0nD3hWRd4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTaiZLdCd4KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLp0xvu8ndtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "-BHc32Av1b2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 5-fold crossvalidation #\n",
        "####################\n",
        "\n",
        "model_name = 'repvgg_a2'\n",
        "# model_name = 'efficientnetv2_rw_m'\n",
        "# model_name = 'mobilenetv3_large_100'\n",
        "\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "# Initialize lists to store AbsErrors for each fold\n",
        "abs_errors_fold = [[] for _ in range(5)]\n",
        "\n",
        "\n",
        "for fold in [0,1,2,3,4]:\n",
        "    # Define dataset & dataloader\n",
        "    train_dataset = Create_Datasets(train_set[fold], CSV_PATH, train_data_transforms)\n",
        "    val_dataset = Create_Datasets(val_set[fold], CSV_PATH, val_data_transforms)\n",
        "    #test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "    #test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "    #Define Model\n",
        "    model_ft = timm.create_model(model_name = model_name, pretrained=True, num_classes=1)\n",
        "\n",
        "    #GPU‰ΩøÁî®\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #ÊêçÂ§±Èñ¢Êï∞„ÇíÂÆöÁæ©\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    # !pip install ranger_adabelief\n",
        "    # from ranger_adabelief import RangerAdaBelief\n",
        "    # optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "    # optimizer_ft =  optim.AdaBound(\n",
        "    #     model_ft.parameters(),\n",
        "    #     lr= 1e-3,\n",
        "    #     betas= (0.9, 0.999),\n",
        "    #     final_lr = 0.1,\n",
        "    #     gamma=1e-3,\n",
        "    #     eps= 1e-8,\n",
        "    #     weight_decay=5e-4,\n",
        "    #     amsbound=False,\n",
        "    # )\n",
        "\n",
        "    # Train model\n",
        "    EPOCH = 1\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n",
        "\n",
        "    #evaluation using validation dataset\n",
        "    val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "\n",
        "    model_ft.eval() # prep model for evaluation\n",
        "\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in val_loader:\n",
        "        target = target.view(len(target), 1)\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        target = target.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "        outputs.append(output[0].item())\n",
        "        targets.append(target[0].item())\n",
        "        #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "        errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "    AbsError = [abs(i) for i in errors]\n",
        "\n",
        "    # Append AbsError from current fold to the corresponding list\n",
        "    abs_errors_fold[fold].extend(AbsError)\n",
        "\n",
        "\n",
        "\n",
        "    print('AveError: '+str(statistics.mean(errors)))\n",
        "    print('StdError: '+str(statistics.stdev(errors)))\n",
        "    print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "    print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "    #Âπ≥Âùá„Åã„Çâ„ÅÆÂ∑ÆÂàÜ„ÇíË£úÊ≠£\n",
        "    corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "    corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "    corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "    round_output = [my_round(i) for i in outputs]\n",
        "    round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "    print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "    print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "    print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "    print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "    # Calculate the probabilities\n",
        "    abs_error_np = np.array(AbsError)\n",
        "    prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "    prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "    print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "    print('Probability of AbsError <= 2:', prob_less_than_2)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df_abs_errors = pd.DataFrame({\n",
        "    f'Fold_{i}': errors for i, errors in enumerate(abs_errors_fold)\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Draw Box_plot\n",
        "# Melting the DataFrame to long format suitable for Seaborn\n",
        "df_long = df_abs_errors.melt(var_name='Fold', value_name='AbsError')\n",
        "\n",
        "# Create the boxplot\n",
        "\n",
        "sns.boxplot(x='Fold', y='AbsError', data=df_long)\n",
        "# Adding titles and labels\n",
        "plt.title('Boxplot of Absolute Errors for Each Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Absolute Error')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_abs_errors.to_csv(f'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_{model_name}.csv', index=False)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "todo:\n",
        "df„Çí‰ΩúÊàê„Åô„Çã„ÄÇÁîªÂÉè„ÅÆpath„ÄÅlabel„ÄÅfoldÊØé„ÅÆÂà§ÂÆöÁµêÊûú\n",
        "df„Åã„Çâ„ÄÅ\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3uAaRHyehpQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9aad00a8-a1cf-4b27-e952-783fdc37364d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-34e82c59d329>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#evaluation using validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-68273327f946>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, batch_size, optimizer, patience, n_epochs, device, alpha)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;31m# convert batch-size labels to batch-size x 1 tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m#target = target.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0d6a5e19d841>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpilr_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtensor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilr_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhertel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EeuEYbSvne-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fxsq2b9nfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEQIJyYUnfE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFGlh1_7nfHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZ2vGUF4nfMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --include 'coreml' --nms --weights $weight_path"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --include 'coreml' --weights $weight_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3bGseObGPLg",
        "outputId": "2e53b1c9-d7b5-42af-a380-625f4e4f2ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['coreml']\n",
            "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt with output shape (1, 25200, 7) (3.7 MB)\n",
            "scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
            "XGBoost version 2.0.1 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "2023-11-11 06:39:03.361674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-11 06:39:03.361740: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-11 06:39:03.361772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "TensorFlow version 2.14.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 7.1...\n",
            "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_targer' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats\n",
            "Tuple detected at graph output. This will be flattened in the converted model.\n",
            "Converting PyTorch Frontend ==> MIL Ops: 100% 607/609 [00:00<00:00, 3968.51 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100% 5/5 [00:00<00:00, 230.02 passes/s]\n",
            "Running MIL default pipeline: 100% 71/71 [00:01<00:00, 44.08 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100% 12/12 [00:00<00:00, 373.03 passes/s]\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export failure ‚ùå 6.6s: For an ML Program, extension must be .mlpackage (not .mlmodel). Please see https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats to see the difference between neuralnetwork and mlprogram model types.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFoB1lpJGPNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkxZXJEZGPPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ê§úË®ºÁî®letterbox‰ΩúÊàê„Çπ„ÇØ„É™„Éó„Éà**\n",
        "\n",
        "ÊåáÂÆö„Åó„Åü„Éë„Çπ„ÅÆÁîªÂÉè„ÇíÈï∑„ÅÑËæ∫„Çí‰∏ÄËæ∫„Å®„Åó„Åületterbox(ÈªíÂ°ó„Çä)„Å´„Åô„Çã"
      ],
      "metadata": {
        "id": "cCGQw9ndwxuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "parent_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig\"\n",
        "basename_list = [\n",
        "    \"22.JPG\",\n",
        "    \"62.JPG\",\n",
        "    \"72.JPG\"\n",
        "]\n",
        "\n",
        "def convert_to_letterbox(path):\n",
        "    # ÁîªÂÉè„ÇíÈñã„Åè\n",
        "    with Image.open(path) as img:\n",
        "        # ÂÖÉ„ÅÆÁîªÂÉè„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\n",
        "        width, height = img.size\n",
        "\n",
        "        # Èï∑Ëæ∫„ÇíÂü∫Ê∫ñ„Å´Êñ∞„Åó„ÅÑ„Çµ„Ç§„Ç∫„ÇíË®àÁÆó\n",
        "        if width > height:\n",
        "            new_size = (width, width)\n",
        "        else:\n",
        "            new_size = (height, height)\n",
        "\n",
        "        # Êñ∞„Åó„ÅÑÁîªÂÉè„Çí‰ΩúÊàêÔºàËÉåÊôØ„ÅØÈªíÔºâ\n",
        "        new_img = Image.new(\"RGB\", new_size, (0, 0, 0))\n",
        "\n",
        "        # ÁîªÂÉè„Çí‰∏≠Â§Æ„Å´ÈÖçÁΩÆ\n",
        "        new_img.paste(img, ((new_size[0] - width) // 2, (new_size[1] - height) // 2))\n",
        "\n",
        "        return new_img\n",
        "\n",
        "\n",
        "for basename in basename_list:\n",
        "    img_path = os.path.join(parent_path, basename)\n",
        "    letterbox_img = convert_to_letterbox(img_path)\n",
        "    letterbox_img.save(os.path.join(\"/content\", basename))"
      ],
      "metadata": {
        "id": "DYuBdiIBzmRk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcddETYo3W0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}