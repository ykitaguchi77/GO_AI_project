{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de9df9f8c40c40eabab6ea5417e2a5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d49bf70abcc423f8469340708052406",
              "IPY_MODEL_8c6e53bd4f324234badf6380d794c908",
              "IPY_MODEL_3a1a0175db4b4a55906714fecf213b5c"
            ],
            "layout": "IPY_MODEL_428d03b85d29441ca3957f5122e46c5f"
          }
        },
        "0d49bf70abcc423f8469340708052406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1379aadc02443699e42cd0f3b2482f2",
            "placeholder": "​",
            "style": "IPY_MODEL_8a8f7e57c0bb48a0b0f9405b8584c4c8",
            "value": "model.safetensors: 100%"
          }
        },
        "8c6e53bd4f324234badf6380d794c908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ee5bff71bfc4e5cbd21703142f13462",
            "max": 214290028,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c2bccf5c3614e0ab101830f92953ff4",
            "value": 214290028
          }
        },
        "3a1a0175db4b4a55906714fecf213b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8bac21650f943359985bd2f45513eb3",
            "placeholder": "​",
            "style": "IPY_MODEL_afafd321d1d945989f9b976398e4683d",
            "value": " 214M/214M [00:14&lt;00:00, 17.3MB/s]"
          }
        },
        "428d03b85d29441ca3957f5122e46c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1379aadc02443699e42cd0f3b2482f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a8f7e57c0bb48a0b0f9405b8584c4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ee5bff71bfc4e5cbd21703142f13462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2bccf5c3614e0ab101830f92953ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8bac21650f943359985bd2f45513eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afafd321d1d945989f9b976398e4683d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Hertel_YOLOv5_MobileNetv3_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation YOLOv5-MobileNetv3 pipeline**\n",
        "\n",
        "Train YOLOv5\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlibで目が2つ検出されるものを抜き出す\n",
        "YOLOv5を用いて左右とバウンディングボックスを認識させる\n",
        "抜き出した画像についてMobileNetV3で回帰（5-fold ensemble）を行う\n",
        "スマホに実装\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98990663-b616-48a6-fb74-d7d3d4967fb2"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 10 15:01:52 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4733bdd-501d-472f-e229-c0892a63e26c"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.14"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b809faca-e703-4dc7-b85e-44f58958edef"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#親フォルダ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#元画像フォルダ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#元画像をコピー\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#切りぬいた画像を保存するフォルダ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#トレーニングされたYOLOv5で切り抜いた画像を保存するフォルダ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSVファイルのフォルダ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Refresh folder (内容が削除されるので注意！！) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# parent_dirがあれば削除する\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# 新しくparent_dirを作成する\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirを新規に作成する\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dirにdataset_dir直下のファイルをすべてコピーする\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"処理が完了しました。\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnV1hEgIu87W",
        "outputId": "3a0912d6-c869-4522-8f53-9805cb9470b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 100%|██████████| 1016/1016 [00:33<00:00, 30.43file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "処理が完了しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeを用いて目を検出**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**目が2つ以上検出されたものを抜き出す**\n",
        "\n",
        "dlibで検出されたものから、上下左右に0.1倍ずつ拡大した範囲を抜き出している"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #フォルダ数の分だけ\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # 画像グレースケール化\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix以上のもので目に見えるものを抽出\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # 眼検出判定\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #画像の切り抜きと保存（2個以上検出の時に限る）\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #本来の切り抜きより幅の0.1倍ずつ水増しする\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #横幅の半分より左にあるのは右眼\n",
        "                      else:\n",
        "                          side = \"L\" #横幅の半分よりより右にあるのは左眼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #対応表の作成\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ここで、目以外が誤検出されているものを手動で抜き出して削除する**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csvから、削除して画像のパスが存在しなくなっている行を削除する\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrameを読み込む\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# 存在しない画像パスをチェックし、そのリストを保持する\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# 存在しない画像パスを表示\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # 存在しない画像パスの行を削除\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # 更新されたDataFrameを保存する\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "35178cc0-37a3-4148-b3fd-fa12e141342a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframeの整理**\n",
        "\n",
        "・ hertel_dfを参照して、coordinates_dfにヘルテル値を記入する\n",
        "\n",
        "・idが\"16_R, 16_L\"という形式になるようにデータフレームを整理する"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6e553e1b-105b-4366-fd88-7dd9508f2c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-817e306f-678a-4ab3-a935-a6e022b87f2b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-817e306f-678a-4ab3-a935-a6e022b87f2b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-817e306f-678a-4ab3-a935-a6e022b87f2b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "画像パスの抽出（RLともに揃っているもの）\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "画像の分割 train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "フォルダの作成\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "画像のコピー\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d78c062-ef26-47b7-b3b2-e8be204b050a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|██████████| 760/760 [00:09<00:00, 82.08it/s]\n",
            "Copying valid images: 100%|██████████| 190/190 [00:02<00:00, 67.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\" {side} {cx} {cy} {w} {h}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# trainとvalidのディレクトリパス\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# trainディレクトリでラベルファイルを生成\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# validディレクトリでラベルファイルを生成\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e09a5a-a1ab-48cf-b03b-385a93882b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 760/760 [00:05<00:00, 132.46it/s]\n",
            "Processing images: 100%|██████████| 190/190 [00:01<00:00, 142.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## バウンディングボックスのサンプル描画 ##\n",
        "## (これは実行しなくて良い)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# バウンディングボックスを描画する関数\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            class_id, cx, cy, bw, bh = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得する関数\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# 画像パスとラベルファイルパス\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# 画像のサイズを取得\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "id": "OhSI3Wc_-ylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "4237beaa-b891-4844-fbf8-9e1204bc6422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5_iFish --> train_batchの精度が低いので一旦却下とした\n",
        "# %cd $out_dir\n",
        "# !git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationを実装したバージョン\n",
        "# %cd yolov5-iFish\n",
        "# %pip install -qr requirements.txt\n",
        "\n",
        "# import torch\n",
        "# import utils\n",
        "# display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "x-33rbP1-iQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "%cd $out_dir\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "vdYFiF39egKw",
        "outputId": "d6748592-94d0-408c-a413-b6ed0a67e7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "id": "7iRcXa8I-iSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 途中から\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/data.yaml --resume $out_dir/yolov5/runs/train/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = f\"{out_dir}/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)\n",
        "\n",
        "dst_pt = f\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2",
        "outputId": "50b709c2-ed57-4272-e70d-d3438424c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference original dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb903b0-a0a0-4dd0-c0c4-427fc745869c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 28.3/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = f\"{out_dir}/dataset_uni/eyecrop-yolov5n-iFish_169epoch.pt\"\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "\n",
        "# もともとのデータセット\n",
        "orig_dir = orig_dir #元画像\n",
        "cropped_dir = cropped_dir #YOLOv5で切り抜いた画像用\n",
        "\n",
        "if os.path.exists(cropped_dir):\n",
        "    shutil.rmtree(cropped_dir)\n",
        "os.makedirs(cropped_dir)\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image作成、リサイズ\n",
        "    # 元の画像のサイズを取得\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # 正方形のサイズを決定（元の画像の長辺に合わせる）\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # 正方形のキャンバスを作成（背景は黒）\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # 元の画像を正方形の画像の中央に配置するための開始点（x,y）を計算\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # 元の画像を正方形のキャンバスにコピーする\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    # 変換した画像を保存\n",
        "    return letterbox_img_resized"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## バウンディングボックス&切り抜き demo ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img_cv2_resized, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "P-J5WiSyXGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# ## 切り抜き + 保存 ##\n",
        "# #################\n",
        "# \"\"\"\n",
        "# Letterbox & 250px正方形にリサイズ\n",
        "# cropped_dirに保存\n",
        "# \"\"\"\n",
        "\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "# start_index = 0\n",
        "# end_index = len(os.listdir(orig_dir))\n",
        "\n",
        "# class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# for i in range(start_index, end_index):\n",
        "#     img = image_path[i]\n",
        "#     pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "#     img_cv2 = cv2.imread(img)\n",
        "#     img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "#     for bbox in pred[0]:\n",
        "#         x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "#         prob = bbox[4].item()\n",
        "#         class_name = class_names[bbox[5].item()]\n",
        "\n",
        "#         #print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "#         # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "#         img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "#         #print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "#         padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "#         padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "#         x1 = x1 - padding_x\n",
        "#         y1 = y1 - padding_y\n",
        "#         x2 = x2 - padding_x\n",
        "#         y2 = y2 - padding_y\n",
        "#         #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "#         # Crop and save the image\n",
        "#         mag = 640 / img_cv2.shape[1]\n",
        "#         cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "#         letterbox_img = make_letterbox_image(cropped_img)\n",
        "#         #cv2_imshow(letterbox_img)\n",
        "\n",
        "#         base_name = os.path.splitext(os.path.basename(img))[0]\n",
        "#         cropped_img_path = os.path.join(f\"{cropped_dir}/cropped_images\", f\"{base_name}_{class_name}.png\")\n",
        "#         cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "#         print(f\"succefully saved, image {i}: {cropped_img_path}\")\n"
      ],
      "metadata": {
        "id": "iAelqChiXGne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Load image paths\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 0\n",
        "end_index = len(image_paths)\n",
        "\n",
        "# Define class names\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# List to hold images with incorrect detections\n",
        "incorrect_detections = []\n",
        "\n",
        "# Iterate over images\n",
        "for i in range(start_index, end_index):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "    # Check if the detections are not equal to 2\n",
        "    if len(pred[0]) != 2:\n",
        "        incorrect_detections.append((img_path, len(pred[0])))\n",
        "        continue  # Skip the rest of the loop and do not process this image\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #バウンディングボックスが左に切れる場合の対処\n",
        "            x1 = 0\n",
        "        if y2>img_width: #バウンディングボックスが右に切れる場合の対処\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        # Save the cropped image\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        class_name = class_names[class_num]\n",
        "        cropped_img_path = os.path.join(f\"{cropped_dir}\", f\"{base_name}_{class_name}.png\")\n",
        "        cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "        print(f\"Successfully saved, image {i}: {cropped_img_path}\")\n",
        "\n",
        "# Output images with incorrect detections\n",
        "print(\"Images with incorrect detections:\")\n",
        "for img_path, num_detections in incorrect_detections:\n",
        "    print(f\"{img_path} - Number of detections: {num_detections}\")\n",
        "incorrect_paths = [path for path, _ in incorrect_detections]"
      ],
      "metadata": {
        "id": "WdowrfNNRhNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## うまく左右眼を検出できなかった例の確認 ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = [\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig/92.JPG\"]\n",
        "#image_path = incorrect_paths\n",
        "start_index = 0\n",
        "end_index = len(image_path)\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img_cv2_resized, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "PCBjJv1KlEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder\n",
        "%cd \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5\"\n",
        "folder_path = \"dataset_yolo_cropped\"\n",
        "\n",
        "# Zip the folder\n",
        "!zip -r /content/cropped_images.zip \"$folder_path\"\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('/content/cropped_images.zip')"
      ],
      "metadata": {
        "id": "vEepFKfW2HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To do\n",
        "\n",
        "・Group 5-fold split\n",
        "・MobileNetV3でcross validation --> 精度プロット作成\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_estimation_crossvalidation_noTestset.ipynb\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_ensemble_quick.ipynb\n",
        "・当院データセットでtestする\n",
        "'''"
      ],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3 using cropped images**\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_yolo_cropped/cropped_images\n",
        "の画像を手動で確認、不適切な画像を削除\n",
        "\n",
        "・https://tcd-theme.com/2019/12/mac-zip-compression.html\n",
        "\n",
        "を参考にして圧縮\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_cropped_for_MobileNet_training/cropped_images.zipとしてアップロード"
      ],
      "metadata": {
        "id": "lvkfZ9aQ9S1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv5で抜き出した画像を規定のフォルダに移動"
      ],
      "metadata": {
        "id": "rKFHyFisE2Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5で抜き出した画像を規定のフォルダに移動\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "parent_folder = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training'\n",
        "\n",
        "# zipファイルのパス\n",
        "zip_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/cropped_images.zip'\n",
        "\n",
        "# zipファイルを解凍\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(parent_folder)\n"
      ],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ここから"
      ],
      "metadata": {
        "id": "vdOpequeE781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer --q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install pingouin --q\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True"
      ],
      "metadata": {
        "id": "q7Zp_wf_JQjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f140c136-ff6b-426b-ac50-064717386239"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRandom Seed:  1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training\"\n",
        "# os.chdir(path)\n",
        "\n",
        "# contains train, val\n",
        "DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/dataset_yolo_cropped\"\n",
        "#DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_250px_uni_periocular\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_unilateral.csv\"\n",
        "LOG_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/log_multi.txt\"\n",
        "ROC_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/roc_multi.png\"\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# transforms param\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])])"
      ],
      "metadata": {
        "id": "8lzHTjGEL8eQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-Foldに分割**"
      ],
      "metadata": {
        "id": "kW9iYtvtOPiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(DATASET_PATH)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"patiend num: {}\".format(len(id)))"
      ],
      "metadata": {
        "id": "eMO4j991OO8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7924e96b-67bd-41be-bb32-33d3cbaf3470"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patiend num: 1986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #testsetなし。Group K-foldを用いてデータセット分け\n",
        "# from sklearn.model_selection import GroupKFold\n",
        "# from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# #fold数だけ空のリストを作成\n",
        "# num_folds = 5\n",
        "# train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "# X = np.ones(len(id))\n",
        "# y = np.ones(len(id))\n",
        "# gkf = GroupKFold(n_splits=num_folds)\n",
        "# i=0\n",
        "# for train_idxs, val_idxs in gkf.split(X, y, groups=patient):\n",
        "#     for idx in train_idxs:\n",
        "#         train_set[i].append(path_list[idx])\n",
        "#     for idx in val_idxs:\n",
        "#         val_set[i].append(path_list[idx])\n",
        "#     i+=1\n",
        "\n",
        "# print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "# print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "\n",
        "# print(\"extracted_id (example): {}\".format(extract_ids(train_set[0])[0]))"
      ],
      "metadata": {
        "id": "4QNmg_1gOO9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split ＋　Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9で分割\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "groups = patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "zjQNXUD2OO_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a29023-1fab-42b9-99f5-0524115a82eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1431\n",
            "val_dataset: 358\n",
            "test_dataset: 197\n",
            "\n",
            "extracted_id (example): 493_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シンプルなK-fold (group_K_Foldではない)\n",
        "\n",
        "num_folds = 5 #number of folds\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "#まず全体の1割をテストセットとしてよけておく\n",
        "remain_set, test_set = train_test_split(path_list, test_size=0.1, shuffle=True, random_state=random_seed)\n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_set):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "JatpoewI699u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3c4782-6ce1-4274-d689-6778e6ae47c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1429\n",
            "val_dataset: 358\n",
            "test_dataset: 199\n",
            "\n",
            "extracted_id (example): 830_L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create Datasets**"
      ],
      "metadata": {
        "id": "_hiIomNwbBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "\n",
        "    def __init__(self, img_list, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for image_path in img_list:\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([image_path, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.item_paths[index][0]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image).float()\n",
        "        hertel = self.item_paths[index][1]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))"
      ],
      "metadata": {
        "id": "IteKm-r5brwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f69758-aea2-4cc2-8dc5-5d843a2bde75"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1429\n",
            "val_dataset_size: 358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test with early-stopping**"
      ],
      "metadata": {
        "id": "JGWnnohkWBpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size,optimizer, patience, n_epochs, device,  alpha=0):\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "\n",
        "            #l2_normalization\n",
        "            l2 = torch.tensor(0., requires_grad=True)\n",
        "            for w in model.parameters():\n",
        "                l2 = l2 + torch.norm(w)**2\n",
        "            loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(n_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "\n",
        "        print(print_msg)\n",
        "\n",
        "\n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "iJyYRN8SOPB0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ConvNetの調整**"
      ],
      "metadata": {
        "id": "E2WdAbf9WMMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###オリジナルRepVGG-A2使用\n"
      ],
      "metadata": {
        "id": "w4uM5uvHV8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc1 = nn.Linear(in_features=1408, out_features=512) #out_featuresを1に\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep_learning/666mai_dataset/RepVGG-A2-train.pth\"))\n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "xooKuBLyV_xH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3ace04-b946-44ec-f0a5-a23ebca5ce77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Timm使用の場合"
      ],
      "metadata": {
        "id": "DLA4qqaoYeCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "efFP6EU1WH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc = nn.Linear(in_features=1280, out_features=1) #モデルに応じてin_featuresを調整、out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x) #dropoutを1層追加\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # fc layer 2つのバージョン\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc1 = nn.Linear(in_features=1280, out_features=512) #モデルに応じてin_featuresを調整\n",
        "#         self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Batch_norm plus dropout (for RepVGG_A2: イマイチ)\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.bn = nn.BatchNorm2d(1408)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "#         self.dropout = nn.Dropout(0.25)\n",
        "#         self.fc = nn.Linear(in_features=12672, out_features=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = self.bn(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.maxpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "model_ft = timm.create_model(model_name = 'efficientnetv2_rw_m', pretrained=True, num_classes=1)\n",
        "#model_ft = timm.create_model(model_name = 'repvgg_a2', pretrained=True, num_classes=1)\n",
        "#model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "#model_ft = mod_CNNModel()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "#optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "optimizer_ft =  optim.AdaBound(\n",
        "    model_ft.parameters(),\n",
        "    lr= 1e-3,\n",
        "    betas= (0.9, 0.999),\n",
        "    final_lr = 0.1,\n",
        "    gamma=1e-3,\n",
        "    eps= 1e-8,\n",
        "    weight_decay=5e-4,\n",
        "    amsbound=False,\n",
        ")"
      ],
      "metadata": {
        "id": "-VvPVPdJWIER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "de9df9f8c40c40eabab6ea5417e2a5f6",
            "0d49bf70abcc423f8469340708052406",
            "8c6e53bd4f324234badf6380d794c908",
            "3a1a0175db4b4a55906714fecf213b5c",
            "428d03b85d29441ca3957f5122e46c5f",
            "e1379aadc02443699e42cd0f3b2482f2",
            "8a8f7e57c0bb48a0b0f9405b8584c4c8",
            "4ee5bff71bfc4e5cbd21703142f13462",
            "6c2bccf5c3614e0ab101830f92953ff4",
            "b8bac21650f943359985bd2f45513eb3",
            "afafd321d1d945989f9b976398e4683d"
          ]
        },
        "outputId": "81159cea-f601-44e0-f341-0f5c934a2021"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/214M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de9df9f8c40c40eabab6ea5417e2a5f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ranger_adabelief\n",
            "  Downloading ranger_adabelief-0.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from ranger_adabelief) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->ranger_adabelief) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->ranger_adabelief) (1.3.0)\n",
            "Installing collected packages: ranger_adabelief\n",
            "Successfully installed ranger_adabelief-0.1.0\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)"
      ],
      "metadata": {
        "id": "o46zFFxQ7zZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc7327c-c708-4118-e5a7-223101b3d9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [  1/100] \n",
            "train_loss: 17.59143 \n",
            "valid_loss: 6.94011 \n",
            "Validation loss decreased (inf --> 6.940109).  Saving model ...\n",
            "\n",
            "Epoch: [  2/100] \n",
            "train_loss: 7.26140 \n",
            "valid_loss: 8.77340 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [  3/100] \n",
            "train_loss: 7.74438 \n",
            "valid_loss: 6.08501 \n",
            "Validation loss decreased (6.940109 --> 6.085014).  Saving model ...\n",
            "\n",
            "Epoch: [  4/100] \n",
            "train_loss: 7.59616 \n",
            "valid_loss: 5.94538 \n",
            "Validation loss decreased (6.085014 --> 5.945379).  Saving model ...\n",
            "\n",
            "Epoch: [  5/100] \n",
            "train_loss: 7.53835 \n",
            "valid_loss: 9.23535 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [  6/100] \n",
            "train_loss: 6.31132 \n",
            "valid_loss: 8.62305 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [  7/100] \n",
            "train_loss: 5.97656 \n",
            "valid_loss: 11.51789 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [  8/100] \n",
            "train_loss: 5.40654 \n",
            "valid_loss: 13.46367 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [  9/100] \n",
            "train_loss: 5.13626 \n",
            "valid_loss: 4.96523 \n",
            "Validation loss decreased (5.945379 --> 4.965231).  Saving model ...\n",
            "\n",
            "Epoch: [ 10/100] \n",
            "train_loss: 4.72844 \n",
            "valid_loss: 4.08278 \n",
            "Validation loss decreased (4.965231 --> 4.082782).  Saving model ...\n",
            "\n",
            "Epoch: [ 11/100] \n",
            "train_loss: 3.61482 \n",
            "valid_loss: 4.01136 \n",
            "Validation loss decreased (4.082782 --> 4.011362).  Saving model ...\n",
            "\n",
            "Epoch: [ 12/100] \n",
            "train_loss: 3.16921 \n",
            "valid_loss: 4.15866 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 13/100] \n",
            "train_loss: 2.69931 \n",
            "valid_loss: 3.72856 \n",
            "Validation loss decreased (4.011362 --> 3.728560).  Saving model ...\n",
            "\n",
            "Epoch: [ 14/100] \n",
            "train_loss: 2.42315 \n",
            "valid_loss: 3.58656 \n",
            "Validation loss decreased (3.728560 --> 3.586562).  Saving model ...\n",
            "\n",
            "Epoch: [ 15/100] \n",
            "train_loss: 2.21441 \n",
            "valid_loss: 3.65972 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 16/100] \n",
            "train_loss: 2.11363 \n",
            "valid_loss: 3.63227 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 17/100] \n",
            "train_loss: 1.69046 \n",
            "valid_loss: 3.65656 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 18/100] \n",
            "train_loss: 1.74512 \n",
            "valid_loss: 3.50492 \n",
            "Validation loss decreased (3.586562 --> 3.504918).  Saving model ...\n",
            "\n",
            "Epoch: [ 19/100] \n",
            "train_loss: 1.69402 \n",
            "valid_loss: 3.67704 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 20/100] \n",
            "train_loss: 1.89718 \n",
            "valid_loss: 3.68149 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 21/100] \n",
            "train_loss: 1.76233 \n",
            "valid_loss: 3.38026 \n",
            "Validation loss decreased (3.504918 --> 3.380263).  Saving model ...\n",
            "\n",
            "Epoch: [ 22/100] \n",
            "train_loss: 1.64967 \n",
            "valid_loss: 4.10411 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 23/100] \n",
            "train_loss: 1.65570 \n",
            "valid_loss: 3.58220 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 24/100] \n",
            "train_loss: 1.78221 \n",
            "valid_loss: 4.07715 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 25/100] \n",
            "train_loss: 1.67387 \n",
            "valid_loss: 3.74852 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 26/100] \n",
            "train_loss: 1.67145 \n",
            "valid_loss: 4.62380 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 27/100] \n",
            "train_loss: 1.95179 \n",
            "valid_loss: 8.21556 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 28/100] \n",
            "train_loss: 1.81256 \n",
            "valid_loss: 8.25842 \n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "Epoch: [ 29/100] \n",
            "train_loss: 1.86289 \n",
            "valid_loss: 7.06833 \n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "Epoch: [ 30/100] \n",
            "train_loss: 1.96666 \n",
            "valid_loss: 7.08266 \n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "Epoch: [ 31/100] \n",
            "train_loss: 2.04058 \n",
            "valid_loss: 4.08041 \n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "Epoch: [ 32/100] \n",
            "train_loss: 1.70879 \n",
            "valid_loss: 3.94687 \n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "Epoch: [ 33/100] \n",
            "train_loss: 1.74795 \n",
            "valid_loss: 5.19733 \n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "Epoch: [ 34/100] \n",
            "train_loss: 2.29443 \n",
            "valid_loss: 4.81210 \n",
            "EarlyStopping counter: 13 out of 20\n",
            "\n",
            "Epoch: [ 35/100] \n",
            "train_loss: 2.26744 \n",
            "valid_loss: 4.57722 \n",
            "EarlyStopping counter: 14 out of 20\n",
            "\n",
            "Epoch: [ 36/100] \n",
            "train_loss: 1.75791 \n",
            "valid_loss: 4.62307 \n",
            "EarlyStopping counter: 15 out of 20\n",
            "\n",
            "Epoch: [ 37/100] \n",
            "train_loss: 1.92403 \n",
            "valid_loss: 5.00043 \n",
            "EarlyStopping counter: 16 out of 20\n",
            "\n",
            "Epoch: [ 38/100] \n",
            "train_loss: 2.76673 \n",
            "valid_loss: 5.32826 \n",
            "EarlyStopping counter: 17 out of 20\n",
            "\n",
            "Epoch: [ 39/100] \n",
            "train_loss: 2.07967 \n",
            "valid_loss: 3.68233 \n",
            "EarlyStopping counter: 18 out of 20\n",
            "\n",
            "Epoch: [ 40/100] \n",
            "train_loss: 2.04273 \n",
            "valid_loss: 4.00885 \n",
            "EarlyStopping counter: 19 out of 20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Draw Learning Curves**"
      ],
      "metadata": {
        "id": "nGCIPkQbd8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1\n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "HRqX-uCLWIIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluation using testset**"
      ],
      "metadata": {
        "id": "HFAFvtSxeBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in val_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "id": "gcZBDepNWIKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using test dataset\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #平均誤差\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #誤差標準偏差\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #平均絶対誤差\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #絶対誤差標準偏差\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #平均絶対誤差(四捨五入)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #絶対誤差標準偏差(四捨五入)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)\n"
      ],
      "metadata": {
        "id": "8p5vrzYkJQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "qa1NMrMQJQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bland-Altman-Plot\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fs7DIPTJeVpo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "2d055f33-332e-4554-be0e-5114bf079053"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#線形近似式算出\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# 説明変数xに \"x1\"のデータを使用\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# 目的変数yに \"x2\"のデータを使用\n",
        "y = target.values\n",
        "\n",
        "# 予測モデルを作成（単回帰）\n",
        "clf.fit(x, y)\n",
        "\n",
        "# パラメータ（回帰係数、切片）を抽出\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# パラメータの表示\n",
        "print(\"回帰係数:\", a)\n",
        "print(\"切片:\", b)\n",
        "print(\"決定係数:\", clf.score(x, y))\n",
        "\n",
        "#平均値により補正した値\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#回帰直線により補正した値\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#残差\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#残差\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]"
      ],
      "metadata": {
        "id": "_1MEuRIReVtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765405a5-a377-48d5-c8ca-3f7f2379bf79"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回帰係数: 0.9836896764222057\n",
            "切片: 0.1227305172859765\n",
            "決定係数: 0.6307785162106618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "heTFISEt9D_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#平均近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "\n",
        "# print(\"\")\n",
        "# print('Error<-1 and Error>-1: ' + str(sum((-1 < i < 1 for i in df['Residual_error_2']))))\n",
        "# print('Error<-2 and Error>2: ' + str(sum((-2 < i < 2 for i in df['Residual_error_2']))))\n",
        "# print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "# print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "total_errors = len(df['Residual_error_2'])\n",
        "\n",
        "error_minus1_to_1_count = sum((-1 < i < 1 for i in df['Residual_error_2']))\n",
        "error_minus1_to_1_percentage = (error_minus1_to_1_count / total_errors) * 100\n",
        "\n",
        "error_minus2_to_2_count = sum((-2 < i < 2 for i in df['Residual_error_2']))\n",
        "error_minus2_to_2_percentage = (error_minus2_to_2_count / total_errors) * 100\n",
        "\n",
        "error_less_equal_minus2_count = sum((i <= -2 for i in df['Residual_error_2']))\n",
        "error_less_equal_minus2_percentage = (error_less_equal_minus2_count / total_errors) * 100\n",
        "\n",
        "error_greater_equal_2_count = sum((i >= 2 for i in df['Residual_error_2']))\n",
        "error_greater_equal_2_percentage = (error_greater_equal_2_count / total_errors) * 100\n",
        "\n",
        "print(\"\")\n",
        "print(f'Error<-1 and Error>-1: {error_minus1_to_1_count}/{total_errors} ({error_minus1_to_1_percentage:.2f}%)')\n",
        "print(f'Error<-2 and Error>2: {error_minus2_to_2_count}/{total_errors} ({error_minus2_to_2_percentage:.2f}%)')\n",
        "print(f'Error<=-2: {error_less_equal_minus2_count}/{total_errors} ({error_less_equal_minus2_percentage:.2f}%)')\n",
        "print(f'Error>=2: {error_greater_equal_2_count}/{total_errors} ({error_greater_equal_2_percentage:.2f}%)')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1\n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1\n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')"
      ],
      "metadata": {
        "id": "cHjgGcbk9EBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77e71c4a-1008-4dac-b46f-aac057b9f659"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-93237807d0da>:3: UserWarning: \n",
            "\n",
            "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
            "\n",
            "Please adapt your code to use either `displot` (a figure-level function with\n",
            "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "\n",
            "For a guide to updating your code to use the new functions, please see\n",
            "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
            "\n",
            "  sns.distplot(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: -4.0168873252769484e-16\n",
            "StdError: 1.7314349924009136\n",
            "AveAbsError: 1.351292565592533\n",
            "StdAbsError: 1.078264047113555\n",
            "Round_Corrected_AveAbsError: 1.3316582914572865\n",
            "Round_Corrected_StdAbsError: 1.1723035959295651\n",
            "\n",
            "Error<-1 and Error>-1: 85/199 (42.71%)\n",
            "Error<-2 and Error>2: 155/199 (77.89%)\n",
            "Error<=-2: 23/199 (11.56%)\n",
            "Error>=2: 21/199 (10.55%)\n",
            "\n",
            "Hertel 18mm以上の検出精度\n",
            "TP: 50\n",
            "FP: 10\n",
            "FN: 24\n",
            "TN: 115\n",
            "Sensitivity: 0.6756756756756757\n",
            "Specificity: 0.92\n",
            "Positive predictive value: 0.8333333333333334\n",
            "Negative predictive value: 0.8273381294964028\n",
            "\n",
            "推測18mm以上だが実は16mm未満(過剰): 0例\n",
            "推測16mm未満だが実は18mm以上（見逃がし）: 6例\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cs3KMCqreVvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NS0xdWJndml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLp0xvu8ndtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "-BHc32Av1b2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 5-fold crossvalidation #\n",
        "####################\n",
        "\n",
        "\n",
        "for fold in [0,1,2,3,4]:\n",
        "    # Define dataset & dataloader\n",
        "    train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "    val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "    test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "    #Define Model\n",
        "    model_ft = timm.create_model(model_name = 'efficientnetv2_rw_m', pretrained=True, num_classes=1)\n",
        "    #model_ft = timm.create_model(model_name = 'repvgg_a2', pretrained=True, num_classes=1)\n",
        "    #model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    #optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    !pip install ranger_adabelief\n",
        "    from ranger_adabelief import RangerAdaBelief\n",
        "    optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "    optimizer_ft =  optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=5e-4,\n",
        "        amsbound=False,\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n",
        "\n",
        "    #evaluation using validation dataset\n",
        "    val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "    def my_round(x, d=0):\n",
        "        p = 10 ** d\n",
        "        return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "    model_ft.eval() # prep model for evaluation\n",
        "\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in val_loader:\n",
        "          target = target.view(len(target), 1)\n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "          outputs.append(output[0].item())\n",
        "          targets.append(target[0].item())\n",
        "          #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "          errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "    AbsError = [abs(i) for i in errors]\n",
        "\n",
        "    print('AveError: '+str(statistics.mean(errors)))\n",
        "    print('StdError: '+str(statistics.stdev(errors)))\n",
        "    print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "    print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "    #平均からの差分を補正\n",
        "    corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "    corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "    corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "    round_output = [my_round(i) for i in outputs]\n",
        "    round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "    print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "    print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "    print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "    print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "    # Calculate the probabilities\n",
        "    abs_error_np = np.array(AbsError)\n",
        "    prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "    prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "    print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "    print('Probability of AbsError <= 2:', prob_less_than_2)\n",
        "\n",
        "\"\"\"\n",
        "todo:\n",
        "dfを作成する。画像のpath、label、fold毎の判定結果\n",
        "dfから、\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3uAaRHyehpQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EeuEYbSvne-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fxsq2b9nfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEQIJyYUnfE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFGlh1_7nfHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZ2vGUF4nfMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5XdJerf7Zxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpdE1Mov7Zzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\""
      ],
      "metadata": {
        "id": "vwbKpW0n2ZOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python export.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt --nms --include coreml\n",
        "!python export.py --weights $weight_path --nms --include \"coreml\"\n"
      ],
      "metadata": {
        "id": "Nnxz6A9pyhvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxnOT6leGS9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-EigenCAM**\n",
        "\n",
        "https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
      ],
      "metadata": {
        "id": "YXt1zi1TGm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "DM7nTgGnjwYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "0xDMsB0GkMyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "FzxYfIoemXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "id": "DKEgZ-Laojx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from PIL import Image\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "def parse_detections(results):\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    detections = detections.to_dict()\n",
        "    boxes, colors, names = [], [], []\n",
        "\n",
        "    for i in range(len(detections[\"xmin\"])):\n",
        "        confidence = detections[\"confidence\"][i]\n",
        "        if confidence < 0.2:\n",
        "            continue\n",
        "        xmin = int(detections[\"xmin\"][i])\n",
        "        ymin = int(detections[\"ymin\"][i])\n",
        "        xmax = int(detections[\"xmax\"][i])\n",
        "        ymax = int(detections[\"ymax\"][i])\n",
        "        name = detections[\"name\"][i]\n",
        "        category = int(detections[\"class\"][i])\n",
        "        color = COLORS[category]\n",
        "\n",
        "        boxes.append((xmin, ymin, xmax, ymax))\n",
        "        colors.append(color)\n",
        "        names.append(name)\n",
        "    return boxes, colors, names\n",
        "\n",
        "\n",
        "def draw_detections(boxes, colors, names, img):\n",
        "    for box, color, name in zip(boxes, colors, names):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (xmin, ymin),\n",
        "            (xmax, ymax),\n",
        "            color,\n",
        "            2)\n",
        "\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n",
        "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
        "img = cv2.resize(img, (640, 640))\n",
        "rgb_img = img.copy()\n",
        "img = np.float32(img) / 255\n",
        "transform = transforms.ToTensor()\n",
        "tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "target_layers = [model.model.model.model[-2]]\n",
        "\n",
        "results = model([rgb_img])\n",
        "boxes, colors, names = parse_detections(results)\n",
        "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
        "Image.fromarray(detections)"
      ],
      "metadata": {
        "id": "i_WLp1kGjvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "・外部のデータセット（Treated）を洗い出し\n",
        "\n",
        "・内部のデータセットをさらに水増し\n",
        "\n",
        "・内部および外部データセットより、test用各100枚（grav50枚、cont50枚）を抜き出しておき、合体する\n",
        "\n",
        "・既存のYOLOv5を用いてbounding boxを抜き出し、新たにトレーニングする"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}