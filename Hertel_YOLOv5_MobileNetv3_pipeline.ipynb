{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Hertel_YOLOv5_MobileNetv3_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation YOLOv5-MobileNetv3 pipeline**\n",
        "\n",
        "Train YOLOv5\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlibで目が2つ検出されるものを抜き出す\n",
        "YOLOv5を用いて左右とバウンディングボックスを認識させる\n",
        "抜き出した画像についてMobileNetV3で回帰（5-fold ensemble）を行う\n",
        "スマホに実装\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###はじめの設定"
      ],
      "metadata": {
        "id": "TEGv1MI4DnFM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47d530c-5e2d-4e32-e09c-abdc27bd2060"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 13 08:11:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362557b3-9966-413c-a3cd-0d8b5474752b"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.99"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd52f5a-fcd3-4da2-91ba-1e9e1305a696"
      },
      "source": [
        "'''\n",
        "Google Colabをマウント\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#フォルダ設定\n",
        "#親フォルダ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#元画像フォルダ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#元画像をコピー\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#切りぬいた画像を保存するフォルダ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#トレーニングされたYOLOv5で切り抜いた画像を保存するフォルダ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSVファイルのフォルダ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Refresh folder (内容が削除されるので注意！！) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# parent_dirがあれば削除する\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# 新しくparent_dirを作成する\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirを新規に作成する\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dirにdataset_dir直下のファイルをすべてコピーする\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"処理が完了しました。\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "EnV1hEgIu87W",
        "outputId": "9a63ef42-ae9d-4448-d096-002d30eebd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# parent_dirがあれば削除する\\nif os.path.exists(parent_dir):\\n    shutil.rmtree(parent_dir)\\n\\n# 新しくparent_dirを作成する\\nos.makedirs(parent_dir)\\n\\n# orig_dir, out_dirを新規に作成する\\nos.makedirs(orig_dir)\\nos.makedirs(out_dir)\\nos.makedirs(cropped_dir)\\n\\n# orig_dirにdataset_dir直下のファイルをすべてコピーする\\nfile_list = os.listdir(dataset_dir)\\nfor filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\\n    src_path = os.path.join(dataset_dir, filename)\\n    dst_path = os.path.join(orig_dir, filename)\\n    shutil.copy(src_path, dst_path)\\n\\nprint(\"処理が完了しました。\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeを用いて目を検出**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**目が2つ以上検出されたものを抜き出す**\n",
        "\n",
        "dlibで検出されたものから、上下左右に0.1倍ずつ拡大した範囲を抜き出している"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #フォルダ数の分だけ\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # 画像グレースケール化\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix以上のもので目に見えるものを抽出\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # 眼検出判定\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #画像の切り抜きと保存（2個以上検出の時に限る）\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #本来の切り抜きより幅の0.1倍ずつ水増しする\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #横幅の半分より左にあるのは右眼\n",
        "                      else:\n",
        "                          side = \"L\" #横幅の半分よりより右にあるのは左眼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #対応表の作成\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ここで、目以外が誤検出されているものを手動で抜き出して削除する**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csvから、削除して画像のパスが存在しなくなっている行を削除する\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrameを読み込む\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# 存在しない画像パスをチェックし、そのリストを保持する\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# 存在しない画像パスを表示\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # 存在しない画像パスの行を削除\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # 更新されたDataFrameを保存する\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "35178cc0-37a3-4148-b3fd-fa12e141342a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframeの整理**\n",
        "\n",
        "・ hertel_dfを参照して、coordinates_dfにヘルテル値を記入する\n",
        "\n",
        "・idが\"16_R, 16_L\"という形式になるようにデータフレームを整理する"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6e553e1b-105b-4366-fd88-7dd9508f2c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7915ad4-d6e5-4775-ae5c-c058cfd2cf9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-817e306f-678a-4ab3-a935-a6e022b87f2b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-817e306f-678a-4ab3-a935-a6e022b87f2b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-817e306f-678a-4ab3-a935-a6e022b87f2b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "画像パスの抽出（RLともに揃っているもの）\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "画像の分割 train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "フォルダの作成\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "画像のコピー\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d78c062-ef26-47b7-b3b2-e8be204b050a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|██████████| 760/760 [00:09<00:00, 82.08it/s]\n",
            "Copying valid images: 100%|██████████| 190/190 [00:02<00:00, 67.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\" {side} {cx} {cy} {w} {h}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# trainとvalidのディレクトリパス\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# trainディレクトリでラベルファイルを生成\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# validディレクトリでラベルファイルを生成\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e09a5a-a1ab-48cf-b03b-385a93882b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 760/760 [00:05<00:00, 132.46it/s]\n",
            "Processing images: 100%|██████████| 190/190 [00:01<00:00, 142.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## バウンディングボックスのサンプル描画 ##\n",
        "## (これは実行しなくて良い)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# バウンディングボックスを描画する関数\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            class_id, cx, cy, bw, bh = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得する関数\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# 画像パスとラベルファイルパス\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# 画像のサイズを取得\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "id": "OhSI3Wc_-ylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "4237beaa-b891-4844-fbf8-9e1204bc6422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5**"
      ],
      "metadata": {
        "id": "473ybkfvE_4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup YOLOv5"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5_iFish --> train_batchの精度が低いので一旦却下とした\n",
        "# %cd $out_dir\n",
        "# !git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationを実装したバージョン\n",
        "# %cd yolov5-iFish\n",
        "# %pip install -qr requirements.txt\n",
        "\n",
        "# import torch\n",
        "# import utils\n",
        "# display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "x-33rbP1-iQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "%cd $out_dir\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "vdYFiF39egKw",
        "outputId": "d6748592-94d0-408c-a413-b6ed0a67e7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train YOLOv5##"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "id": "7iRcXa8I-iSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 途中から\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/data.yaml --resume $out_dir/yolov5/runs/train/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = f\"{out_dir}/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)\n",
        "\n",
        "dst_pt = f\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2",
        "outputId": "50b709c2-ed57-4272-e70d-d3438424c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##YOLOv5 Inference original dataset"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d93b49-888a-4615-b755-12b658a234af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "#weight = f\"{out_dir}/dataset_uni/eyecrop-yolov5n-iFish_169epoch.pt\"\n",
        "shutil.copy(\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5_backup/eyecrop-yolov5n-300epoch.pt\", \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\")\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "\n",
        "# もともとのデータセット\n",
        "orig_dir = orig_dir #元画像\n",
        "cropped_dir = cropped_dir #YOLOv5で切り抜いた画像用\n",
        "\n",
        "# if os.path.exists(cropped_dir):\n",
        "#     shutil.rmtree(cropped_dir)\n",
        "# os.makedirs(cropped_dir)\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image作成、リサイズ\n",
        "    # 元の画像のサイズを取得\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # 正方形のサイズを決定（元の画像の長辺に合わせる）\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # 正方形のキャンバスを作成（背景は黒）\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # 元の画像を正方形の画像の中央に配置するための開始点（x,y）を計算\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # 元の画像を正方形のキャンバスにコピーする\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    # 変換した画像を保存\n",
        "    return letterbox_img_resized"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## バウンディングボックス&切り抜き demo ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    print(img)\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #バウンディングボックスが左に切れる場合の対処\n",
        "            x1 = 0\n",
        "        if y2>img_width: #バウンディングボックスが右に切れる場合の対処\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "P-J5WiSyXGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# ## 切り抜き + 保存 ##\n",
        "# #################\n",
        "# \"\"\"\n",
        "# Letterbox & 250px正方形にリサイズ\n",
        "# cropped_dirに保存\n",
        "# \"\"\"\n",
        "\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "# start_index = 0\n",
        "# end_index = len(os.listdir(orig_dir))\n",
        "\n",
        "# class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# for i in range(start_index, end_index):\n",
        "#     img = image_path[i]\n",
        "#     pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "#     img_cv2 = cv2.imread(img)\n",
        "#     img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "#     for bbox in pred[0]:\n",
        "#         x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "#         prob = bbox[4].item()\n",
        "#         class_name = class_names[bbox[5].item()]\n",
        "\n",
        "#         #print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "#         # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "#         img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "#         #print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "#         padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "#         padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "#         x1 = x1 - padding_x\n",
        "#         y1 = y1 - padding_y\n",
        "#         x2 = x2 - padding_x\n",
        "#         y2 = y2 - padding_y\n",
        "#         #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "#         # Crop and save the image\n",
        "#         mag = 640 / img_cv2.shape[1]\n",
        "#         cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "#         letterbox_img = make_letterbox_image(cropped_img)\n",
        "#         #cv2_imshow(letterbox_img)\n",
        "\n",
        "#         base_name = os.path.splitext(os.path.basename(img))[0]\n",
        "#         cropped_img_path = os.path.join(f\"{cropped_dir}/cropped_images\", f\"{base_name}_{class_name}.png\")\n",
        "#         cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "#         print(f\"succefully saved, image {i}: {cropped_img_path}\")\n"
      ],
      "metadata": {
        "id": "iAelqChiXGne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Load image paths\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 0\n",
        "end_index = len(image_paths)\n",
        "\n",
        "# Define class names\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "# List to hold images with incorrect detections\n",
        "incorrect_detections = []\n",
        "\n",
        "# Iterate over images\n",
        "for i in range(start_index, end_index):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "    # Check if the detections are not equal to 2\n",
        "    if len(pred[0]) != 2:\n",
        "        incorrect_detections.append((img_path, len(pred[0])))\n",
        "        continue  # Skip the rest of the loop and do not process this image\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "        if x1<0: #バウンディングボックスが左に切れる場合の対処\n",
        "            x1 = 0\n",
        "        if y2>img_width: #バウンディングボックスが右に切れる場合の対処\n",
        "            x2 = img_width\n",
        "\n",
        "        # Crop and resize logic (assuming make_letterbox_image is defined)\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "        letterbox_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "\n",
        "        # Save the cropped image\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        class_name = class_names[class_num]\n",
        "        cropped_img_path = os.path.join(f\"{cropped_dir}\", f\"{base_name}_{class_name}.png\")\n",
        "        cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "        print(f\"Successfully saved, image {i}: {cropped_img_path}\")\n",
        "\n",
        "# Output images with incorrect detections\n",
        "print(\"Images with incorrect detections:\")\n",
        "for img_path, num_detections in incorrect_detections:\n",
        "    print(f\"{img_path} - Number of detections: {num_detections}\")\n",
        "incorrect_paths = [path for path, _ in incorrect_detections]"
      ],
      "metadata": {
        "id": "WdowrfNNRhNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## うまく左右眼を検出できなかった例の確認 ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = [\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig/92.JPG\"]\n",
        "#image_path = incorrect_paths\n",
        "start_index = 0\n",
        "end_index = len(image_path)\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img_cv2_resized, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "PCBjJv1KlEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder\n",
        "%cd \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5\"\n",
        "folder_path = \"dataset_yolo_cropped\"\n",
        "\n",
        "# Zip the folder\n",
        "!zip -r /content/cropped_images.zip \"$folder_path\"\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('/content/cropped_images.zip')"
      ],
      "metadata": {
        "id": "vEepFKfW2HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To do\n",
        "\n",
        "・Group 5-fold split\n",
        "・MobileNetV3でcross validation --> 精度プロット作成\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_estimation_crossvalidation_noTestset.ipynb\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_ensemble_quick.ipynb\n",
        "・当院データセットでtestする\n",
        "'''"
      ],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "def letterbox_image(image_path, output_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Calculate padding sizes\n",
        "    h, w = image.shape[:2]\n",
        "    max_side = max(h, w)\n",
        "    top = bottom = (max_side - h) // 2\n",
        "    left = right = (max_side - w) // 2\n",
        "\n",
        "    # Add black padding to make the image square\n",
        "    square_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "    # Resize the image to 640x640\n",
        "    resized_image = cv2.resize(square_image, (1080, 1080))\n",
        "\n",
        "    # Save the processed image\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "def select_and_process_files(input_directory, output_directory, num_files=5):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # List all files in the input directory\n",
        "    all_files = [f for f in os.listdir(input_directory) if os.path.isfile(os.path.join(input_directory, f))]\n",
        "\n",
        "    # Randomly select num_files files\n",
        "    selected_files = random.sample(all_files, num_files)\n",
        "\n",
        "    # Copy and process selected files\n",
        "    for file in selected_files:\n",
        "        input_path = os.path.join(input_directory, file)\n",
        "        output_path = os.path.join(output_directory, file)\n",
        "        shutil.copy(input_path, output_path)\n",
        "        letterbox_image(output_path, output_path)\n",
        "\n",
        "    return selected_files\n",
        "\n",
        "# Example usage\n",
        "input_directory = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig'\n",
        "output_directory = '/content/letterbox'  # Replace with your desired output directory path\n",
        "if os.path.exists(output_directory):\n",
        "    shutil.rmtree(output_directory)\n",
        "os.makedirs(output_directory)\n",
        "selected_files = select_and_process_files(input_directory, output_directory)\n",
        "print(\"Selected and processed files:\", selected_files)\n"
      ],
      "metadata": {
        "id": "pUYctpy0gNIN",
        "outputId": "fd06c8dc-747a-4d25-9bf2-a1e8c2c62dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected and processed files: ['917.JPG', '42.JPG', '333.JPG', '46.JPG', '530.JPG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3 using cropped images**\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_yolo_cropped/cropped_images\n",
        "の画像を手動で確認、不適切な画像を削除\n",
        "\n",
        "・https://tcd-theme.com/2019/12/mac-zip-compression.html\n",
        "\n",
        "を参考にして圧縮\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_cropped_for_MobileNet_training/cropped_images.zipとしてアップロード"
      ],
      "metadata": {
        "id": "lvkfZ9aQ9S1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv5で抜き出した画像を規定のフォルダに移動"
      ],
      "metadata": {
        "id": "rKFHyFisE2Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5で抜き出した画像を規定のフォルダに移動\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# parent_folder = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training'\n",
        "\n",
        "# # zipファイルのパス\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/cropped_images.zip'\n",
        "\n",
        "# # zipファイルを解凍\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(parent_folder)\n"
      ],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ここから"
      ],
      "metadata": {
        "id": "vdOpequeE781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer --q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install pingouin --q\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True"
      ],
      "metadata": {
        "id": "q7Zp_wf_JQjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7ef5ea-1306-4473-b810-f7e284c0d891"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Random Seed:  1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training\"\n",
        "# os.chdir(path)\n",
        "\n",
        "# contains train, val\n",
        "DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/dataset_yolo_cropped\"\n",
        "#DATASET_PATH = r\"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_250px_uni_periocular\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_unilateral.csv\"\n",
        "LOG_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/log_multi.txt\"\n",
        "ROC_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/roc_multi.png\"\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# transforms param\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])])"
      ],
      "metadata": {
        "id": "8lzHTjGEL8eQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5-Foldに分割"
      ],
      "metadata": {
        "id": "kW9iYtvtOPiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(DATASET_PATH)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"patiend num: {}\".format(len(id)))"
      ],
      "metadata": {
        "id": "eMO4j991OO8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443c02c3-cccb-4c59-be7f-ee11c1a8c7b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patiend num: 1986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testsetなし。Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=patient):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(path_list[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(path_list[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(train_set[0])[0]))"
      ],
      "metadata": {
        "id": "4QNmg_1gOO9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca46918a-1975-4d38-eea2-1ccd24fceb78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1588\n",
            "val_dataset: 398\n",
            "extracted_id (example): 870_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split ＋　Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9で分割\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "groups = patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "zjQNXUD2OO_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da839bf8-332d-4e3f-e7e3-f5375160e456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1431\n",
            "val_dataset: 358\n",
            "test_dataset: 197\n",
            "\n",
            "extracted_id (example): 563_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シンプルなK-fold (group_K_Foldではない)\n",
        "\n",
        "num_folds = 5 #number of folds\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "#まず全体の1割をテストセットとしてよけておく\n",
        "remain_set, test_set = train_test_split(path_list, test_size=0.1, shuffle=True, random_state=random_seed)\n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_set):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "JatpoewI699u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3c4782-6ce1-4274-d689-6778e6ae47c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1429\n",
            "val_dataset: 358\n",
            "test_dataset: 199\n",
            "\n",
            "extracted_id (example): 830_L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Datasets"
      ],
      "metadata": {
        "id": "_hiIomNwbBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "\n",
        "    def __init__(self, img_list, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for image_path in img_list:\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([image_path, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.item_paths[index][0]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image).float()\n",
        "        hertel = self.item_paths[index][1]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "#test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "#test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))"
      ],
      "metadata": {
        "id": "IteKm-r5brwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013b260b-ebe0-4e4f-accd-ad64b4034f1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1588\n",
            "val_dataset_size: 398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test with early-stopping"
      ],
      "metadata": {
        "id": "JGWnnohkWBpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size,optimizer, patience, n_epochs, device,  alpha=0):\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "\n",
        "            #l2_normalization\n",
        "            l2 = torch.tensor(0., requires_grad=True)\n",
        "            for w in model.parameters():\n",
        "                l2 = l2 + torch.norm(w)**2\n",
        "            loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(n_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "\n",
        "        print(print_msg)\n",
        "\n",
        "\n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "iJyYRN8SOPB0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ConvNetの調整"
      ],
      "metadata": {
        "id": "E2WdAbf9WMMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###オリジナルRepVGG-A2使用\n"
      ],
      "metadata": {
        "id": "w4uM5uvHV8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc1 = nn.Linear(in_features=1408, out_features=512) #out_featuresを1に\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep_learning/666mai_dataset/RepVGG-A2-train.pth\"))\n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "xooKuBLyV_xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Timm使用の場合"
      ],
      "metadata": {
        "id": "DLA4qqaoYeCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "efFP6EU1WH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc = nn.Linear(in_features=1280, out_features=1) #モデルに応じてin_featuresを調整、out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x) #dropoutを1層追加\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # fc layer 2つのバージョン\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc1 = nn.Linear(in_features=1280, out_features=512) #モデルに応じてin_featuresを調整\n",
        "#         self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Batch_norm plus dropout (for RepVGG_A2: イマイチ)\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.bn = nn.BatchNorm2d(1408)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "#         self.dropout = nn.Dropout(0.25)\n",
        "#         self.fc = nn.Linear(in_features=12672, out_features=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = self.bn(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.maxpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "#model_ft = timm.create_model(model_name = 'efficientnetv2_rw_m', pretrained=True, num_classes=1)\n",
        "#model_ft = timm.create_model(model_name = 'repvgg_a2', pretrained=True, num_classes=1)\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "#model_ft = mod_CNNModel()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "#optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "optimizer_ft =  optim.AdaBound(\n",
        "    model_ft.parameters(),\n",
        "    lr= 1e-3,\n",
        "    betas= (0.9, 0.999),\n",
        "    final_lr = 0.1,\n",
        "    gamma=1e-3,\n",
        "    eps= 1e-8,\n",
        "    weight_decay=5e-4,\n",
        "    amsbound=False,\n",
        ")"
      ],
      "metadata": {
        "id": "-VvPVPdJWIER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bcb424-1d0e-481e-8cc6-f1efb5fcf74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ranger_adabelief in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from ranger_adabelief) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger_adabelief) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->ranger_adabelief) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->ranger_adabelief) (1.3.0)\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)"
      ],
      "metadata": {
        "id": "o46zFFxQ7zZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Draw Learning Curves"
      ],
      "metadata": {
        "id": "nGCIPkQbd8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1\n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "HRqX-uCLWIIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation using testset"
      ],
      "metadata": {
        "id": "HFAFvtSxeBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#既存のモデルをロードする時\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_0.pth\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "# state_dict をロードしてモデルに適用\n",
        "model_ft.load_state_dict(torch.load(model_weight))\n",
        "model_ft.to(device)"
      ],
      "metadata": {
        "id": "WqAWYvwtHbMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in val_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "id": "gcZBDepNWIKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fcd854-89cc-471c-ddea-ee3b997de261"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: -0.5527114029505744\n",
            "StdError: 1.7488486740381806\n",
            "AveAbsError: 1.423884748813495\n",
            "StdAbsError: 1.1541994525796486\n",
            "\n",
            "Corrected_AveAbsError: 1.3640384237223395\n",
            "Corrected_StdAbsError: 1.0923297173249142\n",
            "Round_Corrected_AveAbsError: 1.3165829145728642\n",
            "Round_Corrected_StdAbsError: 1.1577257223848767\n",
            "Probability of AbsError <= 1: 0.45979899497487436\n",
            "Probability of AbsError <= 2: 0.7412060301507538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using test dataset\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #平均誤差\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #誤差標準偏差\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #平均絶対誤差\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #絶対誤差標準偏差\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #平均絶対誤差(四捨五入)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #絶対誤差標準偏差(四捨五入)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)\n"
      ],
      "metadata": {
        "id": "8p5vrzYkJQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "qa1NMrMQJQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bland-Altman-Plot\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fs7DIPTJeVpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#線形近似式算出\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# 説明変数xに \"x1\"のデータを使用\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# 目的変数yに \"x2\"のデータを使用\n",
        "y = target.values\n",
        "\n",
        "# 予測モデルを作成（単回帰）\n",
        "clf.fit(x, y)\n",
        "\n",
        "# パラメータ（回帰係数、切片）を抽出\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# パラメータの表示\n",
        "print(\"回帰係数:\", a)\n",
        "print(\"切片:\", b)\n",
        "print(\"決定係数:\", clf.score(x, y))\n",
        "\n",
        "#平均値により補正した値\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#回帰直線により補正した値\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#残差\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#残差\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]"
      ],
      "metadata": {
        "id": "_1MEuRIReVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "heTFISEt9D_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#平均近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "\n",
        "# print(\"\")\n",
        "# print('Error<-1 and Error>-1: ' + str(sum((-1 < i < 1 for i in df['Residual_error_2']))))\n",
        "# print('Error<-2 and Error>2: ' + str(sum((-2 < i < 2 for i in df['Residual_error_2']))))\n",
        "# print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "# print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "total_errors = len(df['Residual_error_2'])\n",
        "\n",
        "error_minus1_to_1_count = sum((-1 < i < 1 for i in df['Residual_error_2']))\n",
        "error_minus1_to_1_percentage = (error_minus1_to_1_count / total_errors) * 100\n",
        "\n",
        "error_minus2_to_2_count = sum((-2 < i < 2 for i in df['Residual_error_2']))\n",
        "error_minus2_to_2_percentage = (error_minus2_to_2_count / total_errors) * 100\n",
        "\n",
        "error_less_equal_minus2_count = sum((i <= -2 for i in df['Residual_error_2']))\n",
        "error_less_equal_minus2_percentage = (error_less_equal_minus2_count / total_errors) * 100\n",
        "\n",
        "error_greater_equal_2_count = sum((i >= 2 for i in df['Residual_error_2']))\n",
        "error_greater_equal_2_percentage = (error_greater_equal_2_count / total_errors) * 100\n",
        "\n",
        "print(\"\")\n",
        "print(f'Error<-1 and Error>-1: {error_minus1_to_1_count}/{total_errors} ({error_minus1_to_1_percentage:.2f}%)')\n",
        "print(f'Error<-2 and Error>2: {error_minus2_to_2_count}/{total_errors} ({error_minus2_to_2_percentage:.2f}%)')\n",
        "print(f'Error<=-2: {error_less_equal_minus2_count}/{total_errors} ({error_less_equal_minus2_percentage:.2f}%)')\n",
        "print(f'Error>=2: {error_greater_equal_2_count}/{total_errors} ({error_greater_equal_2_percentage:.2f}%)')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1\n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1\n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')"
      ],
      "metadata": {
        "id": "cHjgGcbk9EBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference pipeline YOLOv5→MobileNetv3**"
      ],
      "metadata": {
        "id": "450sY_r6ZvZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imgs_dir = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/HerteL_images_TED\"\n",
        "imgs_dir = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Hertel_images_TUMOR\"\n",
        "\n",
        "#csv_path = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Img_Hertel_list_TED.csv\"\n",
        "csv_path = \"/content/drive/MyDrive/Deep_learning/Hertel_study/Hertel_dataset_handai/Img_Hertel_list_TUMOR.csv\"\n",
        "\n",
        "yolo_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\"\n",
        "mobilenet_weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_yolo_mobilenetv3_group_notestset/mobilenetv3_large_100_0.pth\""
      ],
      "metadata": {
        "id": "Cs3KMCqreVvN"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List comprehension to create a list of file paths\n",
        "imgs_list = [os.path.join(imgs_dir, path) for path in os.listdir(imgs_dir)]\n",
        "img_paths = imgs_list[0:]  # Displaying the first five file paths"
      ],
      "metadata": {
        "id": "2NS0xdWJndml"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# 結果を格納するためのリスト\n",
        "labels_list = []\n",
        "\n",
        "# img_pathの各パスに対して\n",
        "for path in img_paths:\n",
        "    # パスからベースネーム（ファイル名）を抽出\n",
        "    base_name = os.path.basename(path)\n",
        "\n",
        "    # ベースネームに対応する行を見つける\n",
        "    row = df[df['file_name'] == base_name]\n",
        "\n",
        "    # \"Hertel_R\" と \"Hertel_L\" の値を取得し、リストに追加\n",
        "    if not row.empty:\n",
        "        labels_list.append(row[['file_name','Hertel_R', 'Hertel_L']].values[0].tolist())\n",
        "    else:\n",
        "        # 対応する行がない場合は、None または適切な値を追加\n",
        "        labels_list.append(None)\n",
        "        print(f\"not found in csv file, {base_name}\")\n",
        "\n",
        "# 新しいデータフレームを作成\n",
        "new_df = pd.DataFrame(labels_list, columns=['file_name', 'Hertel_R', 'Hertel_L'])\n",
        "\n",
        "# 予測値記録用にデータフレームに欄を作っておく\n",
        "new_df['pred_R'] = pd.NA\n",
        "new_df['pred_L'] = pd.NA\n",
        "\n",
        "# 新しいデータフレームを表示\n",
        "#print(new_df)"
      ],
      "metadata": {
        "id": "rj2aAK7YYvTu"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Y8ElaId4Ct",
        "outputId": "04d98909-02a5-4e75-e149-91c926325448"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 27.5/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "# state_dict をロードしてモデルに適用\n",
        "model_ft.load_state_dict(torch.load(mobilenet_weight))\n",
        "model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "metadata": {
        "id": "zEDchCiOttlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image作成、リサイズ\n",
        "    # 元の画像のサイズを取得\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # 正方形のサイズを決定（元の画像の長辺に合わせる）\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # 正方形のキャンバスを作成（背景は黒）\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # 元の画像を正方形の画像の中央に配置するための開始点（x,y）を計算\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # 元の画像を正方形のキャンバスにコピーする\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    return letterbox_img_resized\n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image):\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(250),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.1,saturation=0.1, hue=0.2),\n",
        "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#MobileNetv3の前処理\n",
        "def image_eval(image_tensor):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    pred = round(output.item(), 2)\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)"
      ],
      "metadata": {
        "id": "tMyb62rOoD2F"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display  # For displaying images in Colab\n",
        "\n",
        "# Assuming yolo_weight and other necessary variables are defined elsewhere\n",
        "\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "def process_images(image_paths, labels_llist):\n",
        "    for i, (img_path, label) in enumerate(zip(image_paths, labels_list)):\n",
        "        pred = inference(img_path, yolo_weight)  # Ensure this function is defined and weight is loaded\n",
        "\n",
        "        img_cv2 = cv2.imread(img_path)\n",
        "        img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Resize width to 640px\n",
        "\n",
        "        if len(pred[0]) != 2:\n",
        "            print(\"Number of eyes are not 2 in image:\", img_path)\n",
        "\n",
        "        for bbox in pred[0]:\n",
        "            x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6]).tolist()\n",
        "\n",
        "            # Adjust bounding box coordinates\n",
        "            img_height, img_width, _ = img_cv2_resized.shape\n",
        "            padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "            padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "            x1 = max(x1 - padding_x, 0)\n",
        "            y1 = max(y1 - padding_y, 0)\n",
        "            x2 = min(x2 - padding_x, img_width)\n",
        "            y2 = min(y2 - padding_y, img_height)\n",
        "\n",
        "            # Crop and resize logic\n",
        "            mag = 640 / img_cv2.shape[1]\n",
        "            cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)]\n",
        "            #resized_img = make_letterbox_image(cropped_img)  # Ensure this function is defined\n",
        "            resized_img = cv2.resize(cropped_img,(250,250))\n",
        "\n",
        "            # Display image\n",
        "            #cv2_imshow(resized_img)\n",
        "\n",
        "            #MobileNet用の前処理\n",
        "            resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
        "            resized_img= Image.fromarray(resized_img)\n",
        "            image_tensor = image_transform(resized_img)\n",
        "            pred = image_eval(image_tensor)\n",
        "            if class_num == 0: #右眼\n",
        "                print(f\"file_name: {label[0]}, side: R, pred: {pred}, label: {label[1]}\")\n",
        "                new_df.at[i, \"pred_R\"] = pred\n",
        "            elif class_num == 1: #左眼\n",
        "                print(f\"file_name: {label[0]}, side: R, pred: {pred}, label: {label[2]}\")\n",
        "                new_df.at[i, \"pred_L\"] = pred\n",
        "\n",
        "# Example usage\n",
        "process_images(img_paths, labels_list)\n",
        "new_df"
      ],
      "metadata": {
        "id": "wQazEg8Tcivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_targets = pd.concat([new_df[\"Hertel_R\"], new_df[\"Hertel_L\"]], ignore_index=True)\n",
        "df_outputs = pd.concat([new_df[\"pred_R\"], new_df[\"pred_L\"]], ignore_index=True)\n",
        "df_concat = pd.concat([df_targets, df_outputs], axis=1).dropna()\n",
        "targets = df_concat[0].tolist()\n",
        "outputs = df_concat[1].tolist()\n",
        "errors = (df_concat[0] - df_concat[1]).tolist()"
      ],
      "metadata": {
        "id": "DFDI3frKLj5E"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #平均誤差\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #誤差標準偏差\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #平均絶対誤差\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #絶対誤差標準偏差\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #平均絶対誤差(四捨五入)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #絶対誤差標準偏差(四捨五入)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jac7UNZ9Jf4r",
        "outputId": "ae6ff47e-0e4a-4d0e-bfff-e6cab616de3b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: 0.7042749529190208\n",
            "StdError: 3.6201429128093703\n",
            "AveAbsError: 2.8783239171374766\n",
            "StdAbsError: 2.3026058283041193\n",
            "\n",
            "Corrected_AveError: -1.4085499058380406\n",
            "Corrected_StdError: 3.6201429128093703\n",
            "Corrected_AveAbsError: 2.979513124155468\n",
            "Corrected_StdAbsError: 2.4897675619913793\n",
            "Round_Corrected_AveAbsError: 2.983050847457627\n",
            "Round_Corrected_StdAbsError: 2.516679250193209\n",
            "\n",
            "Probability of AbsError <= 1: 0.24858757062146894\n",
            "Probability of AbsError <= 2: 0.416195856873823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "fcNiZ2NIQKMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data provided by the user\n",
        "data = {\n",
        "    \"file_name\": [\n",
        "        \"129-20191002-76-162415_118f83a916863e06de64edf...\",\n",
        "        \"9129-20191002-49-140906_ac8b709121a742083607af...\",\n",
        "        \"6336-20190213-79-132839_e0f0869a867604753f5741...\",\n",
        "        \"818-20181130-35-090753_614e05ca1e8e501ca739873...\",\n",
        "        \"1910-20181114-49-103737_71dcc2f31d839f0c77b9f2...\"\n",
        "    ],\n",
        "    \"Hertel_R\": [18, 18, 19, 16, 14],\n",
        "    \"Hertel_L\": [19, 19, 17, 15, 14],\n",
        "    \"pred_R\": [14, 15, 15, 15, 14],\n",
        "    \"pred_L\": [12, 14, 13, 15, 14]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# # Plotting with seaborn\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(data=df, x=\"Hertel_R\", y=\"pred_R\")\n",
        "# plt.title(\"Comparison of Hertel and Predicted Values\")\n",
        "# plt.xlabel(\"Hertel Value\")\n",
        "# plt.ylabel(\"Predicted Value\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "FzEwxwJ4kfP7",
        "outputId": "0cfe324a-98fe-4bf7-8c5f-b3f90b102e53"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0nD3hWRd4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTaiZLdCd4KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLp0xvu8ndtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "-BHc32Av1b2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 5-fold crossvalidation #\n",
        "####################\n",
        "\n",
        "model_name = 'repvgg_a2'\n",
        "# model_name = 'efficientnetv2_rw_m'\n",
        "# model_name = 'mobilenetv3_large_100'\n",
        "\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "# Initialize lists to store AbsErrors for each fold\n",
        "abs_errors_fold = [[] for _ in range(5)]\n",
        "\n",
        "\n",
        "for fold in [0,1,2,3,4]:\n",
        "    # Define dataset & dataloader\n",
        "    train_dataset = Create_Datasets(train_set[fold], CSV_PATH, train_data_transforms)\n",
        "    val_dataset = Create_Datasets(val_set[fold], CSV_PATH, val_data_transforms)\n",
        "    #test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "    #test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "    #Define Model\n",
        "    model_ft = timm.create_model(model_name = model_name, pretrained=True, num_classes=1)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    # !pip install ranger_adabelief\n",
        "    # from ranger_adabelief import RangerAdaBelief\n",
        "    # optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "    # optimizer_ft =  optim.AdaBound(\n",
        "    #     model_ft.parameters(),\n",
        "    #     lr= 1e-3,\n",
        "    #     betas= (0.9, 0.999),\n",
        "    #     final_lr = 0.1,\n",
        "    #     gamma=1e-3,\n",
        "    #     eps= 1e-8,\n",
        "    #     weight_decay=5e-4,\n",
        "    #     amsbound=False,\n",
        "    # )\n",
        "\n",
        "    # Train model\n",
        "    EPOCH = 1\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n",
        "\n",
        "    #evaluation using validation dataset\n",
        "    val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "\n",
        "    model_ft.eval() # prep model for evaluation\n",
        "\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in val_loader:\n",
        "        target = target.view(len(target), 1)\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        target = target.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "        outputs.append(output[0].item())\n",
        "        targets.append(target[0].item())\n",
        "        #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "        errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "    AbsError = [abs(i) for i in errors]\n",
        "\n",
        "    # Append AbsError from current fold to the corresponding list\n",
        "    abs_errors_fold[fold].extend(AbsError)\n",
        "\n",
        "\n",
        "\n",
        "    print('AveError: '+str(statistics.mean(errors)))\n",
        "    print('StdError: '+str(statistics.stdev(errors)))\n",
        "    print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "    print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "    #平均からの差分を補正\n",
        "    corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "    corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "    corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "    round_output = [my_round(i) for i in outputs]\n",
        "    round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "    print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "    print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "    print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "    print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "    # Calculate the probabilities\n",
        "    abs_error_np = np.array(AbsError)\n",
        "    prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "    prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "    print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "    print('Probability of AbsError <= 2:', prob_less_than_2)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df_abs_errors = pd.DataFrame({\n",
        "    f'Fold_{i}': errors for i, errors in enumerate(abs_errors_fold)\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Draw Box_plot\n",
        "# Melting the DataFrame to long format suitable for Seaborn\n",
        "df_long = df_abs_errors.melt(var_name='Fold', value_name='AbsError')\n",
        "\n",
        "# Create the boxplot\n",
        "\n",
        "sns.boxplot(x='Fold', y='AbsError', data=df_long)\n",
        "# Adding titles and labels\n",
        "plt.title('Boxplot of Absolute Errors for Each Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Absolute Error')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_abs_errors.to_csv(f'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/crossvalidation_{model_name}.csv', index=False)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "todo:\n",
        "dfを作成する。画像のpath、label、fold毎の判定結果\n",
        "dfから、\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3uAaRHyehpQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9aad00a8-a1cf-4b27-e952-783fdc37364d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-34e82c59d329>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#evaluation using validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-68273327f946>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_func, batch_size, optimizer, patience, n_epochs, device, alpha)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;31m# convert batch-size labels to batch-size x 1 tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m#target = target.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0d6a5e19d841>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpilr_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtensor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilr_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhertel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EeuEYbSvne-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fxsq2b9nfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEQIJyYUnfE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFGlh1_7nfHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZ2vGUF4nfMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --include 'coreml' --nms --weights $weight_path"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --include 'coreml' --weights $weight_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3bGseObGPLg",
        "outputId": "2e53b1c9-d7b5-42af-a380-625f4e4f2ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['coreml']\n",
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-300epoch.pt with output shape (1, 25200, 7) (3.7 MB)\n",
            "scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
            "XGBoost version 2.0.1 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "2023-11-11 06:39:03.361674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-11 06:39:03.361740: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-11 06:39:03.361772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "TensorFlow version 2.14.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 7.1...\n",
            "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_targer' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats\n",
            "Tuple detected at graph output. This will be flattened in the converted model.\n",
            "Converting PyTorch Frontend ==> MIL Ops: 100% 607/609 [00:00<00:00, 3968.51 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100% 5/5 [00:00<00:00, 230.02 passes/s]\n",
            "Running MIL default pipeline: 100% 71/71 [00:01<00:00, 44.08 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100% 12/12 [00:00<00:00, 373.03 passes/s]\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export failure ❌ 6.6s: For an ML Program, extension must be .mlpackage (not .mlmodel). Please see https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats to see the difference between neuralnetwork and mlprogram model types.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFoB1lpJGPNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkxZXJEZGPPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**検証用letterbox作成スクリプト**\n",
        "\n",
        "指定したパスの画像を長い辺を一辺としたletterbox(黒塗り)にする"
      ],
      "metadata": {
        "id": "cCGQw9ndwxuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "parent_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig\"\n",
        "basename_list = [\n",
        "    \"22.JPG\",\n",
        "    \"62.JPG\",\n",
        "    \"72.JPG\"\n",
        "]\n",
        "\n",
        "def convert_to_letterbox(path):\n",
        "    # 画像を開く\n",
        "    with Image.open(path) as img:\n",
        "        # 元の画像サイズを取得\n",
        "        width, height = img.size\n",
        "\n",
        "        # 長辺を基準に新しいサイズを計算\n",
        "        if width > height:\n",
        "            new_size = (width, width)\n",
        "        else:\n",
        "            new_size = (height, height)\n",
        "\n",
        "        # 新しい画像を作成（背景は黒）\n",
        "        new_img = Image.new(\"RGB\", new_size, (0, 0, 0))\n",
        "\n",
        "        # 画像を中央に配置\n",
        "        new_img.paste(img, ((new_size[0] - width) // 2, (new_size[1] - height) // 2))\n",
        "\n",
        "        return new_img\n",
        "\n",
        "\n",
        "for basename in basename_list:\n",
        "    img_path = os.path.join(parent_path, basename)\n",
        "    letterbox_img = convert_to_letterbox(img_path)\n",
        "    letterbox_img.save(os.path.join(\"/content\", basename))"
      ],
      "metadata": {
        "id": "DYuBdiIBzmRk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcddETYo3W0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}