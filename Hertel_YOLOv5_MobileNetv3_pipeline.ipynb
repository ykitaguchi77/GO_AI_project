{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Hertel_YOLOv5_MobileNetv3_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation YOLOv5-MobileNetv3 pipeline**\n",
        "\n",
        "Train YOLOv5 using iFish augmentation\n",
        "\n",
        "```\n",
        "iFish: https://github.com/Gil-Mor/iFish.git\n",
        "\n",
        "yolov5_iFish: https://github.com/ykitaguchi77/yolov5-iFish.git\n",
        "‚Äª FishAugment(distortion_range=(0, 0.3), p=0.5)\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlib„ÅßÁõÆ„Åå2„Å§Ê§úÂá∫„Åï„Çå„Çã„ÇÇ„ÅÆ„ÇíÊäú„ÅçÂá∫„Åô\n",
        "YOLOv5„ÇíÁî®„ÅÑ„Å¶Â∑¶Âè≥„Å®„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíË™çË≠ò„Åï„Åõ„Çã\n",
        "Êäú„ÅçÂá∫„Åó„ÅüÁîªÂÉè„Å´„Å§„ÅÑ„Å¶MobileNetV3„ÅßÂõûÂ∏∞Ôºà5-fold ensembleÔºâ„ÇíË°å„ÅÜ\n",
        "„Çπ„Éû„Éõ„Å´ÂÆüË£Ö\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fac747-2c7d-4d43-9320-b227670f009b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  7 13:15:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea869e7c-ac0f-419e-c458-0f95ce02eba1"
      },
      "source": [
        "#ÊÆã„ÇäÊôÇÈñìÁ¢∫Ë™ç\n",
        "!cat /proc/uptime | awk '{printf(\"ÊÆã„ÇäÊôÇÈñì : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÊÆã„ÇäÊôÇÈñì : 11.89"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colab„Çí„Éû„Ç¶„É≥„Éà"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f563175a-4fab-4a8e-a168-b106b3f5fb9a"
      },
      "source": [
        "'''\n",
        "„Éªdlib„ÇíÁî®„ÅÑ„Å¶ÁõÆ„ÇíÂàá„ÇäÊäú„Åè\n",
        "„ÉªÊ®™ÂπÖ„Çí2ÂÄç„ÄÅÁ∏¶ÂπÖ„Çí‰∏ä„Å´1ÂÄçËøΩÂä†/‰∏ã„Å´0.5ÂÄçËøΩÂä†„Åó„Åü‰∏°Áúº„ÅÆÁîªÂÉè„ÅåÂê´„Åæ„Çå„Çã„Çà„ÅÜ„Å´Âàá„ÇäÂèñ„ÇãÔºàÁõÆ„ÅÆÂÖ®ÂπÖ„ÄÅÁúâÊØõ„ÅåÂê´„Åæ„Çå„Çã„Çà„ÅÜ„Å´Ôºâ\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ë¶™„Éï„Ç©„É´„ÉÄ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#ÂÖÉÁîªÂÉè„Éï„Ç©„É´„ÉÄ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#ÂÖÉÁîªÂÉè„Çí„Ç≥„Éî„Éº\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#Âàá„Çä„Å¨„ÅÑ„ÅüÁîªÂÉè„Çí‰øùÂ≠ò„Åô„Çã„Éï„Ç©„É´„ÉÄ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüYOLOv5„ÅßÂàá„ÇäÊäú„ÅÑ„ÅüÁîªÂÉè„Çí‰øùÂ≠ò„Åô„Çã„Éï„Ç©„É´„ÉÄ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSV„Éï„Ç°„Ç§„É´„ÅÆ„Éï„Ç©„É´„ÉÄ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Reflesh folder (ÂÜÖÂÆπ„ÅåÂâäÈô§„Åï„Çå„Çã„ÅÆ„ÅßÊ≥®ÊÑèÔºÅÔºÅ) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\n",
        "# parent_dir„Åå„ÅÇ„Çå„Å∞ÂâäÈô§„Åô„Çã\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# Êñ∞„Åó„Åèparent_dir„Çí‰ΩúÊàê„Åô„Çã\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dir„ÇíÊñ∞Ë¶è„Å´‰ΩúÊàê„Åô„Çã\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dir„Å´dataset_dirÁõ¥‰∏ã„ÅÆ„Éï„Ç°„Ç§„É´„Çí„Åô„Åπ„Å¶„Ç≥„Éî„Éº„Åô„Çã\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"Âá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnV1hEgIu87W",
        "outputId": "abe9b82a-2d69-424b-d59a-a7ce8d5e3526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1016/1016 [00:12<00:00, 83.69file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Âá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascade„ÇíÁî®„ÅÑ„Å¶ÁõÆ„ÇíÊ§úÂá∫**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# „Ç´„Çπ„Ç±„Éº„Éâ„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# „Ç´„Çπ„Ç±„Éº„ÉâÂàÜÈ°ûÂô®„ÅÆÁâπÂæ¥ÈáèÂèñÂæó\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ÁõÆ„Åå2„Å§‰ª•‰∏äÊ§úÂá∫„Åï„Çå„Åü„ÇÇ„ÅÆ„ÇíÊäú„ÅçÂá∫„Åô**\n",
        "\n",
        "dlib„ÅßÊ§úÂá∫„Åï„Çå„Åü„ÇÇ„ÅÆ„Åã„Çâ„ÄÅ‰∏ä‰∏ãÂ∑¶Âè≥„Å´0.1ÂÄç„Åö„Å§Êã°Â§ß„Åó„ÅüÁØÑÂõ≤„ÇíÊäú„ÅçÂá∫„Åó„Å¶„ÅÑ„Çã"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #„Éï„Ç©„É´„ÉÄÊï∞„ÅÆÂàÜ„Å†„Åë\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # ÁîªÂÉè„Ç∞„É¨„Éº„Çπ„Ç±„Éº„É´Âåñ\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix‰ª•‰∏ä„ÅÆ„ÇÇ„ÅÆ„ÅßÁõÆ„Å´Ë¶ã„Åà„Çã„ÇÇ„ÅÆ„ÇíÊäΩÂá∫\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # ÁúºÊ§úÂá∫Âà§ÂÆö\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('ÁõÆ„Åå' + str(len(eye_list)) +'ÂÄãÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #ÁîªÂÉè„ÅÆÂàá„ÇäÊäú„Åç„Å®‰øùÂ≠òÔºà2ÂÄã‰ª•‰∏äÊ§úÂá∫„ÅÆÊôÇ„Å´Èôê„ÇãÔºâ\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #Êú¨Êù•„ÅÆÂàá„ÇäÊäú„Åç„Çà„ÇäÂπÖ„ÅÆ0.1ÂÄç„Åö„Å§Ê∞¥Â¢ó„Åó„Åô„Çã\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #Ê®™ÂπÖ„ÅÆÂçäÂàÜ„Çà„ÇäÂ∑¶„Å´„ÅÇ„Çã„ÅÆ„ÅØÂè≥Áúº\n",
        "                      else:\n",
        "                          side = \"L\" #Ê®™ÂπÖ„ÅÆÂçäÂàÜ„Çà„Çä„Çà„ÇäÂè≥„Å´„ÅÇ„Çã„ÅÆ„ÅØÂ∑¶Áúº\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #ÂØæÂøúË°®„ÅÆ‰ΩúÊàê\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**„Åì„Åì„Åß„ÄÅÁõÆ‰ª•Â§ñ„ÅåË™§Ê§úÂá∫„Åï„Çå„Å¶„ÅÑ„Çã„ÇÇ„ÅÆ„ÇíÊâãÂãï„ÅßÊäú„ÅçÂá∫„Åó„Å¶ÂâäÈô§„Åô„Çã**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csv„Åã„Çâ„ÄÅÂâäÈô§„Åó„Å¶ÁîªÂÉè„ÅÆ„Éë„Çπ„ÅåÂ≠òÂú®„Åó„Å™„Åè„Å™„Å£„Å¶„ÅÑ„ÇãË°å„ÇíÂâäÈô§„Åô„Çã\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrame„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# Â≠òÂú®„Åó„Å™„ÅÑÁîªÂÉè„Éë„Çπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„ÄÅ„Åù„ÅÆ„É™„Çπ„Éà„Çí‰øùÊåÅ„Åô„Çã\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# Â≠òÂú®„Åó„Å™„ÅÑÁîªÂÉè„Éë„Çπ„ÇíË°®Á§∫\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # Â≠òÂú®„Åó„Å™„ÅÑÁîªÂÉè„Éë„Çπ„ÅÆË°å„ÇíÂâäÈô§\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # Êõ¥Êñ∞„Åï„Çå„ÅüDataFrame„Çí‰øùÂ≠ò„Åô„Çã\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "0dd85e94-04a3-49bb-f29a-597ea3ce525b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframe„ÅÆÊï¥ÁêÜ**\n",
        "\n",
        "„Éª hertel_df„ÇíÂèÇÁÖß„Åó„Å¶„ÄÅcoordinates_df„Å´„Éò„É´„ÉÜ„É´ÂÄ§„ÇíË®òÂÖ•„Åô„Çã\n",
        "\n",
        "„Éªid„Åå\"16_R, 16_L\"„Å®„ÅÑ„ÅÜÂΩ¢Âºè„Å´„Å™„Çã„Çà„ÅÜ„Å´„Éá„Éº„Çø„Éï„É¨„Éº„É†„ÇíÊï¥ÁêÜ„Åô„Çã"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "af59f4c1-19cc-45be-8706-73637c192c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a03c0c0-42f0-43aa-84b8-94338e24855d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a03c0c0-42f0-43aa-84b8-94338e24855d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a03c0c0-42f0-43aa-84b8-94338e24855d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a03c0c0-42f0-43aa-84b8-94338e24855d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ÁîªÂÉè„Éë„Çπ„ÅÆÊäΩÂá∫ÔºàRL„Å®„ÇÇ„Å´ÊèÉ„Å£„Å¶„ÅÑ„Çã„ÇÇ„ÅÆÔºâ\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "ÁîªÂÉè„ÅÆÂàÜÂâ≤ train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "„Éï„Ç©„É´„ÉÄ„ÅÆ‰ΩúÊàê\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "ÁîªÂÉè„ÅÆ„Ç≥„Éî„Éº\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7d2f38-40d0-4dcd-db66-bbb1b6525673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760/760 [00:07<00:00, 95.80it/s]\n",
            "Copying valid images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [00:02<00:00, 72.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\" {side} {cx} {cy} {w} {h}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSV„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# train„Å®valid„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„Éë„Çπ\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# train„Éá„Ç£„É¨„ÇØ„Éà„É™„Åß„É©„Éô„É´„Éï„Ç°„Ç§„É´„ÇíÁîüÊàê\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# valid„Éá„Ç£„É¨„ÇØ„Éà„É™„Åß„É©„Éô„É´„Éï„Ç°„Ç§„É´„ÇíÁîüÊàê\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc520358-e145-474f-d72d-6cc75c0c8324"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760/760 [00:05<00:00, 145.37it/s]\n",
            "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [01:49<00:00,  1.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆ„Çµ„É≥„Éó„É´ÊèèÁîª ##\n",
        "## („Åì„Çå„ÅØÂÆüË°å„Åó„Å™„Åè„Å¶ËâØ„ÅÑ)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíÊèèÁîª„Åô„ÇãÈñ¢Êï∞\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            cx, cy, bw, bh, class_id = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# „É©„Éô„É´„Éï„Ç°„Ç§„É´„Åã„Çâ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆ„É™„Çπ„Éà„ÇíÂèñÂæó„Åô„ÇãÈñ¢Êï∞\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# ÁîªÂÉè„Éë„Çπ„Å®„É©„Éô„É´„Éï„Ç°„Ç§„É´„Éë„Çπ\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# ÁîªÂÉè„ÅÆ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# „É©„Éô„É´„Éï„Ç°„Ç§„É´„Åã„Çâ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆ„É™„Çπ„Éà„ÇíÂèñÂæó\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíÊèèÁîª\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhSI3Wc_-ylJ",
        "outputId": "3b95a0f1-0fca-4caf-eb69-76d351c1f064"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "c4a19936-58b9-4350-869c-dc3620a76d7c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5_iFish --> train_batch„ÅÆÁ≤æÂ∫¶„Åå‰Ωé„ÅÑ„ÅÆ„Åß‰∏ÄÊó¶Âç¥‰∏ã„Å®„Åó„Åü\n",
        "# %cd $out_dir\n",
        "# !git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentation„ÇíÂÆüË£Ö„Åó„Åü„Éê„Éº„Ç∏„Éß„É≥\n",
        "# %cd yolov5-iFish\n",
        "# %pip install -qr requirements.txt\n",
        "\n",
        "# import torch\n",
        "# import utils\n",
        "# display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-33rbP1-iQ4",
        "outputId": "2599ce37-597f-4dbd-89b2-40ef60fb37d0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ 4d3e758 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 28.2/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "%cd $out_dir\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdYFiF39egKw",
        "outputId": "9f89e245-260b-45e8-8845-df1ba120885d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 28.2/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "id": "7iRcXa8I-iSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÈÄî‰∏≠„Åã„Çâ\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/data.yaml --resume $out_dir/yolov5/runs/train/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a91dd4-1a97-4e92-e524-1b1859a495e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-07 13:15:58.172429: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-07 13:15:58.172484: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-07 13:15:58.172523: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=16, imgsz=640, rect=False, resume=/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/runs/train/exp/weights/last.pt, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1      9471  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "Model summary: 214 layers, 1766623 parameters, 1766623 gradients, 4.2 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/runs/train/exp/weights/last.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "Resuming training from /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/runs/train/exp/weights/last.pt from epoch 47 to 300 total epochs\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/labels.cache... 760 images, 0 backgrounds, 0 corrupt: 100% 760/760 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels.cache... 190 images, 0 backgrounds, 0 corrupt: 100% 190/190 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     47/299      1.93G    0.02527    0.01648    0.01032         29        640: 100% 48/48 [00:46<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:05<00:00,  1.02it/s]\n",
            "                   all        190        380      0.914      0.983       0.99      0.761\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     48/299      1.93G    0.02183    0.01631   0.008702         38        640: 100% 48/48 [00:43<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.21it/s]\n",
            "                   all        190        380      0.971      0.952       0.99      0.781\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     49/299      1.93G    0.02615    0.01727    0.01425         35        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.83it/s]\n",
            "                   all        190        380      0.979      0.981      0.993      0.766\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     50/299      1.93G    0.02218    0.01601    0.01184         30        640: 100% 48/48 [00:46<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.83it/s]\n",
            "                   all        190        380      0.936      0.951       0.98      0.756\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     51/299      1.93G    0.02458    0.01683    0.01166         30        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.03it/s]\n",
            "                   all        190        380      0.974      0.962      0.986       0.66\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     52/299      1.93G    0.02395    0.01664    0.01036         40        640: 100% 48/48 [00:45<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.96it/s]\n",
            "                   all        190        380      0.959      0.934      0.989      0.788\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     53/299      1.93G     0.0227    0.01629   0.009536         43        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.80it/s]\n",
            "                   all        190        380      0.929      0.933      0.977       0.78\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     54/299      1.93G    0.02293    0.01647    0.01062         40        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.19it/s]\n",
            "                   all        190        380      0.982      0.966      0.993      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     55/299      1.93G     0.0254    0.01628    0.01198         36        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.16it/s]\n",
            "                   all        190        380      0.982      0.961      0.992      0.779\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     56/299      1.93G    0.02191    0.01615    0.01006         29        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.40it/s]\n",
            "                   all        190        380      0.995      0.996      0.994      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     57/299      1.93G    0.02409    0.01574   0.009662         31        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.85it/s]\n",
            "                   all        190        380      0.921      0.976      0.989      0.769\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     58/299      1.93G    0.02199    0.01609   0.008489         30        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.89it/s]\n",
            "                   all        190        380      0.969      0.981      0.992      0.792\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     59/299      1.93G    0.02265    0.01589   0.009118         36        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.64it/s]\n",
            "                   all        190        380      0.974      0.953      0.991      0.784\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     60/299      1.93G    0.02389     0.0159   0.009871         32        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.73it/s]\n",
            "                   all        190        380      0.932      0.911      0.981      0.767\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     61/299      1.93G    0.02314    0.01542    0.00953         32        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.81it/s]\n",
            "                   all        190        380      0.968      0.947      0.991      0.741\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     62/299      1.93G    0.02492    0.01557   0.007902         29        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "                   all        190        380      0.942      0.913      0.981      0.739\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     63/299      1.93G    0.02028    0.01549   0.008606         30        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.83it/s]\n",
            "                   all        190        380      0.931      0.981      0.992      0.802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     64/299      1.93G    0.02154    0.01554   0.008564         35        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.45it/s]\n",
            "                   all        190        380      0.985       0.98      0.994      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     65/299      1.93G    0.02457     0.0159   0.009122         36        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.93it/s]\n",
            "                   all        190        380      0.941      0.942       0.99      0.795\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     66/299      1.93G    0.02264    0.01558   0.009561         31        640: 100% 48/48 [00:46<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.01it/s]\n",
            "                   all        190        380      0.966      0.951      0.993      0.809\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     67/299      1.93G     0.0226    0.01609   0.008926         27        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.85it/s]\n",
            "                   all        190        380      0.991      0.989      0.994      0.806\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     68/299      1.93G    0.02201    0.01561   0.007242         39        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.15it/s]\n",
            "                   all        190        380      0.946      0.966       0.99       0.77\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     69/299      1.93G    0.02244     0.0157   0.008773         34        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.881       0.94      0.976      0.776\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     70/299      1.93G    0.02336    0.01569   0.008963         31        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.45it/s]\n",
            "                   all        190        380      0.916      0.966      0.988      0.771\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     71/299      1.93G    0.02223    0.01568   0.008318         43        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.97it/s]\n",
            "                   all        190        380      0.956      0.965      0.992      0.793\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     72/299      1.93G    0.02151    0.01544   0.008401         27        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.957      0.963      0.991      0.803\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     73/299      1.93G    0.02351     0.0157   0.007656         40        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.91it/s]\n",
            "                   all        190        380      0.976      0.969      0.992      0.812\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     74/299      1.93G    0.02228    0.01546   0.008085         28        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.72it/s]\n",
            "                   all        190        380      0.957      0.968      0.991      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     75/299      1.93G    0.02144    0.01526   0.007744         34        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.983      0.974      0.993      0.802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     76/299      1.93G    0.02028     0.0151   0.007957         42        640: 100% 48/48 [00:44<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.70it/s]\n",
            "                   all        190        380      0.937      0.976      0.991      0.808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     77/299      1.93G    0.02268    0.01541   0.009782         31        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.97it/s]\n",
            "                   all        190        380       0.98      0.983      0.994      0.812\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     78/299      1.93G    0.01982     0.0146   0.007425         24        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.68it/s]\n",
            "                   all        190        380       0.96      0.968      0.991      0.822\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     79/299      1.93G    0.02088    0.01498   0.008489         39        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.53it/s]\n",
            "                   all        190        380      0.995       0.99      0.994      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     80/299      1.93G    0.02227    0.01532   0.007661         30        640: 100% 48/48 [00:44<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.77it/s]\n",
            "                   all        190        380      0.982      0.971      0.993      0.806\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     81/299      1.93G    0.02242    0.01543   0.008204         31        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.979      0.974      0.992      0.786\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     82/299      1.93G    0.02064    0.01559   0.006528         39        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.40it/s]\n",
            "                   all        190        380      0.943      0.938      0.986      0.787\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     83/299      1.93G    0.02409    0.01514   0.009535         42        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.01it/s]\n",
            "                   all        190        380      0.978      0.991      0.993      0.795\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     84/299      1.93G    0.01999     0.0152   0.007856         42        640: 100% 48/48 [00:47<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.98it/s]\n",
            "                   all        190        380      0.973      0.976      0.992      0.773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     85/299      1.93G    0.02113    0.01467   0.007801         42        640: 100% 48/48 [00:47<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.16it/s]\n",
            "                   all        190        380      0.971       0.98      0.994       0.81\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     86/299      1.93G    0.02133    0.01539   0.008226         43        640: 100% 48/48 [00:46<00:00,  1.03it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.19it/s]\n",
            "                   all        190        380      0.979      0.979      0.992      0.821\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     87/299      1.93G    0.01984    0.01468   0.006024         30        640: 100% 48/48 [00:47<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.96it/s]\n",
            "                   all        190        380      0.979      0.983      0.994      0.817\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     88/299      1.93G    0.02109     0.0149   0.007475         28        640: 100% 48/48 [00:46<00:00,  1.03it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.74it/s]\n",
            "                   all        190        380      0.945      0.953      0.988      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     89/299      1.93G    0.02163    0.01561   0.007378         38        640: 100% 48/48 [00:45<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.77it/s]\n",
            "                   all        190        380      0.977      0.984      0.993      0.817\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     90/299      1.93G    0.02315    0.01536   0.007561         31        640: 100% 48/48 [00:43<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.79it/s]\n",
            "                   all        190        380       0.99      0.987      0.994      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     91/299      1.93G     0.0211     0.0148   0.006731         36        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.58it/s]\n",
            "                   all        190        380      0.981      0.976      0.994       0.82\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     92/299      1.93G    0.02249    0.01515   0.007501         35        640: 100% 48/48 [00:44<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.75it/s]\n",
            "                   all        190        380      0.992      0.991      0.994      0.799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     93/299      1.93G    0.02258    0.01506   0.007408         30        640: 100% 48/48 [00:46<00:00,  1.03it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.63it/s]\n",
            "                   all        190        380      0.981      0.989      0.993      0.771\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     94/299      1.93G     0.0215    0.01504   0.006542         28        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.98it/s]\n",
            "                   all        190        380      0.974      0.992      0.994       0.81\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     95/299      1.93G    0.02052    0.01469   0.007943         33        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.72it/s]\n",
            "                   all        190        380      0.979      0.982      0.994      0.817\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     96/299      1.93G     0.0206    0.01517   0.007398         38        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.40it/s]\n",
            "                   all        190        380          1       0.98      0.995      0.823\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     97/299      1.93G    0.02172    0.01501   0.007688         36        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.12it/s]\n",
            "                   all        190        380      0.982      0.987      0.994      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     98/299      1.93G    0.02174    0.01454   0.007229         42        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.83it/s]\n",
            "                   all        190        380      0.996      0.996      0.994      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     99/299      1.93G    0.01972    0.01466    0.00561         36        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.93it/s]\n",
            "                   all        190        380      0.991      0.986      0.994      0.802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    100/299      1.93G    0.02093    0.01466   0.005999         37        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.67it/s]\n",
            "                   all        190        380      0.995      0.992      0.995      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    101/299      1.93G    0.02092    0.01507   0.008641         34        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.987      0.976      0.993      0.744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    102/299      1.93G    0.02106    0.01485   0.008561         33        640: 100% 48/48 [00:46<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.67it/s]\n",
            "                   all        190        380      0.945      0.967      0.989      0.799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    103/299      1.93G    0.02281    0.01526   0.008017         36        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.94it/s]\n",
            "                   all        190        380      0.976      0.964      0.993      0.828\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    104/299      1.93G    0.01972    0.01481   0.008114         29        640: 100% 48/48 [00:44<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.962      0.976      0.992      0.824\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    105/299      1.93G    0.02003    0.01467   0.006522         32        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.77it/s]\n",
            "                   all        190        380      0.978      0.979      0.994      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    106/299      1.93G    0.02208    0.01542   0.007897         42        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.81it/s]\n",
            "                   all        190        380      0.984      0.992      0.995      0.815\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    107/299      1.93G    0.01891    0.01479   0.006113         21        640: 100% 48/48 [00:45<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.07it/s]\n",
            "                   all        190        380      0.982      0.992      0.995       0.82\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    108/299      1.93G    0.02086    0.01512   0.006819         34        640: 100% 48/48 [00:46<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.91it/s]\n",
            "                   all        190        380      0.988      0.991      0.995      0.795\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    109/299      1.93G     0.0199    0.01494    0.00697         39        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.22it/s]\n",
            "                   all        190        380      0.991      0.992      0.994      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    110/299      1.93G    0.01869    0.01428   0.007009         41        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.75it/s]\n",
            "                   all        190        380       0.99      0.986      0.994      0.827\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    111/299      1.93G    0.02167    0.01526   0.007198         39        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.73it/s]\n",
            "                   all        190        380      0.974      0.988      0.994      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    112/299      1.93G    0.01964    0.01458   0.006507         37        640: 100% 48/48 [00:43<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.78it/s]\n",
            "                   all        190        380      0.995      0.976      0.995      0.831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    113/299      1.93G    0.01952    0.01429   0.005562         35        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.98it/s]\n",
            "                   all        190        380      0.978      0.981      0.992      0.808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    114/299      1.93G    0.01867    0.01444   0.006495         32        640: 100% 48/48 [00:43<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.05it/s]\n",
            "                   all        190        380      0.994      0.989      0.995      0.823\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    115/299      1.93G    0.01928    0.01378   0.005808         36        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.81it/s]\n",
            "                   all        190        380      0.993      0.984      0.994      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    116/299      1.93G    0.01964    0.01488   0.006881         42        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.04it/s]\n",
            "                   all        190        380      0.994      0.989      0.994      0.813\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    117/299      1.93G    0.01954    0.01449   0.007124         27        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "                   all        190        380      0.994      0.994      0.995      0.824\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    118/299      1.93G     0.0205     0.0148   0.007135         41        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.87it/s]\n",
            "                   all        190        380      0.968       0.97      0.992      0.819\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    119/299      1.93G     0.0202    0.01438   0.007574         36        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.78it/s]\n",
            "                   all        190        380      0.982      0.986      0.994      0.814\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    120/299      1.93G    0.02033    0.01453   0.006235         34        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.62it/s]\n",
            "                   all        190        380      0.986      0.981      0.994      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    121/299      1.93G    0.01839    0.01406   0.006525         35        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.05it/s]\n",
            "                   all        190        380      0.989      0.984      0.994      0.831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    122/299      1.93G    0.02088    0.01446   0.006123         33        640: 100% 48/48 [00:45<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.80it/s]\n",
            "                   all        190        380      0.996      0.992      0.995      0.836\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    123/299      1.93G    0.01965    0.01483   0.005528         46        640: 100% 48/48 [00:43<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.97it/s]\n",
            "                   all        190        380      0.992      0.984      0.994      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    124/299      1.93G    0.02059    0.01412   0.007053         34        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.01it/s]\n",
            "                   all        190        380      0.995      0.989      0.995       0.84\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    125/299      1.93G       0.02    0.01449   0.007389         31        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.99it/s]\n",
            "                   all        190        380      0.987       0.99      0.995      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    126/299      1.93G    0.02097    0.01477   0.007278         35        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.74it/s]\n",
            "                   all        190        380      0.918      0.929      0.991       0.77\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    127/299      1.93G    0.01912    0.01511   0.008905         38        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.94it/s]\n",
            "                   all        190        380      0.967      0.969      0.991      0.823\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    128/299      1.93G    0.02152    0.01497   0.007615         44        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.94it/s]\n",
            "                   all        190        380      0.995      0.989      0.994      0.815\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    129/299      1.93G    0.02019    0.01397   0.007113         33        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.09it/s]\n",
            "                   all        190        380      0.986      0.985      0.993       0.82\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    130/299      1.93G    0.01916    0.01399   0.005357         35        640: 100% 48/48 [00:46<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "                   all        190        380      0.997      0.989      0.995      0.827\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    131/299      1.93G    0.01979    0.01452   0.006194         32        640: 100% 48/48 [00:45<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.11it/s]\n",
            "                   all        190        380      0.987      0.996      0.995      0.799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    132/299      1.93G     0.0184    0.01443   0.005814         36        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.80it/s]\n",
            "                   all        190        380      0.971      0.982      0.994      0.791\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    133/299      1.93G    0.02085    0.01447   0.008048         34        640: 100% 48/48 [00:44<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.98it/s]\n",
            "                   all        190        380       0.99      0.979      0.995      0.804\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    134/299      1.93G    0.01856    0.01417   0.006227         30        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.27it/s]\n",
            "                   all        190        380      0.981      0.995      0.995      0.822\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    135/299      1.93G    0.01994    0.01467   0.006688         30        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.04it/s]\n",
            "                   all        190        380      0.997      0.994      0.995      0.837\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    136/299      1.93G    0.02044    0.01433   0.005802         27        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.72it/s]\n",
            "                   all        190        380      0.974      0.978      0.994      0.835\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    137/299      1.93G    0.01983      0.014   0.006173         42        640: 100% 48/48 [00:43<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.68it/s]\n",
            "                   all        190        380      0.994      0.991      0.995      0.829\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    138/299      1.93G     0.0193    0.01423   0.005492         39        640: 100% 48/48 [00:44<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "                   all        190        380      0.989       0.99      0.995      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    139/299      1.93G    0.02029     0.0142   0.007364         32        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.24it/s]\n",
            "                   all        190        380      0.985      0.999      0.995      0.836\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    140/299      1.93G    0.01906    0.01381   0.006158         28        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.78it/s]\n",
            "                   all        190        380      0.961      0.987      0.994      0.836\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    141/299      1.93G    0.02044    0.01404   0.007789         39        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.92it/s]\n",
            "                   all        190        380       0.97      0.991      0.994      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    142/299      1.93G    0.02045    0.01473   0.006774         39        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.02it/s]\n",
            "                   all        190        380      0.988      0.966      0.994      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    143/299      1.93G    0.02037    0.01434   0.008135         34        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.91it/s]\n",
            "                   all        190        380      0.995      0.994      0.995       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    144/299      1.93G    0.01866    0.01453   0.006185         39        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.64it/s]\n",
            "                   all        190        380      0.984      0.983      0.994      0.817\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    145/299      1.93G    0.02092    0.01428   0.006751         34        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.69it/s]\n",
            "                   all        190        380      0.988      0.983      0.993      0.807\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    146/299      1.93G    0.02009    0.01392   0.006966         31        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.93it/s]\n",
            "                   all        190        380      0.988      0.988      0.993      0.841\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    147/299      1.93G    0.01966    0.01465   0.006534         34        640: 100% 48/48 [00:43<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.19it/s]\n",
            "                   all        190        380       0.99      0.982      0.993      0.803\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    148/299      1.93G    0.01772    0.01377   0.005132         40        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.79it/s]\n",
            "                   all        190        380      0.974       0.98      0.994      0.828\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    149/299      1.93G    0.01787    0.01446     0.0054         42        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.12it/s]\n",
            "                   all        190        380      0.996       0.99      0.995      0.833\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    150/299      1.93G     0.0175    0.01364   0.005871         36        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.27it/s]\n",
            "                   all        190        380      0.992      0.994      0.995      0.823\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    151/299      1.93G    0.01864    0.01411   0.006956         37        640: 100% 48/48 [00:44<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.94it/s]\n",
            "                   all        190        380      0.991      0.986      0.995      0.836\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    152/299      1.93G    0.02041    0.01407   0.005958         37        640: 100% 48/48 [00:46<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.12it/s]\n",
            "                   all        190        380      0.994      0.987      0.995      0.831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    153/299      1.93G    0.01794    0.01383   0.005284         32        640: 100% 48/48 [00:43<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.75it/s]\n",
            "                   all        190        380      0.993      0.995      0.995      0.835\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    154/299      1.93G    0.01973    0.01403   0.006438         41        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.72it/s]\n",
            "                   all        190        380      0.995      0.987      0.995        0.8\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    155/299      1.93G    0.01942    0.01412    0.00625         37        640: 100% 48/48 [00:43<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.98it/s]\n",
            "                   all        190        380      0.997      0.993      0.995       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    156/299      1.93G    0.01963     0.0145   0.007316         40        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.29it/s]\n",
            "                   all        190        380      0.988      0.993      0.994      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    157/299      1.93G    0.01936     0.0143   0.007249         30        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.94it/s]\n",
            "                   all        190        380      0.987      0.971      0.992      0.833\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    158/299      1.93G    0.01808    0.01418   0.005671         36        640: 100% 48/48 [00:46<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.09it/s]\n",
            "                   all        190        380      0.997      0.992      0.994      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    159/299      1.93G    0.01915    0.01395   0.004793         27        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.06it/s]\n",
            "                   all        190        380      0.953      0.992      0.992      0.837\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    160/299      1.93G    0.01859    0.01436   0.006976         27        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.01it/s]\n",
            "                   all        190        380      0.987      0.996      0.994      0.838\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    161/299      1.93G    0.01823    0.01349   0.006806         30        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:03<00:00,  1.71it/s]\n",
            "                   all        190        380      0.991      0.996      0.994      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    162/299      1.93G    0.01768    0.01338   0.007023         25        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.10it/s]\n",
            "                   all        190        380       0.98      0.975      0.994      0.843\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    163/299      1.93G    0.01827    0.01376   0.004932         36        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.21it/s]\n",
            "                   all        190        380       0.99      0.986      0.995      0.783\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    164/299      1.93G    0.01875    0.01452   0.007798         36        640: 100% 48/48 [00:44<00:00,  1.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.47it/s]\n",
            "                   all        190        380      0.989      0.985      0.994      0.834\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    165/299      1.93G    0.01789    0.01416   0.005401         38        640: 100% 48/48 [00:45<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.56it/s]\n",
            "                   all        190        380      0.994      0.989      0.995      0.824\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    166/299      1.93G     0.0194    0.01425   0.006836         42        640: 100% 48/48 [00:45<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.01it/s]\n",
            "                   all        190        380       0.99      0.995      0.995      0.835\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    167/299      1.93G    0.01928    0.01352   0.004666         71        640:  73% 35/48 [00:31<00:11,  1.10it/s]\n",
            "Error in sys.excepthook:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dask/base.py\", line 70, in wrapper\n",
            "    tb = shorten_traceback(tb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dask/utils.py\", line 2158, in shorten_traceback\n",
            "    if when_exp.match(f.f_code.co_filename):\n",
            "KeyboardInterrupt\n",
            "\n",
            "Original exception was:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/train.py\", line 647, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/train.py\", line 536, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/train.py\", line 291, in train\n",
            "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1182, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/yolov5/utils/dataloaders.py\", line 172, in __iter__\n",
            "    yield next(self.iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1284, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/queue.py\", line 180, in get\n",
            "    self.not_empty.wait(remaining)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 324, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best.py„Çírename„Åó„Å¶gdrive„Å´ÁßªÂãï„Åó„Å¶„Åä„Åè\n",
        "orig_pt = f\"{out_dir}/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/eyecrop-yolov5n-167epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab71da51-b9a1-4dcf-a3a3-79b071427729"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-167epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference original dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9459c32c-ba87-4ed3-f570-d1ebe0468388"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ 4d3e758 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 27.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = f\"{out_dir}/dataset_uni/eyecrop-yolov5n-iFish_169epoch.pt\"\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-167epoch.pt\"\n",
        "\n",
        "# „ÇÇ„Å®„ÇÇ„Å®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà\n",
        "orig_dir = orig_dir #ÂÖÉÁîªÂÉè\n",
        "cropped_dir = cropped_dir #YOLOv5„ÅßÂàá„ÇäÊäú„ÅÑ„ÅüÁîªÂÉèÁî®\n",
        "\n",
        "if os.path.exists(cropped_dir):\n",
        "    shutil.rmtree(cropped_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "os.makedirs(f\"{cropped_dir}/cropped_images\")\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_orig/1.JPG\""
      ],
      "metadata": {
        "id": "PVybAqvGFvPs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    cv2_imshow(img_cv2)\n",
        "\n",
        "    # Crop and save the image\n",
        "    cropped_img = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    base_name = os.path.basename(img)\n",
        "    cropped_img_path = os.path.join(cropped_dir, f\"cropped_{base_name}\")\n",
        "    cv2.imwrite(cropped_img_path, cropped_img)\n"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "„ÅØ„ÅøÂá∫„ÅóÈò≤Ê≠¢„ÄÅÈï∑Ëæ∫„Çí1Ëæ∫„Å®„Åó„ÅületterboxÂåñ\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# „ÇÇ„Åócropped_dir„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ‰ΩúÊàê„Åô„Çã\n",
        "if not os.path.exists(cropped_dir):\n",
        "    os.makedirs(cropped_dir)\n",
        "\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "    prob = pred[0][0][4].item()\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))\n",
        "\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    padding_x = (img_width - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_height - min(img_width, img_height)) / 2\n",
        "\n",
        "    x1 = max(x1 - padding_x, 0)\n",
        "    y1 = max(y1 - padding_y, 0)\n",
        "    x2 = min(x2 - padding_x, img_width)\n",
        "    y2 = min(y2 - padding_y, img_height)\n",
        "\n",
        "    # „ÇØ„É≠„ÉÉ„Éó„Åï„Çå„ÅüÁîªÂÉè„ÅÆÂÆüÈöõ„ÅÆ„Çµ„Ç§„Ç∫\n",
        "    cropped_height = int(y2 - y1)\n",
        "    cropped_width = int(x2 - x1)\n",
        "\n",
        "    # Èªí„ÅÑËÉåÊôØÁîªÂÉè„Çí‰ΩúÊàê\n",
        "    if cropped_width > cropped_height:\n",
        "        size = cropped_width\n",
        "    else:\n",
        "        size = cropped_height\n",
        "    padded_img = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "\n",
        "    # „ÇØ„É≠„ÉÉ„Éó„Åó„ÅüÁîªÂÉè„Çí‰∏≠Â§Æ„Å´ÈÖçÁΩÆ\n",
        "    top = (size - cropped_height) // 2\n",
        "    bottom = size - cropped_height - top\n",
        "    left = (size - cropped_width) // 2\n",
        "    right = size - cropped_width - left\n",
        "\n",
        "    cropped_img = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "    padded_img[top:top + cropped_height, left:left + cropped_width] = cropped_img\n",
        "\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "    cv2_imshow(img_cv2)\n",
        "\n",
        "    base_name = os.path.basename(img_path)\n",
        "    cropped_img_path = os.path.join(cropped_dir, f\"cropped_{base_name}\")\n",
        "    cv2.imwrite(cropped_img_path, padded_img)\n"
      ],
      "metadata": {
        "id": "8Bhi-plnJQUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "640px„Å´„É™„Çµ„Ç§„Ç∫„Åó„Å™„ÅÑ„Éê„Éº„Ç∏„Éß„É≥\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# „ÇÇ„Åócropped_dir„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ‰ΩúÊàê„Åô„Çã\n",
        "if not os.path.exists(cropped_dir):\n",
        "    os.makedirs(cropped_dir)\n",
        "\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)\n",
        "    x1, y1, x2, y2, prob, class_num = pred[0][0]\n",
        "    class_name = class_names[class_num]\n",
        "\n",
        "    print(f\"Ë®∫Êñ≠„ÅØ {class_name}„ÄÅÁ¢∫Áéá„ÅØ{prob * 100:.1f}%„Åß„Åô„ÄÇ\")\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # ÁîªÂÉè„Åã„ÇâË©≤ÂΩì„Åô„ÇãÈ†òÂüü„ÇíÂàá„ÇäÊäú„Åè\n",
        "    cropped_img = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # Á¢∫Ë™ç„ÅÆ„Åü„ÇÅ„Å´ÂÖÉÁîªÂÉè„Å´Áü©ÂΩ¢„ÇíÊèèÁîª„Åó„ÄÅË°®Á§∫„Åô„Çã\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "    cv2_imshow(img_cv2)\n",
        "\n",
        "    base_name = os.path.basename(img_path)\n",
        "    cropped_img_path = os.path.join(cropped_dir, f\"cropped_{base_name}\")\n",
        "    cv2.imwrite(cropped_img_path, cropped_img)\n"
      ],
      "metadata": {
        "id": "s6vaadJaJQbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "b909676e-8c40-4b1a-8a5d-f2ff63e06a19"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ 4d3e758 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: [tensor([[111.61232,  56.96371, 485.60168, 487.11520,   0.87449,   0.00000]])]\n",
            "111.61231994628906 56.963714599609375 485.6016845703125 487.1152038574219\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-0d6c9b2e1956>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{x1} {y1} {x2} {y2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ë®∫Êñ≠„ÅØ {class_name}„ÄÅÁ¢∫Áéá„ÅØ{prob * 100:.1f}%„Åß„Åô„ÄÇ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: tensor(0.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kc9fruyiJQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7Zp_wf_JQjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p5vrzYkJQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qa1NMrMQJQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5XdJerf7Zxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpdE1Mov7Zzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\""
      ],
      "metadata": {
        "id": "vwbKpW0n2ZOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python export.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt --nms --include coreml\n",
        "!python export.py --weights $weight_path --nms --include \"coreml\"\n"
      ],
      "metadata": {
        "id": "Nnxz6A9pyhvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# „Éë„Çπ„ÇíÊåáÂÆö„Åô„Çã\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅßÁîªÂÉè„ÇíÂàá„ÇäÊäú„Åè„Äç\n",
        "\n",
        "    if x1 < 0: #Ë≤†„ÅÆÂ†¥Âêà„ÅÆ„Ç®„É©„ÉºÂõûÈÅø\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # Âàá„ÇäÊäú„ÅÑ„ÅüÁîªÂÉè„Çí‰øùÂ≠ò„Åô„Çã\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcampÁî®csv„ÅÆimage_path„ÇíÊîπÂ§â)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxnOT6leGS9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-EigenCAM**\n",
        "\n",
        "https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
      ],
      "metadata": {
        "id": "YXt1zi1TGm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "DM7nTgGnjwYO",
        "outputId": "bc8c9e81-6dbc-4ec4-c0eb-c4f9edc3a822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'pytorch-grad-cam'...\n",
            "remote: Enumerating objects: 1122, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 1122 (delta 8), reused 9 (delta 3), pack-reused 1097\u001b[K\n",
            "Receiving objects: 100% (1122/1122), 110.21 MiB | 15.74 MiB/s, done.\n",
            "Resolving deltas: 100% (617/617), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "0xDMsB0GkMyg",
        "outputId": "239921df-d7d6-45a2-dfb8-89fda0fb434b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-grad-cam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "FzxYfIoemXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "id": "DKEgZ-Laojx8",
        "outputId": "872a8e02-519e-4a1d-cf17-45e907ac1b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from PIL import Image\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "def parse_detections(results):\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    detections = detections.to_dict()\n",
        "    boxes, colors, names = [], [], []\n",
        "\n",
        "    for i in range(len(detections[\"xmin\"])):\n",
        "        confidence = detections[\"confidence\"][i]\n",
        "        if confidence < 0.2:\n",
        "            continue\n",
        "        xmin = int(detections[\"xmin\"][i])\n",
        "        ymin = int(detections[\"ymin\"][i])\n",
        "        xmax = int(detections[\"xmax\"][i])\n",
        "        ymax = int(detections[\"ymax\"][i])\n",
        "        name = detections[\"name\"][i]\n",
        "        category = int(detections[\"class\"][i])\n",
        "        color = COLORS[category]\n",
        "\n",
        "        boxes.append((xmin, ymin, xmax, ymax))\n",
        "        colors.append(color)\n",
        "        names.append(name)\n",
        "    return boxes, colors, names\n",
        "\n",
        "\n",
        "def draw_detections(boxes, colors, names, img):\n",
        "    for box, color, name in zip(boxes, colors, names):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (xmin, ymin),\n",
        "            (xmax, ymax),\n",
        "            color,\n",
        "            2)\n",
        "\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n",
        "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
        "img = cv2.resize(img, (640, 640))\n",
        "rgb_img = img.copy()\n",
        "img = np.float32(img) / 255\n",
        "transform = transforms.ToTensor()\n",
        "tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "target_layers = [model.model.model.model[-2]]\n",
        "\n",
        "results = model([rgb_img])\n",
        "boxes, colors, names = parse_detections(results)\n",
        "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
        "Image.fromarray(detections)"
      ],
      "metadata": {
        "id": "i_WLp1kGjvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "„ÉªÂ§ñÈÉ®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàTreatedÔºâ„ÇíÊ¥ó„ÅÑÂá∫„Åó\n",
        "\n",
        "„ÉªÂÜÖÈÉ®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Åï„Çâ„Å´Ê∞¥Â¢ó„Åó\n",
        "\n",
        "„ÉªÂÜÖÈÉ®„Åä„Çà„Å≥Â§ñÈÉ®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çà„Çä„ÄÅtestÁî®ÂêÑ100ÊûöÔºàgrav50Êûö„ÄÅcont50ÊûöÔºâ„ÇíÊäú„ÅçÂá∫„Åó„Å¶„Åä„Åç„ÄÅÂêà‰Ωì„Åô„Çã\n",
        "\n",
        "„ÉªÊó¢Â≠ò„ÅÆYOLOv5„ÇíÁî®„ÅÑ„Å¶bounding box„ÇíÊäú„ÅçÂá∫„Åó„ÄÅÊñ∞„Åü„Å´„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„Çã"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}