{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Hertel_YOLOv5_MobileNetv3_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation YOLOv5-MobileNetv3 pipeline**\n",
        "\n",
        "Train YOLOv5\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlibで目が2つ検出されるものを抜き出す\n",
        "YOLOv5を用いて左右とバウンディングボックスを認識させる\n",
        "抜き出した画像についてMobileNetV3で回帰（5-fold ensemble）を行う\n",
        "スマホに実装\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc28f4c9-f84f-4ca9-b7c4-7a2cd9758175"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  8 17:55:15 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea869e7c-ac0f-419e-c458-0f95ce02eba1"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.89"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1decbed-53a0-44e4-b35f-d2afecc7b33f"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#親フォルダ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#元画像フォルダ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#元画像をコピー\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#切りぬいた画像を保存するフォルダ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#トレーニングされたYOLOv5で切り抜いた画像を保存するフォルダ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSVファイルのフォルダ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Refresh folder (内容が削除されるので注意！！) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\n",
        "# parent_dirがあれば削除する\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# 新しくparent_dirを作成する\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirを新規に作成する\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dirにdataset_dir直下のファイルをすべてコピーする\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"処理が完了しました。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnV1hEgIu87W",
        "outputId": "abe9b82a-2d69-424b-d59a-a7ce8d5e3526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 100%|██████████| 1016/1016 [00:12<00:00, 83.69file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "処理が完了しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeを用いて目を検出**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**目が2つ以上検出されたものを抜き出す**\n",
        "\n",
        "dlibで検出されたものから、上下左右に0.1倍ずつ拡大した範囲を抜き出している"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #フォルダ数の分だけ\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # 画像グレースケール化\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix以上のもので目に見えるものを抽出\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # 眼検出判定\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #画像の切り抜きと保存（2個以上検出の時に限る）\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #本来の切り抜きより幅の0.1倍ずつ水増しする\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #横幅の半分より左にあるのは右眼\n",
        "                      else:\n",
        "                          side = \"L\" #横幅の半分よりより右にあるのは左眼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #対応表の作成\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ここで、目以外が誤検出されているものを手動で抜き出して削除する**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csvから、削除して画像のパスが存在しなくなっている行を削除する\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrameを読み込む\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# 存在しない画像パスをチェックし、そのリストを保持する\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# 存在しない画像パスを表示\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # 存在しない画像パスの行を削除\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # 更新されたDataFrameを保存する\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "0dd85e94-04a3-49bb-f29a-597ea3ce525b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframeの整理**\n",
        "\n",
        "・ hertel_dfを参照して、coordinates_dfにヘルテル値を記入する\n",
        "\n",
        "・idが\"16_R, 16_L\"という形式になるようにデータフレームを整理する"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "af59f4c1-19cc-45be-8706-73637c192c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a03c0c0-42f0-43aa-84b8-94338e24855d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a03c0c0-42f0-43aa-84b8-94338e24855d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a03c0c0-42f0-43aa-84b8-94338e24855d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a03c0c0-42f0-43aa-84b8-94338e24855d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "画像パスの抽出（RLともに揃っているもの）\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "画像の分割 train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "フォルダの作成\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "画像のコピー\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7d2f38-40d0-4dcd-db66-bbb1b6525673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|██████████| 760/760 [00:07<00:00, 95.80it/s]\n",
            "Copying valid images: 100%|██████████| 190/190 [00:02<00:00, 72.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\" {side} {cx} {cy} {w} {h}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# trainとvalidのディレクトリパス\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# trainディレクトリでラベルファイルを生成\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# validディレクトリでラベルファイルを生成\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc520358-e145-474f-d72d-6cc75c0c8324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 760/760 [00:05<00:00, 145.37it/s]\n",
            "Processing images: 100%|██████████| 190/190 [01:49<00:00,  1.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## バウンディングボックスのサンプル描画 ##\n",
        "## (これは実行しなくて良い)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# バウンディングボックスを描画する関数\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            cx, cy, bw, bh, class_id = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得する関数\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# 画像パスとラベルファイルパス\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# 画像のサイズを取得\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhSI3Wc_-ylJ",
        "outputId": "3b95a0f1-0fca-4caf-eb69-76d351c1f064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "c4a19936-58b9-4350-869c-dc3620a76d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # YOLOv5_iFish --> train_batchの精度が低いので一旦却下とした\n",
        "# %cd $out_dir\n",
        "# !git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationを実装したバージョン\n",
        "# %cd yolov5-iFish\n",
        "# %pip install -qr requirements.txt\n",
        "\n",
        "# import torch\n",
        "# import utils\n",
        "# display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-33rbP1-iQ4",
        "outputId": "2599ce37-597f-4dbd-89b2-40ef60fb37d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 4d3e758 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 28.2/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "%cd $out_dir\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdYFiF39egKw",
        "outputId": "9f89e245-260b-45e8-8845-df1ba120885d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 28.2/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "id": "7iRcXa8I-iSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 途中から\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/data.yaml --resume $out_dir/yolov5/runs/train/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = f\"{out_dir}/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/eyecrop-yolov5n-167epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab71da51-b9a1-4dcf-a3a3-79b071427729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-167epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference original dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2355449e-b337-4be3-8f3e-62669f2b032a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-240-g84ec8b5 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 28.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = f\"{out_dir}/dataset_uni/eyecrop-yolov5n-iFish_169epoch.pt\"\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/eyecrop-yolov5n-167epoch.pt\"\n",
        "\n",
        "# もともとのデータセット\n",
        "orig_dir = orig_dir #元画像\n",
        "cropped_dir = cropped_dir #YOLOv5で切り抜いた画像用\n",
        "\n",
        "if os.path.exists(cropped_dir):\n",
        "    shutil.rmtree(cropped_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "os.makedirs(f\"{cropped_dir}/cropped_images\")\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "def make_letterbox_image(cv2_image): #letterbox_image作成、リサイズ\n",
        "    # 元の画像のサイズを取得\n",
        "    height, width = cv2_image.shape[:2]\n",
        "\n",
        "    # 正方形のサイズを決定（元の画像の長辺に合わせる）\n",
        "    square_size = max(width, height)\n",
        "\n",
        "    # 正方形のキャンバスを作成（背景は黒）\n",
        "    letterbox_img = np.zeros((square_size, square_size, 3), dtype=np.uint8)\n",
        "\n",
        "    # 元の画像を正方形の画像の中央に配置するための開始点（x,y）を計算\n",
        "    x_center = (square_size - width) // 2\n",
        "    y_center = (square_size - height) // 2\n",
        "\n",
        "    # 元の画像を正方形のキャンバスにコピーする\n",
        "    letterbox_img[y_center:y_center+height, x_center:x_center+width] = cv2_image\n",
        "\n",
        "    letterbox_img_resized = cv2.resize(letterbox_img,(250,250))\n",
        "\n",
        "    # 変換した画像を保存\n",
        "    return letterbox_img_resized"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "## バウンディングボックス&切り抜き demo ##\n",
        "###############################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "\"\"\"\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    # Assume the inference function requires a certain image size; resize if needed.\n",
        "    # Replace (640, 640) with the input size expected by your network.\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img_cv2_resized, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        cv2_imshow(letterbox_img)\n",
        "\n",
        "    # After all boxes are drawn, show and save the final image\n",
        "    cv2_imshow(img_cv2_resized)\n"
      ],
      "metadata": {
        "id": "P-J5WiSyXGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "## 切り抜き + 保存 ##\n",
        "#################\n",
        "\"\"\"\n",
        "Letterbox & 250px正方形にリサイズ\n",
        "cropped_dirに保存\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 0\n",
        "end_index = len(os.listdir(orig_dir))\n",
        "\n",
        "class_names = {0: \"R\", 1: \"L\"}\n",
        "\n",
        "for i in range(start_index, end_index):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)  # Ensure this function is defined\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2_resized = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    for bbox in pred[0]:\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(bbox[:6])\n",
        "\n",
        "        prob = bbox[4].item()\n",
        "        class_name = class_names[bbox[5].item()]\n",
        "\n",
        "        #print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "        # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "        img_height, img_width, _ = img_cv2_resized.shape[:3]\n",
        "        #print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "        padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "        padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "        x1 = x1 - padding_x\n",
        "        y1 = y1 - padding_y\n",
        "        x2 = x2 - padding_x\n",
        "        y2 = y2 - padding_y\n",
        "        #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Crop and save the image\n",
        "        mag = 640 / img_cv2.shape[1]\n",
        "        cropped_img = img_cv2[int(y1/mag):int(y2/mag), int(x1/mag):int(x2/mag)] #バウンディングボックスで切り抜き\n",
        "        letterbox_img = make_letterbox_image(cropped_img)\n",
        "        #cv2_imshow(letterbox_img)\n",
        "\n",
        "        base_name = os.path.splitext(os.path.basename(img))[0]\n",
        "        cropped_img_path = os.path.join(f\"{cropped_dir}/cropped_images\", f\"{base_name}_{class_name}.png\")\n",
        "        cv2.imwrite(cropped_img_path, letterbox_img)\n",
        "        print(f\"succefully saved, image {i}: {cropped_img_path}\")\n"
      ],
      "metadata": {
        "id": "iAelqChiXGne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(f\"{cropped_dir}/cropped_images\"))"
      ],
      "metadata": {
        "id": "kc9fruyiJQcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4698bc0e-730d-411b-ae16-57f6e4538ec0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To do\n",
        "\n",
        "・Group 5-fold split\n",
        "・MobileNetV3でcross validation --> 精度プロット作成\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_estimation_crossvalidation_noTestset.ipynb\n",
        "https://github.com/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_ensemble_quick.ipynb\n",
        "・当院データセットでtestする\n",
        "'''"
      ],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3 using cropped images**\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_yolo_cropped/cropped_images\n",
        "の画像を手動で確認、不適切な画像を削除\n",
        "\n",
        "・https://tcd-theme.com/2019/12/mac-zip-compression.htmlを参考にして圧縮\n",
        "\n",
        "・dataset_uni_for_YOLOv5/dataset_cropped_for_MobileNet_training/cropped_images.zipとしてアップロード"
      ],
      "metadata": {
        "id": "lvkfZ9aQ9S1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv5で抜き出した画像を規定のフォルダに移動"
      ],
      "metadata": {
        "id": "rKFHyFisE2Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5で抜き出した画像を規定のフォルダに移動\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "parent_folder = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training'\n",
        "\n",
        "# zipファイルのパス\n",
        "zip_path = f'{parent_folder}/cropped_images.zip'\n",
        "\n",
        "# 展開するディレクトリのパス\n",
        "extract_folder = f'{parent_folder}/cropped_images'\n",
        "'/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training/cropped_images'\n",
        "\n",
        "# フォルダがない場合は新規作成\n",
        "if os.path.exists(extract_folder):\n",
        "    shutil.rmtree(extract_folder)\n",
        "os.makedirs(extract_folder)\n",
        "\n",
        "# zipファイルを解凍\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(parent_folder)\n"
      ],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ここから"
      ],
      "metadata": {
        "id": "vdOpequeE781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install pingouin --q\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True"
      ],
      "metadata": {
        "id": "q7Zp_wf_JQjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cc5d3d-606e-4a30-a737-d1fadd8df07a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer) (2.1.0+cu118)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRandom Seed:  1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_cropped_250px_MobileNet_training\"\n",
        "os.chdir(path)\n",
        "\n",
        "# grav or cont, age, and sex\n",
        "#NUM_CLASSES = 3\n",
        "\n",
        "# contains train, val\n",
        "DATASET_PATH = r\"./cropped_images\"\n",
        "#TRAIN_FOLDER_NAME = \"train\"\n",
        "#VAL_FOLDER_NAME = \"val\"\n",
        "#EFFICIENT_NET_NAME = \"RepVGG-A2-train\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_unilateral.csv\"\n",
        "#OPTIMIZER_PATH = \"./optimizer_multi.pth\"\n",
        "#SEX_DICT_PATH = \"gender_json\"\n",
        "#AGE_DICT_PATH = \"age_json\"\n",
        "LOG_PATH = \"./log_multi.txt\"\n",
        "ROC_PATH = \"./roc_multi.png\"\n",
        "#CHECKPOINT_COUNT = 10\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# transforms param\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])])"
      ],
      "metadata": {
        "id": "8lzHTjGEL8eQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-Foldに分割**"
      ],
      "metadata": {
        "id": "kW9iYtvtOPiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(DATASET_PATH)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"patiend num: {}\".format(len(id)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMO4j991OO8a",
        "outputId": "dc0b661d-5ccf-4240-c57b-944f91f71283"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patiend num: 1949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #testsetなし。Group K-foldを用いてデータセット分け\n",
        "# from sklearn.model_selection import GroupKFold\n",
        "# from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# #fold数だけ空のリストを作成\n",
        "# num_folds = 5\n",
        "# train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "# X = np.ones(len(id))\n",
        "# y = np.ones(len(id))\n",
        "# gkf = GroupKFold(n_splits=num_folds)\n",
        "# i=0\n",
        "# for train_idxs, val_idxs in gkf.split(X, y, groups=patient):\n",
        "#     for idx in train_idxs:\n",
        "#         train_set[i].append(path_list[idx])\n",
        "#     for idx in val_idxs:\n",
        "#         val_set[i].append(path_list[idx])\n",
        "#     i+=1\n",
        "\n",
        "# print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "# print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "\n",
        "# print(\"extracted_id (example): {}\".format(extract_ids(train_set[0])[0]))"
      ],
      "metadata": {
        "id": "4QNmg_1gOO9y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split ＋　Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9で分割\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "groups = patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjQNXUD2OO_2",
        "outputId": "899e22cd-24e0-4f2a-b0ea-243562815f2c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1404\n",
            "val_dataset: 351\n",
            "test_dataset: 194\n",
            "\n",
            "extracted_id (example): 355_L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create Datasets**"
      ],
      "metadata": {
        "id": "_hiIomNwbBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "\n",
        "    def __init__(self, img_list, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for image_path in img_list:\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([image_path, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.item_paths[index][0]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image).float()\n",
        "        hertel = self.item_paths[index][1]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IteKm-r5brwf",
        "outputId": "b1a0fb3c-be74-4a4d-a5fd-89c37b560613"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1404\n",
            "val_dataset_size: 351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test with early-stopping**"
      ],
      "metadata": {
        "id": "JGWnnohkWBpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size,optimizer, patience, n_epochs, device,  alpha=0):\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "\n",
        "            #l2_normalization\n",
        "            l2 = torch.tensor(0., requires_grad=True)\n",
        "            for w in model.parameters():\n",
        "                l2 = l2 + torch.norm(w)**2\n",
        "            loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(n_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "\n",
        "        print(print_msg)\n",
        "\n",
        "\n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "iJyYRN8SOPB0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ConvNetの調整**"
      ],
      "metadata": {
        "id": "E2WdAbf9WMMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###オリジナルRepVGG-A2使用\n"
      ],
      "metadata": {
        "id": "w4uM5uvHV8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc1 = nn.Linear(in_features=1408, out_features=512) #out_featuresを1に\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep_learning/666mai_dataset/RepVGG-A2-train.pth\"))\n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "xooKuBLyV_xH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Timm使用の場合"
      ],
      "metadata": {
        "id": "DLA4qqaoYeCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "efFP6EU1WH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc = nn.Linear(in_features=1280, out_features=1) #モデルに応じてin_featuresを調整、out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x) #dropoutを1層追加\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # fc layer 2つのバージョン\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "#         self.fc1 = nn.Linear(in_features=1280, out_features=512) #モデルに応じてin_featuresを調整\n",
        "#         self.fc2 = nn.Linear(in_features=512, out_features=1) #out_featuresを1に\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Batch_norm plus dropout (for RepVGG_A2: イマイチ)\n",
        "# class mod_CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(mod_CNNModel, self).__init__()\n",
        "#         CNNModel = model_ft\n",
        "#         self.CNNModel = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "#         self.bn = nn.BatchNorm2d(1408)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "#         self.dropout = nn.Dropout(0.25)\n",
        "#         self.fc = nn.Linear(in_features=12672, out_features=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.CNNModel(x)\n",
        "#         x = self.bn(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.maxpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "#model_ft = timm.create_model(model_name = 'efficientnetv2_rw_m', pretrained=True, num_classes=1)\n",
        "#model_ft = timm.create_model(model_name = 'repvgg_a2', pretrained=True, num_classes=1)\n",
        "model_ft = timm.create_model(model_name = 'mobilenetv3_large_100', pretrained=True, num_classes=1)\n",
        "#model_ft = mod_CNNModel()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "-VvPVPdJWIER"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNnQfLn4WIGc",
        "outputId": "5039cef9-c1d4-4408-ab70-e15ea04ccc90"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [  1/100] \n",
            "train_loss: 18.91317 \n",
            "valid_loss: 14.58262 \n",
            "Validation loss decreased (inf --> 14.582616).  Saving model ...\n",
            "\n",
            "Epoch: [  2/100] \n",
            "train_loss: 7.67112 \n",
            "valid_loss: 8.04615 \n",
            "Validation loss decreased (14.582616 --> 8.046155).  Saving model ...\n",
            "\n",
            "Epoch: [  3/100] \n",
            "train_loss: 6.22878 \n",
            "valid_loss: 6.64821 \n",
            "Validation loss decreased (8.046155 --> 6.648208).  Saving model ...\n",
            "\n",
            "Epoch: [  4/100] \n",
            "train_loss: 5.87313 \n",
            "valid_loss: 13.56835 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [  5/100] \n",
            "train_loss: 4.84096 \n",
            "valid_loss: 5.28922 \n",
            "Validation loss decreased (6.648208 --> 5.289216).  Saving model ...\n",
            "\n",
            "Epoch: [  6/100] \n",
            "train_loss: 3.75421 \n",
            "valid_loss: 5.51533 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [  7/100] \n",
            "train_loss: 3.13542 \n",
            "valid_loss: 9.90883 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [  8/100] \n",
            "train_loss: 2.49178 \n",
            "valid_loss: 7.74918 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [  9/100] \n",
            "train_loss: 1.74136 \n",
            "valid_loss: 6.35754 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 10/100] \n",
            "train_loss: 1.35219 \n",
            "valid_loss: 5.76832 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 11/100] \n",
            "train_loss: 1.21911 \n",
            "valid_loss: 5.55197 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 12/100] \n",
            "train_loss: 1.08287 \n",
            "valid_loss: 5.12677 \n",
            "Validation loss decreased (5.289216 --> 5.126769).  Saving model ...\n",
            "\n",
            "Epoch: [ 13/100] \n",
            "train_loss: 1.07765 \n",
            "valid_loss: 4.79587 \n",
            "Validation loss decreased (5.126769 --> 4.795874).  Saving model ...\n",
            "\n",
            "Epoch: [ 14/100] \n",
            "train_loss: 1.11027 \n",
            "valid_loss: 5.75459 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 15/100] \n",
            "train_loss: 0.94387 \n",
            "valid_loss: 8.99635 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 16/100] \n",
            "train_loss: 1.21405 \n",
            "valid_loss: 10.33668 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 17/100] \n",
            "train_loss: 1.17266 \n",
            "valid_loss: 10.88464 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 18/100] \n",
            "train_loss: 1.33838 \n",
            "valid_loss: 7.96231 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 19/100] \n",
            "train_loss: 1.18269 \n",
            "valid_loss: 5.22872 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 20/100] \n",
            "train_loss: 1.27602 \n",
            "valid_loss: 4.55087 \n",
            "Validation loss decreased (4.795874 --> 4.550869).  Saving model ...\n",
            "\n",
            "Epoch: [ 21/100] \n",
            "train_loss: 1.36419 \n",
            "valid_loss: 4.81522 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 22/100] \n",
            "train_loss: 1.56824 \n",
            "valid_loss: 4.76591 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 23/100] \n",
            "train_loss: 1.40297 \n",
            "valid_loss: 4.67340 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 24/100] \n",
            "train_loss: 1.39338 \n",
            "valid_loss: 4.73090 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 25/100] \n",
            "train_loss: 1.58418 \n",
            "valid_loss: 4.81187 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 26/100] \n",
            "train_loss: 1.46891 \n",
            "valid_loss: 5.59374 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 27/100] \n",
            "train_loss: 1.29202 \n",
            "valid_loss: 5.56782 \n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "Epoch: [ 28/100] \n",
            "train_loss: 1.25196 \n",
            "valid_loss: 6.54894 \n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "Epoch: [ 29/100] \n",
            "train_loss: 1.05174 \n",
            "valid_loss: 4.75074 \n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "Epoch: [ 30/100] \n",
            "train_loss: 1.03657 \n",
            "valid_loss: 4.25385 \n",
            "Validation loss decreased (4.550869 --> 4.253849).  Saving model ...\n",
            "\n",
            "Epoch: [ 31/100] \n",
            "train_loss: 0.92190 \n",
            "valid_loss: 4.13048 \n",
            "Validation loss decreased (4.253849 --> 4.130481).  Saving model ...\n",
            "\n",
            "Epoch: [ 32/100] \n",
            "train_loss: 0.86877 \n",
            "valid_loss: 4.97946 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 33/100] \n",
            "train_loss: 0.80757 \n",
            "valid_loss: 4.29833 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 34/100] \n",
            "train_loss: 0.68912 \n",
            "valid_loss: 4.07877 \n",
            "Validation loss decreased (4.130481 --> 4.078767).  Saving model ...\n",
            "\n",
            "Epoch: [ 35/100] \n",
            "train_loss: 0.72423 \n",
            "valid_loss: 4.45602 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 36/100] \n",
            "train_loss: 0.70187 \n",
            "valid_loss: 4.41310 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 37/100] \n",
            "train_loss: 0.67185 \n",
            "valid_loss: 4.38679 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 38/100] \n",
            "train_loss: 0.72899 \n",
            "valid_loss: 4.78872 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 39/100] \n",
            "train_loss: 0.76810 \n",
            "valid_loss: 6.25411 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 40/100] \n",
            "train_loss: 0.89929 \n",
            "valid_loss: 5.40351 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 41/100] \n",
            "train_loss: 0.98450 \n",
            "valid_loss: 5.69691 \n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "Epoch: [ 42/100] \n",
            "train_loss: 0.87892 \n",
            "valid_loss: 5.18343 \n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "Epoch: [ 43/100] \n",
            "train_loss: 0.92880 \n",
            "valid_loss: 5.87987 \n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "Epoch: [ 44/100] \n",
            "train_loss: 0.79168 \n",
            "valid_loss: 4.91957 \n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "Epoch: [ 45/100] \n",
            "train_loss: 0.92025 \n",
            "valid_loss: 4.33363 \n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "Epoch: [ 46/100] \n",
            "train_loss: 0.89605 \n",
            "valid_loss: 3.84904 \n",
            "Validation loss decreased (4.078767 --> 3.849039).  Saving model ...\n",
            "\n",
            "Epoch: [ 47/100] \n",
            "train_loss: 0.88781 \n",
            "valid_loss: 4.31513 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 48/100] \n",
            "train_loss: 0.98281 \n",
            "valid_loss: 4.41786 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 49/100] \n",
            "train_loss: 0.80350 \n",
            "valid_loss: 4.57508 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 50/100] \n",
            "train_loss: 0.53811 \n",
            "valid_loss: 4.75445 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 51/100] \n",
            "train_loss: 0.46307 \n",
            "valid_loss: 5.37330 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 52/100] \n",
            "train_loss: 0.42855 \n",
            "valid_loss: 4.85220 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 53/100] \n",
            "train_loss: 0.48040 \n",
            "valid_loss: 4.90055 \n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "Epoch: [ 54/100] \n",
            "train_loss: 0.51353 \n",
            "valid_loss: 4.54492 \n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "Epoch: [ 55/100] \n",
            "train_loss: 0.55367 \n",
            "valid_loss: 4.49206 \n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "Epoch: [ 56/100] \n",
            "train_loss: 0.58946 \n",
            "valid_loss: 5.84026 \n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "Epoch: [ 57/100] \n",
            "train_loss: 0.70184 \n",
            "valid_loss: 6.70988 \n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "Epoch: [ 58/100] \n",
            "train_loss: 0.69477 \n",
            "valid_loss: 5.23311 \n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "Epoch: [ 59/100] \n",
            "train_loss: 0.88174 \n",
            "valid_loss: 4.58848 \n",
            "EarlyStopping counter: 13 out of 20\n",
            "\n",
            "Epoch: [ 60/100] \n",
            "train_loss: 1.07565 \n",
            "valid_loss: 4.24224 \n",
            "EarlyStopping counter: 14 out of 20\n",
            "\n",
            "Epoch: [ 61/100] \n",
            "train_loss: 0.90068 \n",
            "valid_loss: 4.89631 \n",
            "EarlyStopping counter: 15 out of 20\n",
            "\n",
            "Epoch: [ 62/100] \n",
            "train_loss: 0.88842 \n",
            "valid_loss: 3.82462 \n",
            "Validation loss decreased (3.849039 --> 3.824619).  Saving model ...\n",
            "\n",
            "Epoch: [ 63/100] \n",
            "train_loss: 0.69388 \n",
            "valid_loss: 4.15300 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 64/100] \n",
            "train_loss: 0.55484 \n",
            "valid_loss: 4.86236 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 65/100] \n",
            "train_loss: 0.42413 \n",
            "valid_loss: 6.03673 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 66/100] \n",
            "train_loss: 0.46171 \n",
            "valid_loss: 4.59465 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 67/100] \n",
            "train_loss: 0.43231 \n",
            "valid_loss: 3.71642 \n",
            "Validation loss decreased (3.824619 --> 3.716421).  Saving model ...\n",
            "\n",
            "Epoch: [ 68/100] \n",
            "train_loss: 0.43961 \n",
            "valid_loss: 3.78597 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 69/100] \n",
            "train_loss: 0.42549 \n",
            "valid_loss: 4.04615 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 70/100] \n",
            "train_loss: 0.48749 \n",
            "valid_loss: 4.01890 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 71/100] \n",
            "train_loss: 0.47350 \n",
            "valid_loss: 5.24284 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 72/100] \n",
            "train_loss: 0.55693 \n",
            "valid_loss: 4.85453 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 73/100] \n",
            "train_loss: 0.52094 \n",
            "valid_loss: 4.64352 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 74/100] \n",
            "train_loss: 0.66100 \n",
            "valid_loss: 4.40107 \n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "Epoch: [ 75/100] \n",
            "train_loss: 0.68922 \n",
            "valid_loss: 4.06944 \n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "Epoch: [ 76/100] \n",
            "train_loss: 0.67160 \n",
            "valid_loss: 3.81877 \n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "Epoch: [ 77/100] \n",
            "train_loss: 0.51376 \n",
            "valid_loss: 3.97596 \n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "Epoch: [ 78/100] \n",
            "train_loss: 0.37809 \n",
            "valid_loss: 4.13127 \n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "Epoch: [ 79/100] \n",
            "train_loss: 0.37195 \n",
            "valid_loss: 3.95685 \n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "Epoch: [ 80/100] \n",
            "train_loss: 0.38957 \n",
            "valid_loss: 3.62130 \n",
            "Validation loss decreased (3.716421 --> 3.621297).  Saving model ...\n",
            "\n",
            "Epoch: [ 81/100] \n",
            "train_loss: 0.39436 \n",
            "valid_loss: 3.88935 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [ 82/100] \n",
            "train_loss: 0.44377 \n",
            "valid_loss: 4.05251 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [ 83/100] \n",
            "train_loss: 0.45214 \n",
            "valid_loss: 4.10259 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [ 84/100] \n",
            "train_loss: 0.45806 \n",
            "valid_loss: 3.94754 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [ 85/100] \n",
            "train_loss: 0.43960 \n",
            "valid_loss: 3.98638 \n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "Epoch: [ 86/100] \n",
            "train_loss: 0.52883 \n",
            "valid_loss: 3.93701 \n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "Epoch: [ 87/100] \n",
            "train_loss: 0.50803 \n",
            "valid_loss: 4.35409 \n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "Epoch: [ 88/100] \n",
            "train_loss: 0.46639 \n",
            "valid_loss: 4.88601 \n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "Epoch: [ 89/100] \n",
            "train_loss: 0.51791 \n",
            "valid_loss: 5.04644 \n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "Epoch: [ 90/100] \n",
            "train_loss: 0.46456 \n",
            "valid_loss: 3.95375 \n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "Epoch: [ 91/100] \n",
            "train_loss: 0.53105 \n",
            "valid_loss: 4.27147 \n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "Epoch: [ 92/100] \n",
            "train_loss: 0.54454 \n",
            "valid_loss: 4.42636 \n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "Epoch: [ 93/100] \n",
            "train_loss: 0.45334 \n",
            "valid_loss: 4.96877 \n",
            "EarlyStopping counter: 13 out of 20\n",
            "\n",
            "Epoch: [ 94/100] \n",
            "train_loss: 0.35580 \n",
            "valid_loss: 4.94498 \n",
            "EarlyStopping counter: 14 out of 20\n",
            "\n",
            "Epoch: [ 95/100] \n",
            "train_loss: 0.34633 \n",
            "valid_loss: 5.09907 \n",
            "EarlyStopping counter: 15 out of 20\n",
            "\n",
            "Epoch: [ 96/100] \n",
            "train_loss: 0.35180 \n",
            "valid_loss: 5.61374 \n",
            "EarlyStopping counter: 16 out of 20\n",
            "\n",
            "Epoch: [ 97/100] \n",
            "train_loss: 0.37984 \n",
            "valid_loss: 5.25753 \n",
            "EarlyStopping counter: 17 out of 20\n",
            "\n",
            "Epoch: [ 98/100] \n",
            "train_loss: 0.38089 \n",
            "valid_loss: 6.04974 \n",
            "EarlyStopping counter: 18 out of 20\n",
            "\n",
            "Epoch: [ 99/100] \n",
            "train_loss: 0.38408 \n",
            "valid_loss: 6.59997 \n",
            "EarlyStopping counter: 19 out of 20\n",
            "\n",
            "Epoch: [100/100] \n",
            "train_loss: 0.38620 \n",
            "valid_loss: 5.45347 \n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Draw Learning Curves**"
      ],
      "metadata": {
        "id": "nGCIPkQbd8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1\n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "HRqX-uCLWIIZ",
        "outputId": "00f83788-e157-4a17-f55b-19b536910384"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluation using testset**"
      ],
      "metadata": {
        "id": "HFAFvtSxeBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in val_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcZBDepNWIKm",
        "outputId": "f5a512fa-2e41-454f-e8e2-4a17b0451ee2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: 0.0664682442646081\n",
            "StdError: 1.9045242943672378\n",
            "AveAbsError: 1.4934253285073826\n",
            "StdAbsError: 1.181080812575389\n",
            "\n",
            "Corrected_AveAbsError: 1.4889594072405439\n",
            "Corrected_StdAbsError: 1.1848537400785073\n",
            "Round_Corrected_AveAbsError: 1.4672364672364673\n",
            "Round_Corrected_StdAbsError: 1.2275099294020504\n",
            "Probability of AbsError <= 1: 0.4301994301994302\n",
            "Probability of AbsError <= 2: 0.7236467236467237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using test dataset\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      target = target.view(len(target), 1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())\n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #平均誤差\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #誤差標準偏差\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #平均絶対誤差\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #絶対誤差標準偏差\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #平均絶対誤差(四捨五入)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #絶対誤差標準偏差(四捨五入)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate the probabilities\n",
        "abs_error_np = np.array(AbsError)\n",
        "prob_less_than_1 = np.sum(abs_error_np <= 1) / len(AbsError)\n",
        "prob_less_than_2 = np.sum(abs_error_np <= 2) / len(AbsError)\n",
        "\n",
        "print('Probability of AbsError <= 1:', prob_less_than_1)\n",
        "print('Probability of AbsError <= 2:', prob_less_than_2)\n"
      ],
      "metadata": {
        "id": "8p5vrzYkJQlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e177d8-5069-4d39-b837-8c788b1175e0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveError: 0.2695721940895946\n",
            "StdError: 2.0043178747686596\n",
            "AveAbsError: 1.6370583121309574\n",
            "StdAbsError: 1.1817318287714869\n",
            "\n",
            "Corrected_AveError: 2.5638139950106705e-16\n",
            "Corrected_StdError: 2.0043178747686596\n",
            "Corrected_AveAbsError: 1.6161079593743182\n",
            "Corrected_StdAbsError: 1.179810382519211\n",
            "Round_Corrected_AveAbsError: 1.6185567010309279\n",
            "Round_Corrected_StdAbsError: 1.250436599143747\n",
            "\n",
            "Probability of AbsError <= 1: 0.3865979381443299\n",
            "Probability of AbsError <= 2: 0.7061855670103093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "id": "qa1NMrMQJQnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "6fdcba70-0226-44f5-8663-f9e5f20591fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ee0731ed3a78>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Draw Graphs（もともとの散布図\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'estimate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'whitegrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bland-Altman-Plot\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fs7DIPTJeVpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#線形近似式算出\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# 説明変数xに \"x1\"のデータを使用\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# 目的変数yに \"x2\"のデータを使用\n",
        "y = target.values\n",
        "\n",
        "# 予測モデルを作成（単回帰）\n",
        "clf.fit(x, y)\n",
        "\n",
        "# パラメータ（回帰係数、切片）を抽出\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# パラメータの表示\n",
        "print(\"回帰係数:\", a)\n",
        "print(\"切片:\", b)\n",
        "print(\"決定係数:\", clf.score(x, y))\n",
        "\n",
        "#平均値により補正した値\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#回帰直線により補正した値\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#残差\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#残差\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]"
      ],
      "metadata": {
        "id": "_1MEuRIReVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "heTFISEt9D_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#平均近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "\n",
        "# print(\"\")\n",
        "# print('Error<-1 and Error>-1: ' + str(sum((-1 < i < 1 for i in df['Residual_error_2']))))\n",
        "# print('Error<-2 and Error>2: ' + str(sum((-2 < i < 2 for i in df['Residual_error_2']))))\n",
        "# print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "# print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "total_errors = len(df['Residual_error_2'])\n",
        "\n",
        "error_minus1_to_1_count = sum((-1 < i < 1 for i in df['Residual_error_2']))\n",
        "error_minus1_to_1_percentage = (error_minus1_to_1_count / total_errors) * 100\n",
        "\n",
        "error_minus2_to_2_count = sum((-2 < i < 2 for i in df['Residual_error_2']))\n",
        "error_minus2_to_2_percentage = (error_minus2_to_2_count / total_errors) * 100\n",
        "\n",
        "error_less_equal_minus2_count = sum((i <= -2 for i in df['Residual_error_2']))\n",
        "error_less_equal_minus2_percentage = (error_less_equal_minus2_count / total_errors) * 100\n",
        "\n",
        "error_greater_equal_2_count = sum((i >= 2 for i in df['Residual_error_2']))\n",
        "error_greater_equal_2_percentage = (error_greater_equal_2_count / total_errors) * 100\n",
        "\n",
        "print(\"\")\n",
        "print(f'Error<-1 and Error>-1: {error_minus1_to_1_count}/{total_errors} ({error_minus1_to_1_percentage:.2f}%)')\n",
        "print(f'Error<-2 and Error>2: {error_minus2_to_2_count}/{total_errors} ({error_minus2_to_2_percentage:.2f}%)')\n",
        "print(f'Error<=-2: {error_less_equal_minus2_count}/{total_errors} ({error_less_equal_minus2_percentage:.2f}%)')\n",
        "print(f'Error>=2: {error_greater_equal_2_count}/{total_errors} ({error_greater_equal_2_percentage:.2f}%)')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1\n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1\n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')"
      ],
      "metadata": {
        "id": "cHjgGcbk9EBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cs3KMCqreVvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5XdJerf7Zxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpdE1Mov7Zzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\""
      ],
      "metadata": {
        "id": "vwbKpW0n2ZOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python export.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt --nms --include coreml\n",
        "!python export.py --weights $weight_path --nms --include \"coreml\"\n"
      ],
      "metadata": {
        "id": "Nnxz6A9pyhvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxnOT6leGS9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-EigenCAM**\n",
        "\n",
        "https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
      ],
      "metadata": {
        "id": "YXt1zi1TGm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "DM7nTgGnjwYO",
        "outputId": "bc8c9e81-6dbc-4ec4-c0eb-c4f9edc3a822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'pytorch-grad-cam'...\n",
            "remote: Enumerating objects: 1122, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 1122 (delta 8), reused 9 (delta 3), pack-reused 1097\u001b[K\n",
            "Receiving objects: 100% (1122/1122), 110.21 MiB | 15.74 MiB/s, done.\n",
            "Resolving deltas: 100% (617/617), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "0xDMsB0GkMyg",
        "outputId": "239921df-d7d6-45a2-dfb8-89fda0fb434b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-grad-cam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "FzxYfIoemXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "id": "DKEgZ-Laojx8",
        "outputId": "872a8e02-519e-4a1d-cf17-45e907ac1b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from PIL import Image\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "def parse_detections(results):\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    detections = detections.to_dict()\n",
        "    boxes, colors, names = [], [], []\n",
        "\n",
        "    for i in range(len(detections[\"xmin\"])):\n",
        "        confidence = detections[\"confidence\"][i]\n",
        "        if confidence < 0.2:\n",
        "            continue\n",
        "        xmin = int(detections[\"xmin\"][i])\n",
        "        ymin = int(detections[\"ymin\"][i])\n",
        "        xmax = int(detections[\"xmax\"][i])\n",
        "        ymax = int(detections[\"ymax\"][i])\n",
        "        name = detections[\"name\"][i]\n",
        "        category = int(detections[\"class\"][i])\n",
        "        color = COLORS[category]\n",
        "\n",
        "        boxes.append((xmin, ymin, xmax, ymax))\n",
        "        colors.append(color)\n",
        "        names.append(name)\n",
        "    return boxes, colors, names\n",
        "\n",
        "\n",
        "def draw_detections(boxes, colors, names, img):\n",
        "    for box, color, name in zip(boxes, colors, names):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (xmin, ymin),\n",
        "            (xmax, ymax),\n",
        "            color,\n",
        "            2)\n",
        "\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n",
        "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
        "img = cv2.resize(img, (640, 640))\n",
        "rgb_img = img.copy()\n",
        "img = np.float32(img) / 255\n",
        "transform = transforms.ToTensor()\n",
        "tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "target_layers = [model.model.model.model[-2]]\n",
        "\n",
        "results = model([rgb_img])\n",
        "boxes, colors, names = parse_detections(results)\n",
        "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
        "Image.fromarray(detections)"
      ],
      "metadata": {
        "id": "i_WLp1kGjvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "・外部のデータセット（Treated）を洗い出し\n",
        "\n",
        "・内部のデータセットをさらに水増し\n",
        "\n",
        "・内部および外部データセットより、test用各100枚（grav50枚、cont50枚）を抜き出しておき、合体する\n",
        "\n",
        "・既存のYOLOv5を用いてbounding boxを抜き出し、新たにトレーニングする"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}