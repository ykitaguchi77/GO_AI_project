{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgTAxdWzUBRIGhq1Oglsso",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Interference_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference using pretrained models**"
      ],
      "metadata": {
        "id": "7bnfu3qWA2lX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF6RLIRqARlg",
        "outputId": "9b8912c8-3d86-4eaa-b98d-d2e60952aa38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.12.1+cu113 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#GDriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_newPatient_250px\"\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/5-fold_20221019/Models/moblenetv3_large_100.pth\""
      ],
      "metadata": {
        "id": "laxJknZhCVBG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the image paths\n",
        "image_list = [image_folder_path +\"/スライド\"+str(i+1)+\".jpeg\" for i in range(len(os.listdir(image_folder_path)))]\n",
        "label_list = [1]*len(image_list)"
      ],
      "metadata": {
        "id": "y4-J8tuNA8RZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show sample images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "root = image_folder_path #画像があるフォルダ。適宜変えてください\n",
        "lsdir = os.listdir(root)\n",
        "\n",
        "imgs = []\n",
        "for l in lsdir:\n",
        "    target = os.path.join(root,l)\n",
        "    img = cv2.imread(target)\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) #pyplotで表示するために色変換\n",
        "    imgs.append(img)\n",
        "\n",
        "shownumber = 6 #画像を並べる数\n",
        "showaxis = 1\n",
        "\n",
        "while(showaxis*showaxis < shownumber):\n",
        "    showaxis += 1\n",
        "\n",
        "cnt = 0\n",
        "while(1):\n",
        "    limit = 6\n",
        "    if cnt >= limit:\n",
        "       break\n",
        "    fig,axs = plt.subplots(showaxis,showaxis, figsize=(16.0, 12.0))\n",
        "    ar = axs.ravel()\n",
        "    for i in range(showaxis*showaxis):\n",
        "        ar[i].axis('off')\n",
        "        if i < shownumber:\n",
        "            ar[i].imshow(imgs[cnt])\n",
        "            cnt += 1\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iACUWQq356Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load model**"
      ],
      "metadata": {
        "id": "t4OiMurgHhRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model \n",
        "##########################\n",
        "!pip install --quiet timm\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_ft.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0s6sfExHeeZ",
        "outputId": "f9de0f4b-f3b5-4b39-c962-b637030953bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "from PIL import *\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "def image_loader(image_path):\n",
        "    \"\"\"load image, returns cuda tensor\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    image = img_transforms(image).float()\n",
        "    image = image.unsqueeze(0) \n",
        "    return image.to(device)\n",
        "\n",
        "def my_round(val, digit=0):\n",
        "    p = 10 ** digit\n",
        "    return (val * p * 2 + 1) // 2 / p\n",
        "\n",
        "def interference(image_list):\n",
        "    image_tensor = image_loader(path)\n",
        "\n",
        "    model_ft.to(device)\n",
        "    model_ft.eval()\n",
        "    output = model_ft(image_tensor)\n",
        "    _, pred = torch.max(output, 1) \n",
        "    pred = pred[0].to('cpu').detach().numpy().copy().tolist() \n",
        "\n",
        "    prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "    prob = prob[0][1].cpu().detach().numpy().copy().tolist() #probalility of being positive\n",
        "    prob = my_round(prob, 3)\n",
        "\n",
        "    return pred, prob, image_tensor"
      ],
      "metadata": {
        "id": "vYhubvrbRid7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple image eval\n",
        "pred_list, prob_list = [], []\n",
        "for idx, path in enumerate(image_list, 1):\n",
        "    pred, prob, _ = interference(path) \n",
        "    pred_list.append(pred)\n",
        "    prob_list.append(prob)\n",
        "print(label_list)\n",
        "print(pred_list)\n",
        "print(prob_list)\n",
        "\n",
        "df = pd.DataFrame(index=[], columns=[])\n",
        "df[\"label\"] = label_list\n",
        "df[\"pred\"] = pred_list\n",
        "df[\"prob\"] = prob_list\n",
        "df"
      ],
      "metadata": {
        "id": "kmELszx_XJlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single image eval\n",
        "num = 0\n",
        "path = image_list[num]\n",
        "label = label_list[num]\n",
        "print(f\"path: {path}\")\n",
        "\n",
        "pred, prob, tensor = interference(path) \n",
        "print(f\"label: {label}, pred: {pred}, prob: {prob}\")\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "i4ChqsnIXgfj",
        "outputId": "4d1800e1-f3ea-48d4-a628-09ef8c3480a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_newPatient_250px/スライド1.jpeg\n",
            "label: 1, pred: 1, prob: 0.623\n",
            "tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          ...,\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          ...,\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          ...,\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Output CoreML**"
      ],
      "metadata": {
        "id": "kGUYCKO6pe-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML_example\n",
        "###########################\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import coremltools as ct\n",
        "import numpy as np\n",
        "!pip install --quiet timm\n",
        "\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "\n",
        "base_model = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            base_model,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "import urllib\n",
        "label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = class_labels[1:] # remove the first class which is background\n",
        "assert len(class_labels) == 1000\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "image_input = ct.ImageType(name=\"input_1\",\n",
        "                           shape=example_input.shape,\n",
        "                           scale=scale,\n",
        "                           bias=[red_bias, green_bias, blue_bias])\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[image_input], \n",
        "    classifier_config = ct.ClassifierConfig(class_labels), \n",
        "    compute_units=ct.ComputeUnit.CPU_ONLY,\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(\"MobileNetV3_pytorch.mlmodel\")"
      ],
      "metadata": {
        "id": "EN85pcwkeznp",
        "outputId": "a8106d96-bd25-4182-bcbf-7f2d5a197dc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 906.63 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 38/38 [00:01<00:00, 23.58 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 39.47 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:01<00:00, 574.83 ops/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML 飛ばして下さい\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "    classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(\"/content/gravcont_mobilenetv3.mlmodel\")\n"
      ],
      "metadata": {
        "id": "nggE6m1ah8G0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149ba990-a0df-457f-ace7-b2425a963e94"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2611.01 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 38/38 [00:00<00:00, 59.96 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 82.22 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1798.14 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "### Output as CoreML (Tensor type) ###\n",
        "################################\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.TensorType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "    classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(\"/content/gravcont_mobilenetv3.mlmodel\")"
      ],
      "metadata": {
        "id": "ndvCOs4aGQ18",
        "outputId": "a4ef679d-adf7-4799-9993-ecf8bfff7f82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2451.94 ops/s]\n",
            "Running MIL Common passes:   0%|          | 0/38 [00:00<?, ? passes/s]/usr/local/lib/python3.7/dist-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '879', of the source model, has been renamed to 'var_879' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL Common passes: 100%|██████████| 38/38 [00:00<00:00, 60.45 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 85.48 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1717.72 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference on CoreML model**"
      ],
      "metadata": {
        "id": "a8hqka6ery11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This script can be used in Mac only\n",
        "\n",
        "##https://gist.github.com/ozgurshn/85cf74558d82c831827e12f015f752a1\n",
        "##https://github.com/apple/coremltools/blob/master/examples/APIExamples.md\n",
        "import coremltools\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "# load a model whose input type is \"Image\"\n",
        "model = coremltools.models.MLModel('/content/gravcont_mobilenetv3.mlmodel')\n",
        "\n",
        "Height = 224  # use the correct input image height\n",
        "Width = 224  # use the correct input image width\n",
        "\n",
        "\n",
        "# Scenario 1: load an image from disk\n",
        "def load_image(path, resize_to=None):\n",
        "    # resize_to: (Width, Height)\n",
        "    img = PIL.Image.open(path)\n",
        "    if resize_to is not None:\n",
        "        img = img.resize(resize_to, PIL.Image.ANTIALIAS)\n",
        "    img_np = np.array(img).astype(np.float32)\n",
        "    return img_np, img\n",
        "\n",
        "\n",
        "# load the image and resize using PIL utilities\n",
        "_, img = load_image('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_newPatient_250px/スライド1.jpeg', resize_to=(Width, Height))\n",
        "out_dict = model.predict({'image': img})\n",
        "\n",
        "# Scenario 2: load an image from a numpy array\n",
        "shape = (Height, Width, 3)  # height x width x RGB\n",
        "data = np.zeros(shape, dtype=np.uint8)\n",
        "# manipulate numpy data\n",
        "pil_img = PIL.Image.fromarray(data)\n",
        "out_dict = model.predict({'image': pil_img})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "mlesVk5aQln-",
        "outputId": "2fe8cb94-f9c7-457d-e587-080f18fac2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c0ccb8862f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# load the image and resize using PIL utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_newPatient_250px/スライド1.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mout_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Scenario 2: load an image from a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/coremltools/models/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_macos_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 raise Exception(\n\u001b[0;32m--> 524\u001b[0;31m                     \u001b[0;34m\"Model prediction is only supported on macOS version 10.13 or later.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                 )\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Model prediction is only supported on macOS version 10.13 or later."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Interference without Mac**"
      ],
      "metadata": {
        "id": "UMIWSpNBr8A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://tvm.apache.org/docs/how_to/compile_models/from_coreml.html\n",
        "!pip install --quiet apache-tvm\n",
        "!pip install --quiet coremltools\n",
        "import tvm\n",
        "from tvm import te\n",
        "import tvm.relay as relay\n",
        "from tvm.contrib.download import download_testdata\n",
        "import coremltools as cm\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK8mOh84o3Ih",
        "outputId": "aee9507e-41fc-4c37-828d-96b2ee08006c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 42.8 MB 1.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load the model**"
      ],
      "metadata": {
        "id": "5S-9HAe7qjC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://docs-assets.developer.apple.com/coreml/models/MobileNet.mlmodel\"\n",
        "model_file = \"mobilenet.mlmodel\"\n",
        "model_path = download_testdata(model_url, model_file, module=\"coreml\")\n",
        "# Now you have mobilenet.mlmodel on disk\n",
        "mlmodel = cm.models.MLModel(model_path)"
      ],
      "metadata": {
        "id": "1k6jpavuqFHs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load the test image**"
      ],
      "metadata": {
        "id": "hEI_hbRbqJdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\n",
        "img_path = download_testdata(img_url, \"cat.png\", module=\"data\")\n",
        "img = Image.open(img_path).resize((224, 224))\n",
        "# Mobilenet.mlmodel's input is BGR format\n",
        "img_bgr = np.array(img)[:, :, ::-1]\n",
        "x = np.transpose(img_bgr, (2, 0, 1))[np.newaxis, :]"
      ],
      "metadata": {
        "id": "kCriDoPpqfxM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Compile model on relay**"
      ],
      "metadata": {
        "id": "_uTYH_-prJ5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"llvm\"\n",
        "shape_dict = {\"image\": x.shape}\n",
        "\n",
        "# Parse CoreML model and convert into Relay computation graph\n",
        "mod, params = relay.frontend.from_coreml(mlmodel, shape_dict)\n",
        "\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target, params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD584_4lqPGX",
        "outputId": "385a912a-3d2b-44c8-85e7-692141e2d62e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tvm/driver/build_module.py:268: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
            "  \"target_host parameter is going to be deprecated. \"\n",
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute on TVM (これはサンプルの通り)\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "dev = tvm.cpu(0)\n",
        "dtype = \"float32\"\n",
        "m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "# set inputs\n",
        "m.set_input(\"image\", tvm.nd.array(x.astype(dtype)))\n",
        "# execute\n",
        "m.run()\n",
        "# get outputs\n",
        "tvm_output = m.get_output(0)\n",
        "top1 = np.argmax(tvm_output.numpy()[0])"
      ],
      "metadata": {
        "id": "eEVnsfV8rU5_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Look up system name**"
      ],
      "metadata": {
        "id": "10OvE25urm0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synset_url = \"\".join(\n",
        "    [\n",
        "        \"https://gist.githubusercontent.com/zhreshold/\",\n",
        "        \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n",
        "        \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n",
        "        \"imagenet1000_clsid_to_human.txt\",\n",
        "    ]\n",
        ")\n",
        "synset_name = \"imagenet1000_clsid_to_human.txt\"\n",
        "synset_path = download_testdata(synset_url, synset_name, module=\"data\")\n",
        "with open(synset_path) as f:\n",
        "    synset = eval(f.read())\n",
        "# You should see the following result: Top-1 id 282 class name tiger cat\n",
        "print(\"Top-1 id\", top1, \"class name\", synset[top1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNZsGci-rhPY",
        "outputId": "e1ac8a1c-d2bd-497c-a63b-c5d7e2cac521"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 id 282 class name tiger cat\n"
          ]
        }
      ]
    }
  ]
}