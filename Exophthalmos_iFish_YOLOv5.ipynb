{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Exophthalmos_iFish_YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation iFish YOLOv5**\n",
        "\n",
        "Train YOLOv5 using iFish augmentation\n",
        "\n",
        "```\n",
        "iFish: https://github.com/Gil-Mor/iFish.git\n",
        "\n",
        "yolov5_iFish: https://github.com/ykitaguchi77/yolov5-iFish.git\n",
        "â€» FishAugment(distortion_range=(0, 0.3), p=0.5)\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlibã§ç›®ãŒ2ã¤æ¤œå‡ºã•ã‚Œã‚‹ã‚‚ã®ã‚’æŠœãå‡ºã™\n",
        "YOLOv5ã‚’ç”¨ã„ã¦å·¦å³ã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’èªè­˜ã•ã›ã‚‹\n",
        "ãã®ç”»åƒã«é–¢ã—ã¦ã€YOLOv5ã§ãã®ã¾ã¾å›å¸°ã‚’è©¦ã¿ã‚‹\n",
        "ã‚¹ãƒãƒ›ã«å®Ÿè£…\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #ã‚µãƒãƒ¼ãƒˆãƒ‘ãƒƒãƒã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea869e7c-ac0f-419e-c458-0f95ce02eba1"
      },
      "source": [
        "#æ®‹ã‚Šæ™‚é–“ç¢ºèª\n",
        "!cat /proc/uptime | awk '{printf(\"æ®‹ã‚Šæ™‚é–“ : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ®‹ã‚Šæ™‚é–“ : 11.89"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabã‚’ãƒã‚¦ãƒ³ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f87eaf6-2753-4515-a6f9-179fd109d500"
      },
      "source": [
        "'''\n",
        "ãƒ»dlibã‚’ç”¨ã„ã¦ç›®ã‚’åˆ‡ã‚ŠæŠœã\n",
        "ãƒ»æ¨ªå¹…ã‚’2å€ã€ç¸¦å¹…ã‚’ä¸Šã«1å€è¿½åŠ /ä¸‹ã«0.5å€è¿½åŠ ã—ãŸä¸¡çœ¼ã®ç”»åƒãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«åˆ‡ã‚Šå–ã‚‹ï¼ˆç›®ã®å…¨å¹…ã€çœ‰æ¯›ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ï¼‰\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#å…ƒç”»åƒãƒ•ã‚©ãƒ«ãƒ€\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#å…ƒç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#åˆ‡ã‚Šã¬ã„ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Reflesh folder (å†…å®¹ãŒå‰Šé™¤ã•ã‚Œã‚‹ã®ã§æ³¨æ„ï¼ï¼) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\n",
        "# parent_dirãŒã‚ã‚Œã°å‰Šé™¤ã™ã‚‹\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# æ–°ã—ãparent_dirã‚’ä½œæˆã™ã‚‹\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirã‚’æ–°è¦ã«ä½œæˆã™ã‚‹\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "# orig_dirã«dataset_dirç›´ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦ã‚³ãƒ”ãƒ¼ã™ã‚‹\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnV1hEgIu87W",
        "outputId": "e1c7316a-9313-4604-cb54-f5110592343f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1016/1016 [00:42<00:00, 23.86file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeã‚’ç”¨ã„ã¦ç›®ã‚’æ¤œå‡º**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰åˆ†é¡å™¨ã®ç‰¹å¾´é‡å–å¾—\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ç›®ãŒ2ã¤ä»¥ä¸Šæ¤œå‡ºã•ã‚ŒãŸã‚‚ã®ã‚’æŠœãå‡ºã™**\n",
        "\n",
        "dlibã§æ¤œå‡ºã•ã‚ŒãŸã‚‚ã®ã‹ã‚‰ã€ä¸Šä¸‹å·¦å³ã«0.1å€ãšã¤æ‹¡å¤§ã—ãŸç¯„å›²ã‚’æŠœãå‡ºã—ã¦ã„ã‚‹"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #ãƒ•ã‚©ãƒ«ãƒ€æ•°ã®åˆ†ã ã‘\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # ç”»åƒã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«åŒ–\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pixä»¥ä¸Šã®ã‚‚ã®ã§ç›®ã«è¦‹ãˆã‚‹ã‚‚ã®ã‚’æŠ½å‡º\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # çœ¼æ¤œå‡ºåˆ¤å®š\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('ç›®ãŒ' + str(len(eye_list)) +'å€‹æ¤œå‡ºã•ã‚Œã¾ã—ãŸ')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #ç”»åƒã®åˆ‡ã‚ŠæŠœãã¨ä¿å­˜ï¼ˆ2å€‹ä»¥ä¸Šæ¤œå‡ºã®æ™‚ã«é™ã‚‹ï¼‰\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #æœ¬æ¥ã®åˆ‡ã‚ŠæŠœãã‚ˆã‚Šå¹…ã®0.1å€ãšã¤æ°´å¢—ã—ã™ã‚‹\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #æ¨ªå¹…ã®åŠåˆ†ã‚ˆã‚Šå·¦ã«ã‚ã‚‹ã®ã¯å³çœ¼\n",
        "                      else:\n",
        "                          side = \"L\" #æ¨ªå¹…ã®åŠåˆ†ã‚ˆã‚Šã‚ˆã‚Šå³ã«ã‚ã‚‹ã®ã¯å·¦çœ¼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #å¯¾å¿œè¡¨ã®ä½œæˆ\n",
        "                      writer.writerow([id, file_path, side, ex+round(ew/2), ey+round(eh/2), ew, eh])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ã“ã“ã§ã€ç›®ä»¥å¤–ãŒèª¤æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’æ‰‹å‹•ã§æŠœãå‡ºã—ã¦å‰Šé™¤ã™ã‚‹**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csvã‹ã‚‰ã€å‰Šé™¤ã—ã¦ç”»åƒã®ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªããªã£ã¦ã„ã‚‹è¡Œã‚’å‰Šé™¤ã™ã‚‹\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrameã‚’èª­ã¿è¾¼ã‚€\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# å­˜åœ¨ã—ãªã„ç”»åƒãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ãã®ãƒªã‚¹ãƒˆã‚’ä¿æŒã™ã‚‹\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# å­˜åœ¨ã—ãªã„ç”»åƒãƒ‘ã‚¹ã‚’è¡¨ç¤º\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # å­˜åœ¨ã—ãªã„ç”»åƒãƒ‘ã‚¹ã®è¡Œã‚’å‰Šé™¤\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # æ›´æ–°ã•ã‚ŒãŸDataFrameã‚’ä¿å­˜ã™ã‚‹\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "58633999-cc9c-474a-8476-60515172b932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframeã®æ•´ç†**\n",
        "\n",
        "ãƒ» hertel_dfã‚’å‚ç…§ã—ã¦ã€coordinates_dfã«ãƒ˜ãƒ«ãƒ†ãƒ«å€¤ã‚’è¨˜å…¥ã™ã‚‹\n",
        "\n",
        "ãƒ»idãŒ\"16_R, 16_L\"ã¨ã„ã†å½¢å¼ã«ãªã‚‹ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´ç†ã™ã‚‹"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the coordinate_uni_for_YOLO5.csv into a DataFrame\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# Load the Hertel.csv into a DataFrame\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "# Show the first few rows of each DataFrame to understand their structure\n",
        "coordinates_df_head = coordinates_df.head()\n",
        "hertel_df_head = hertel_df.head()\n",
        "\n",
        "\n",
        "# Add a new column 'Hertel' to the coordinates DataFrame\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "# Define a function to look up the Hertel value based on 'id' and 'side R/L'\n",
        "def get_hertel_value(row):\n",
        "    # Extract the id and side\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "\n",
        "    # Find the corresponding Hertel value in the Hertel DataFrame\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "\n",
        "    # Return the Hertel value if it exists, otherwise return None\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Apply the function to each row in the coordinates DataFrame\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(get_hertel_value, axis=1)\n",
        "\n",
        "\n",
        "# Create the new id by combining 'id' and 'side R/L'\n",
        "coordinates_df['new_id'] = coordinates_df['id'].astype(str) + '_' + coordinates_df['side R/L']\n",
        "\n",
        "# Select the required columns for the new dataframe\n",
        "new_columns = ['new_id', 'img_path', 'ex', 'ey', 'ew', 'Hertel']\n",
        "new_df = coordinates_df[new_columns].copy()\n",
        "new_df.rename(columns={'new_id': 'id'}, inplace=True)\n",
        "\n",
        "new_df.head()  # Display the first few rows of the new dataframe\n",
        "\n",
        "new_df.to_csv(csv_integrated_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7pagrZ0-u9D8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2wO-hK7u9GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26f9JENWu9IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_images: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images'))}\")\n",
        "print(f\"val_images: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images'))}\")\n",
        "print(f\"train_labels: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels'))}\")\n",
        "print(f\"val_labels: {len(os.listdir('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels'))}\")\n",
        "\n",
        "def check_basename(image_dir, label_dir):\n",
        "    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ä¸€è¦§ã‚’å–å¾—\n",
        "    image_files = [os.path.splitext(file)[0] for file in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, file))]\n",
        "    image_files = set(image_files)  # é‡è¤‡ã‚’å‰Šé™¤\n",
        "\n",
        "    # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ä¸€è¦§ã‚’å–å¾—\n",
        "    label_files = [os.path.splitext(file)[0] for file in os.listdir(label_dir) if os.path.isfile(os.path.join(label_dir, file))]\n",
        "    label_files = set(label_files)  # é‡è¤‡ã‚’å‰Šé™¤\n",
        "\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª\n",
        "    match = image_files == label_files\n",
        "\n",
        "    if match:\n",
        "        print(\"ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ãŒä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\"ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ãŒä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "check_basename('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images', '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels')\n",
        "check_basename('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images', '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8WcREJmtMLv",
        "outputId": "de7bf91f-f32c-4baf-e4ac-8ed64d245062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images: 3498\n",
            "val_images: 795\n",
            "train_labels: 3498\n",
            "val_labels: 795\n",
            "ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ãŒä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚\n",
            "ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ãŒä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels'\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n",
        "files = os.listdir(directory)\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è¡¨ç¤º\n",
        "for file in files:\n",
        "    if file.endswith('.txt'):\n",
        "        file_path = os.path.join(directory, file)\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read().strip()\n",
        "            if content:\n",
        "                print(f\"ãƒ•ã‚¡ã‚¤ãƒ«å: {file}\\nå†…å®¹: {content}\\n\")\n",
        "            else:\n",
        "                print(f\"ãƒ•ã‚¡ã‚¤ãƒ«å: {file}\\nã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒç©ºæ¬„ã§ã™ã€‚\\n\")\n",
        "                sys.exit(1)  # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä¸­æ­¢\n"
      ],
      "metadata": {
        "id": "E-NkFCNSwQLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Albumentationã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**\n",
        "\n",
        "(YOLOv5-iFish https://github.com/ykitaguchi77/yolov5-iFish.git ã«çµ„ã¿è¾¼ã¿æ¸ˆã¿)"
      ],
      "metadata": {
        "id": "ORyTzZcA7sPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from albumentations import DualTransform\n",
        "from math import sqrt\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "class FishEyeEffect(DualTransform):\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(FishEyeEffect, self).__init__(always_apply, p)\n",
        "        self.image_shape = None  # Initialize the instance variable to store image shape\n",
        "\n",
        "\n",
        "    def get_fish_xn_yn(self, source_x, source_y, radius, distortion):\n",
        "        if 1 - distortion*(radius**2) == 0:\n",
        "            return source_x, source_y\n",
        "        return source_x / (1 - (distortion*(radius**2))), source_y / (1 - (distortion*(radius**2)))\n",
        "\n",
        "    def get_original_xn_yn(self, dest_x, dest_y, radius, distortion):\n",
        "        return dest_x * (1 - distortion * (radius**2)), dest_y * (1 - distortion * (radius**2))\n",
        "\n",
        "\n",
        "    def apply(self, img, distortion=0, **params):\n",
        "        self.image_shape = img.shape  # set image shape here\n",
        "        # your fish-eye effect implementation here\n",
        "        dstimg = np.zeros_like(img)\n",
        "        w, h = float(img.shape[0]), float(img.shape[1])\n",
        "        for x in range(len(dstimg)):\n",
        "            for y in range(len(dstimg[x])):\n",
        "                xnd, ynd = float((2*x - w)/w), float((2*y - h)/h)\n",
        "                rd = sqrt(xnd**2 + ynd**2)\n",
        "                xdu, ydu = self.get_fish_xn_yn(xnd, ynd, rd, distortion)\n",
        "                xu, yu = int(((xdu + 1)*w)/2), int(((ydu + 1)*h)/2)\n",
        "                if 0 <= xu < img.shape[0] and 0 <= yu < img.shape[1]:\n",
        "                    dstimg[x][y] = img[xu][yu]\n",
        "        return dstimg\n",
        "\n",
        "    def apply_to_bbox(self, bbox, distortion=0, **params):\n",
        "        img_h, img_w = self.image_shape[:2]  # Use the stored image shape\n",
        "\n",
        "        # YOLOå½¢å¼ã‹ã‚‰4ã¤ã®è§’ã®åº§æ¨™ã‚’å–å¾—\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "\n",
        "        # 4ã¤ã®è§’ã®åº§æ¨™ã‚’æ­£è¦åŒ–\n",
        "        corners = [(x_min, y_min),\n",
        "                  (x_max, y_min),\n",
        "                  (x_min, y_max),\n",
        "                  (x_max, y_max)]\n",
        "\n",
        "        # æ­£è¦åŒ–ã•ã‚ŒãŸåº§æ¨™ã‚’é­šçœ¼å¤‰æ›ç”¨ã«å¤‰æ›\n",
        "        corners = [(2*x - 1, 2*y - 1) for x, y in corners]\n",
        "\n",
        "        print(\"Transformed corners:\", corners)\n",
        "\n",
        "        # å„è§’ã‚’FishEyeã§é€†å¤‰æ›\n",
        "        distorted_corners = [self.get_original_xn_yn(x, y, sqrt(x**2 + y**2), distortion) for x, y in corners]\n",
        "\n",
        "        print(\"Distorted corners:\", distorted_corners)\n",
        "\n",
        "        # é€†å¤‰æ›ã•ã‚ŒãŸç‚¹ã‚’å«ã‚€æœ€å°ã®é•·æ–¹å½¢ã‚’æ±‚ã‚ã‚‹\n",
        "        x_min_new = min(x for x, y in distorted_corners)\n",
        "        x_max_new = max(x for x, y in distorted_corners)\n",
        "        y_min_new = min(y for x, y in distorted_corners)\n",
        "        y_max_new = max(y for x, y in distorted_corners)\n",
        "\n",
        "        # é­šçœ¼é€†å¤‰æ›å¾Œã®åº§æ¨™ã‚’ç”»åƒåº§æ¨™ç³»ã«æˆ»ã™\n",
        "        x_min_new = (x_min_new + 1) / 2\n",
        "        x_max_new = (x_max_new + 1) / 2\n",
        "        y_min_new = (y_min_new + 1)  / 2\n",
        "        y_max_new = (y_max_new + 1)  / 2\n",
        "\n",
        "        print(x_min_new, y_min_new, x_max_new, y_max_new)\n",
        "\n",
        "        return x_min_new, y_min_new, x_max_new, y_max_new\n",
        "\n",
        "\n",
        "    def get_params(self):\n",
        "        # Random distortion coefficient between 0 and 0.3\n",
        "        return {'distortion': random.uniform(0, 0.3)}\n",
        "\n",
        "\n",
        "# å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’å®šç¾©\n",
        "transform = A.Compose([\n",
        "    #A.RandomCrop(width=400, height=400),\n",
        "    FishEyeEffect(p=1),\n",
        "    #A.HorizontalFlip(p=1),\n",
        "    #A.VerticalFlip(p=1),\n",
        "    A.RandomBrightnessContrast(p=1),\n",
        "], bbox_params=A.BboxParams(format='yolo',min_area=1024,\n",
        "min_visibility=0.1, label_fields=['class_labels']))\n",
        "\n",
        "\n",
        "\n",
        "# ç”»åƒèª­ã¿è¾¼ã¿\n",
        "image = cv2.imread(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/images/1043.jpg\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# yoloã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
        "f = open('/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/train/labels/1043.txt', 'r')\n",
        "anno_lists = f.readlines()\n",
        "f.close()\n",
        "\n",
        "# ç©ºã®é…åˆ—ã‚’ç”¨æ„\n",
        "bboxes = []\n",
        "class_labels = []\n",
        "\n",
        "# ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ä¸ãˆã‚‹å½¢ã«å¤‰æ›\n",
        "for anno_list in anno_lists:\n",
        "    # ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‹\n",
        "    anno_list = anno_list.split()\n",
        "\n",
        "    # 0ç•ªç›®ã®è¦ç´ ã‹ã‚‰ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’å–ã‚Šå‡ºã—\n",
        "    class_labels.append(anno_list[0])\n",
        "\n",
        "    # BBoxã®å€¤ã‚’floatã«å¤‰æ›\n",
        "    anno_list = [float(i) for i in anno_list[1:]]\n",
        "\n",
        "    # BBoxã®é…åˆ—ã«è¿½åŠ \n",
        "    bboxes.append(anno_list)\n",
        "\n",
        "\n",
        "# å¤‰æ›ã‚’å®Ÿè¡Œã—ã€å¤‰æ›å¾Œã®image, bboxes, class_labelsã‚’å–å¾—\n",
        "transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "transformed_image = transformed['image']\n",
        "transformed_bboxes = transformed['bboxes']\n",
        "transformed_class_labels = transformed['class_labels']\n",
        "\n",
        "\n",
        "\n",
        "# é–¢æ•°ï¼šYOLOå½¢å¼ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’matplotlibã®rectangleã«å¤‰æ›\n",
        "def yolo_to_rect(bbox, img_shape):\n",
        "    cx, cy, w, h = bbox\n",
        "    img_h, img_w = img_shape[:2]\n",
        "    tlx = int((cx - w/2) * img_w)\n",
        "    tly = int((cy - h/2) * img_h)\n",
        "    rect_w = int(w * img_w)\n",
        "    rect_h = int(h * img_h)\n",
        "    return tlx, tly, rect_w, rect_h\n",
        "\n",
        "# å…ƒã®ç”»åƒã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "for bbox in bboxes:\n",
        "    tlx, tly, rect_w, rect_h = yolo_to_rect(bbox, image.shape)\n",
        "    rect = plt.Rectangle((tlx, tly), rect_w, rect_h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "plt.title('Before Augmentation')\n",
        "\n",
        "# å¤‰æ›å¾Œã®ç”»åƒã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(transformed_image)\n",
        "plt.axis('off')\n",
        "for bbox in transformed_bboxes:\n",
        "    tlx, tly, rect_w, rect_h = yolo_to_rect(bbox, transformed_image.shape)\n",
        "    rect = plt.Rectangle((tlx, tly), rect_w, rect_h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "plt.title('After Augmentation')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # To save the transformed annotations:\n",
        "# with open('result.txt', 'w') as f:\n",
        "#     for i, transformed_bbox in enumerate(transformed_bboxes):\n",
        "#         f.write(\"{} {} {} {} {}\\n\".format(\n",
        "#             transformed_class_labels[i],\n",
        "#             transformed_bbox[0], transformed_bbox[1],\n",
        "#             transformed_bbox[2], transformed_bbox[3]\n",
        "#         ))\n",
        "\n"
      ],
      "metadata": {
        "id": "TfgxSSHW70zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training\n",
        "!git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationã‚’å®Ÿè£…ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³\n",
        "%cd yolov5-iFish\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "ZjV_xXLpd5__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é€”ä¸­ã‹ã‚‰\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/dataset.yaml --resume /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/last.pt\n"
      ],
      "metadata": {
        "id": "5126RlFFfXWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_t1_bMiSeo09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyã‚’renameã—ã¦gdriveã«ç§»å‹•ã—ã¦ãŠã\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "outputId": "8526b8d8-09fd-48a3-a372-dcf2f602c40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCJ9m4Swo0Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KrBGgvJ_AvBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRe67Lg1AvD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKOIspiWAvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxDheZVGAvJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7xRpeWqObKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0UeNKxikObNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Show examples**"
      ],
      "metadata": {
        "id": "MjJoQlzAOTS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "val_images_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/images\"\n",
        "val_labels_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/valid/labels\"\n",
        "\n",
        "for sample_num in range(21):\n",
        "    sample_image_path = glob.glob(f\"{val_images_dir}/*\")[sample_num]\n",
        "    sample_label_path = glob.glob(f\"{val_labels_dir}/*\")[sample_num]\n",
        "\n",
        "    # ç”»åƒã®èª­ã¿è¾¼ã¿\n",
        "    image = cv2.imread(sample_image_path)\n",
        "    height, width = image.shape[:2]\n",
        "    image = cv2.resize(image, (640, int(height * 640 / width)))\n",
        "\n",
        "    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®æƒ…å ±\n",
        "    with open(sample_label_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "    content_list = content.split()\n",
        "\n",
        "    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®åº§æ¨™ã‚’è¨ˆç®—\n",
        "    x = float(content_list[1]) * image.shape[1]\n",
        "    y = float(content_list[2]) * image.shape[0]\n",
        "    box_width = float(content_list[3]) * image.shape[1]\n",
        "    box_height = float(content_list[4]) * image.shape[0]\n",
        "\n",
        "    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®åº§æ¨™ã‚’è¨ˆç®—\n",
        "    left = int(x - (box_width / 2))\n",
        "    top = int(y - (box_height / 2))\n",
        "    right = int(x + (box_width / 2))\n",
        "    bottom = int(y + (box_height / 2))\n",
        "\n",
        "    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ç”»åƒã«é‡ã­ã‚‹\n",
        "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
        "\n",
        "    # ç”»åƒã®è¡¨ç¤º\n",
        "    cv2_imshow(image)\n"
      ],
      "metadata": {
        "id": "iRp4OAwxOXIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419d4427-d9f6-43da-ecde-8cbef11e4f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ğŸš€ 2269e2e Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (8 CPUs, 51.0 GB RAM, 26.5/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5n_130epch.pt\"\n",
        "\n",
        "# æ¨ªå¹…ã‚’640pxã«ãƒªã‚µã‚¤ã‚ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "dataset_olympia_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "dataset_olympia_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "dataset_handai_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/grav\"\n",
        "dataset_handai_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/cont\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#ã‚µãƒãƒ¼ãƒˆãƒ‘ãƒƒãƒã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2ã§é–‹ã\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ä¸Šä¸‹padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # ãƒãƒƒãƒå¯¾å¿œ\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "\n",
        "image_path = glob.glob(f\"{dataset_olympia_grav}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"cont\", 1: \"grav\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"è¨ºæ–­ã¯ %sã€ç¢ºç‡ã¯%.1fï¼…ã§ã™ã€‚\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # æ¨ªå¹…ã‚’640pxã«ãƒªã‚µã‚¤ã‚º\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640ã«paddingã•ã‚Œã¦ã„ã‚‹åˆ†ã®åº§æ¨™ã‚’è¶³ã™)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testsetã®æ­£è§£ç‡ã‚’ç¢ºèª\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "class_names = {0: \"cont\", 1: \"grav\"}\n",
        "\n",
        "\n",
        "\n",
        "def interference_testset(image_path, answer):\n",
        "    pred_list = []\n",
        "    correct_list = [] #æ­£è§£ãªã‚‰1ã€ä¸æ­£è§£ãªã‚‰0\n",
        "\n",
        "    for img in image_path:\n",
        "        pred = interference(img, weight)\n",
        "\n",
        "        # output result\n",
        "        x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "        # probability\n",
        "        prob = pred[0][0][4].item()\n",
        "\n",
        "        # class\n",
        "        class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "        if class_name == answer:\n",
        "            correct_list.append(1)\n",
        "        else:\n",
        "            correct_list.append(0)\n",
        "\n",
        "        print(\"è¨ºæ–­ã¯ %sã€ç¢ºç‡ã¯%.1fï¼…ã§ã™ã€‚\" % (class_name, prob * 100))\n",
        "\n",
        "    total_count = len(correct_list)\n",
        "    one_count = correct_list.count(1)\n",
        "    percentage = (one_count / total_count) * 100\n",
        "    print(\"\")\n",
        "    print(f\"image_path: {image_path}\")\n",
        "    print(f\"The percentage of corrects in the list is: {percentage:.2f}%\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "interference_testset(glob.glob(f\"{dataset_olympia_grav}/*\"), \"grav\")\n",
        "interference_testset(glob.glob(f\"{dataset_olympia_cont}/*\"), \"cont\")\n",
        "interference_testset(glob.glob(f\"{dataset_handai_grav}/*\"), \"grav\")\n",
        "interference_testset(glob.glob(f\"{dataset_handai_cont}/*\"), \"cont\")\n"
      ],
      "metadata": {
        "id": "4iGCwXZrAcRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5XdJerf7Zxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpdE1Mov7Zzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\""
      ],
      "metadata": {
        "id": "vwbKpW0n2ZOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python export.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt --nms --include coreml\n",
        "!python export.py --weights $weight_path --nms --include \"coreml\"\n"
      ],
      "metadata": {
        "id": "Nnxz6A9pyhvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ğŸš€ v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ‘ã‚¹ã‚’æŒ‡å®šã™ã‚‹\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#ã‚µãƒãƒ¼ãƒˆãƒ‘ãƒƒãƒã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2ã§é–‹ã\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ä¸Šä¸‹padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # ãƒãƒƒãƒå¯¾å¿œ\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"è¨ºæ–­ã¯ %sã€ç¢ºç‡ã¯%.1fï¼…ã§ã™ã€‚\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640ã«paddingã•ã‚Œã¦ã„ã‚‹åˆ†ã®åº§æ¨™ã‚’è¶³ã™)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã§ç”»åƒã‚’åˆ‡ã‚ŠæŠœãã€\n",
        "\n",
        "    if x1 < 0: #è² ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼å›é¿\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # åˆ‡ã‚ŠæŠœã„ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcampç”¨csvã®image_pathã‚’æ”¹å¤‰)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxnOT6leGS9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-EigenCAM**\n",
        "\n",
        "https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
      ],
      "metadata": {
        "id": "YXt1zi1TGm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "DM7nTgGnjwYO",
        "outputId": "bc8c9e81-6dbc-4ec4-c0eb-c4f9edc3a822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'pytorch-grad-cam'...\n",
            "remote: Enumerating objects: 1122, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 1122 (delta 8), reused 9 (delta 3), pack-reused 1097\u001b[K\n",
            "Receiving objects: 100% (1122/1122), 110.21 MiB | 15.74 MiB/s, done.\n",
            "Resolving deltas: 100% (617/617), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "0xDMsB0GkMyg",
        "outputId": "239921df-d7d6-45a2-dfb8-89fda0fb434b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-grad-cam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "FzxYfIoemXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "id": "DKEgZ-Laojx8",
        "outputId": "872a8e02-519e-4a1d-cf17-45e907ac1b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from PIL import Image\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "def parse_detections(results):\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    detections = detections.to_dict()\n",
        "    boxes, colors, names = [], [], []\n",
        "\n",
        "    for i in range(len(detections[\"xmin\"])):\n",
        "        confidence = detections[\"confidence\"][i]\n",
        "        if confidence < 0.2:\n",
        "            continue\n",
        "        xmin = int(detections[\"xmin\"][i])\n",
        "        ymin = int(detections[\"ymin\"][i])\n",
        "        xmax = int(detections[\"xmax\"][i])\n",
        "        ymax = int(detections[\"ymax\"][i])\n",
        "        name = detections[\"name\"][i]\n",
        "        category = int(detections[\"class\"][i])\n",
        "        color = COLORS[category]\n",
        "\n",
        "        boxes.append((xmin, ymin, xmax, ymax))\n",
        "        colors.append(color)\n",
        "        names.append(name)\n",
        "    return boxes, colors, names\n",
        "\n",
        "\n",
        "def draw_detections(boxes, colors, names, img):\n",
        "    for box, color, name in zip(boxes, colors, names):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (xmin, ymin),\n",
        "            (xmax, ymax),\n",
        "            color,\n",
        "            2)\n",
        "\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n",
        "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
        "img = cv2.resize(img, (640, 640))\n",
        "rgb_img = img.copy()\n",
        "img = np.float32(img) / 255\n",
        "transform = transforms.ToTensor()\n",
        "tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "target_layers = [model.model.model.model[-2]]\n",
        "\n",
        "results = model([rgb_img])\n",
        "boxes, colors, names = parse_detections(results)\n",
        "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
        "Image.fromarray(detections)"
      ],
      "metadata": {
        "id": "i_WLp1kGjvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ»å¤–éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆTreatedï¼‰ã‚’æ´—ã„å‡ºã—\n",
        "\n",
        "ãƒ»å†…éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã•ã‚‰ã«æ°´å¢—ã—\n",
        "\n",
        "ãƒ»å†…éƒ¨ãŠã‚ˆã³å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ˆã‚Šã€testç”¨å„100æšï¼ˆgrav50æšã€cont50æšï¼‰ã‚’æŠœãå‡ºã—ã¦ãŠãã€åˆä½“ã™ã‚‹\n",
        "\n",
        "ãƒ»æ—¢å­˜ã®YOLOv5ã‚’ç”¨ã„ã¦bounding boxã‚’æŠœãå‡ºã—ã€æ–°ãŸã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}