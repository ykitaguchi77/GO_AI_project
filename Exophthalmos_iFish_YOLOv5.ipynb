{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GO_AI_project/blob/main/Exophthalmos_iFish_YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation iFish YOLOv5**\n",
        "\n",
        "Train YOLOv5 using iFish augmentation\n",
        "\n",
        "```\n",
        "iFish: https://github.com/Gil-Mor/iFish.git\n",
        "\n",
        "yolov5_iFish: https://github.com/ykitaguchi77/yolov5-iFish.git\n",
        "※ FishAugment(distortion_range=(0, 0.3), p=0.5)\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "Olympia dataset\n",
        "Dlibで目が2つ検出されるものを抜き出す\n",
        "YOLOv5を用いて左右とバウンディングボックスを認識させる\n",
        "その画像に関して、YOLOv5でそのまま回帰を試みる\n",
        "スマホに実装\n",
        "```\n",
        "\n",
        "Output as CoreML"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea869e7c-ac0f-419e-c458-0f95ce02eba1"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.89"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0576c52-b2ef-4930-e578-42026c835be8"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#親フォルダ\n",
        "parent_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5'\n",
        "\n",
        "#元画像フォルダ\n",
        "dataset_dir = '/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset'\n",
        "\n",
        "#元画像をコピー\n",
        "orig_dir = f\"{parent_dir}/dataset_orig\"\n",
        "\n",
        "#切りぬいた画像を保存するフォルダ\n",
        "out_dir = f\"{parent_dir}/dataset_uni\"\n",
        "\n",
        "#トレーニングされたYOLOv5で切り抜いた画像を保存するフォルダ\n",
        "cropped_dir = f\"{parent_dir}/dataset_yolo_cropped\"\n",
        "\n",
        "#CSVファイルのフォルダ\n",
        "csv_hertel_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel.csv\"\n",
        "csv_coordinate_path = f\"{parent_dir}/coordinate_uni_for_YOLO5.csv\"\n",
        "csv_integrated_path = f\"{parent_dir}/integrated_uni_for_YOLO5.csv\""
      ],
      "metadata": {
        "id": "lfBOBJ8Su85t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###################################\n",
        "# Reflesh folder (内容が削除されるので注意！！) #\n",
        "###################################\n",
        "\"\"\"\n",
        "\n",
        "# parent_dirがあれば削除する\n",
        "if os.path.exists(parent_dir):\n",
        "    shutil.rmtree(parent_dir)\n",
        "\n",
        "# 新しくparent_dirを作成する\n",
        "os.makedirs(parent_dir)\n",
        "\n",
        "# orig_dir, out_dirを新規に作成する\n",
        "os.makedirs(orig_dir)\n",
        "os.makedirs(out_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "\n",
        "# orig_dirにdataset_dir直下のファイルをすべてコピーする\n",
        "file_list = os.listdir(dataset_dir)\n",
        "for filename in tqdm(file_list, desc=\"Copying files\", unit=\"file\"):\n",
        "    src_path = os.path.join(dataset_dir, filename)\n",
        "    dst_path = os.path.join(orig_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"処理が完了しました。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnV1hEgIu87W",
        "outputId": "abe9b82a-2d69-424b-d59a-a7ce8d5e3526"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 100%|██████████| 1016/1016 [00:12<00:00, 83.69file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "処理が完了しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HaarCascadeを用いて目を検出**"
      ],
      "metadata": {
        "id": "o8CwyG8Wv_NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "metadata": {
        "id": "wtgU9Nb2u89i"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**目が2つ以上検出されたものを抜き出す**\n",
        "\n",
        "dlibで検出されたものから、上下左右に0.1倍ずつ拡大した範囲を抜き出している"
      ],
      "metadata": {
        "id": "bGRyj1BjwDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(csv_coordinate_path, 'w', newline='') as f:\n",
        "        #fieldnames = ['Number', 'Folder', 'FileName']\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id','img_path', 'side R/L', 'ex', 'ey', 'ew', 'eh'])  #header\n",
        "\n",
        "        files = os.listdir(orig_dir)\n",
        "\n",
        "        k=0\n",
        "        for file in files:  #フォルダ数の分だけ\n",
        "              file_path = f\"{orig_dir}/{file}\"\n",
        "              id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "              img = cv2.imread(file_path)\n",
        "              img2 = img.copy()\n",
        "\n",
        "              # 画像グレースケール化\n",
        "              grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "              #300pix以上のもので目に見えるものを抽出\n",
        "              eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "\n",
        "              # 眼検出判定\n",
        "              if len(eye_list) >= 1:\n",
        "                  print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "              else:\n",
        "                  print(\"eye detection error\")\n",
        "\n",
        "              #画像の切り抜きと保存（2個以上検出の時に限る）\n",
        "              if len(eye_list) >= 2:\n",
        "                  for (ex, ey, ew, eh) in eye_list:\n",
        "                      print(f\"img_width: {img2.shape[1]}\")\n",
        "                      print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "                      cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "                      img_cropped = img[int(ey-0.1*eh): int(ey+1.1*eh), int(ex-0.1*ew): int(ex+1.1*ew)] #本来の切り抜きより幅の0.1倍ずつ水増しする\n",
        "                      #img_cropped = img[int(ey): int(ey+eh), int(ex): int(ex+ew)]\n",
        "\n",
        "\n",
        "                      if ex+eh*0.5 <= img2.shape[1]/2:\n",
        "                          side = \"R\" #横幅の半分より左にあるのは右眼\n",
        "                      else:\n",
        "                          side = \"L\" #横幅の半分よりより右にあるのは左眼\n",
        "\n",
        "                      print(f\"side: {side}\")\n",
        "                      print(\"\")\n",
        "\n",
        "                      # Check if coordinates are within the image bounds\n",
        "                      ey_start = max(int(ey - 0.1 * eh), 0)\n",
        "                      ey_end = min(int(ey + 1.1 * eh), img.shape[0])\n",
        "                      ex_start = max(int(ex - 0.1 * ew), 0)\n",
        "                      ex_end = min(int(ex + 1.1 * ew), img.shape[1])\n",
        "\n",
        "                      # Ensure we have a valid crop area\n",
        "                      if ex_start < ex_end and ey_start < ey_end:\n",
        "                          img_cropped = img[ey_start: ey_end, ex_start: ex_end]\n",
        "\n",
        "                          # Now do the checks for the right/left side, write image and row\n",
        "                          # ...\n",
        "\n",
        "                          cv2.imwrite(f\"{out_dir}/{id}_{side}.png\", img_cropped)\n",
        "\n",
        "                      #対応表の作成\n",
        "                      writer.writerow([id, file_path, side, ex-round(ew*0.1), ey-round(eh*0.1), round(ew*1.2), round(eh*1.2)])\n",
        "\n",
        "                      #cv2_imshow(img_cropped)\n",
        "                  else:\n",
        "                      pass\n"
      ],
      "metadata": {
        "id": "n-Hxkynvu8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ここで、目以外が誤検出されているものを手動で抜き出して削除する**\n",
        "\n",
        "```\n",
        "coordinate_uni_for_YOLO5.csvから、削除して画像のパスが存在しなくなっている行を削除する\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u_Aqt0ByQNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrameを読み込む\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "\n",
        "# 存在しない画像パスをチェックし、そのリストを保持する\n",
        "nonexistent_paths = coordinates_df[~coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# 存在しない画像パスを表示\n",
        "print(\"Nonexistent image paths:\")\n",
        "print(nonexistent_paths['img_path'])\n",
        "\n",
        "# # 存在しない画像パスの行を削除\n",
        "# coordinates_df = coordinates_df[coordinates_df['img_path'].apply(os.path.exists)]\n",
        "\n",
        "# # 更新されたDataFrameを保存する\n",
        "# coordinates_df.to_csv('coordinate_uni_for_YOLO5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "A33onv-mQZ10",
        "outputId": "0dd85e94-04a3-49bb-f29a-597ea3ce525b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nonexistent image paths:\n",
            "Series([], Name: img_path, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataframeの整理**\n",
        "\n",
        "・ hertel_dfを参照して、coordinates_dfにヘルテル値を記入する\n",
        "\n",
        "・idが\"16_R, 16_L\"という形式になるようにデータフレームを整理する"
      ],
      "metadata": {
        "id": "G2UKV8PTS2Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_coordinate_path and csv_hertel_path are defined paths to the CSV files\n",
        "coordinates_df = pd.read_csv(csv_coordinate_path)\n",
        "hertel_df = pd.read_csv(csv_hertel_path)\n",
        "\n",
        "coordinates_df['Hertel'] = None\n",
        "\n",
        "def get_hertel_value(row, hertel_df):\n",
        "    id = row['id']\n",
        "    side = row['side R/L']\n",
        "    hertel_value = hertel_df.loc[hertel_df['number'] == id, side].values\n",
        "    return hertel_value[0] if len(hertel_value) > 0 else None\n",
        "\n",
        "# Use .copy() to ensure that you're working with a copy and not a view\n",
        "coordinates_df['Hertel'] = coordinates_df.apply(lambda row: get_hertel_value(row, hertel_df), axis=1)\n",
        "\n",
        "id_counts = coordinates_df.groupby('id')['side R/L'].value_counts().unstack()\n",
        "valid_ids = id_counts[(id_counts['R'] == 1) & (id_counts['L'] == 1)].index\n",
        "\n",
        "# Filter the DataFrame to only include these ids\n",
        "# Use .copy() to avoid SettingWithCopyWarning when modifying this DataFrame later\n",
        "coordinates_filtered_df = coordinates_df[coordinates_df['id'].isin(valid_ids)].copy()\n",
        "coordinates_filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "coordinates_filtered_df.to_csv(csv_integrated_path, index=False)\n",
        "coordinates_filtered_df.head()"
      ],
      "metadata": {
        "id": "C2wO-hK7u9GI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "af59f4c1-19cc-45be-8706-73637c192c1f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           img_path side R/L    ex   ey  \\\n",
              "0  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   107  557   \n",
              "1  19  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1513  547   \n",
              "2  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        R    74  483   \n",
              "3  20  /content/drive/MyDrive/Deep_learning/Olympia_d...        L  1488  486   \n",
              "4  21  /content/drive/MyDrive/Deep_learning/Olympia_d...        R   147  555   \n",
              "\n",
              "    ew   eh  Hertel  \n",
              "0  835  835    15.0  \n",
              "1  850  850    16.0  \n",
              "2  942  942    18.0  \n",
              "3  978  978    18.0  \n",
              "4  868  868    19.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a03c0c0-42f0-43aa-84b8-94338e24855d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_path</th>\n",
              "      <th>side R/L</th>\n",
              "      <th>ex</th>\n",
              "      <th>ey</th>\n",
              "      <th>ew</th>\n",
              "      <th>eh</th>\n",
              "      <th>Hertel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>107</td>\n",
              "      <td>557</td>\n",
              "      <td>835</td>\n",
              "      <td>835</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1513</td>\n",
              "      <td>547</td>\n",
              "      <td>850</td>\n",
              "      <td>850</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>74</td>\n",
              "      <td>483</td>\n",
              "      <td>942</td>\n",
              "      <td>942</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>L</td>\n",
              "      <td>1488</td>\n",
              "      <td>486</td>\n",
              "      <td>978</td>\n",
              "      <td>978</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/Olympia_d...</td>\n",
              "      <td>R</td>\n",
              "      <td>147</td>\n",
              "      <td>555</td>\n",
              "      <td>868</td>\n",
              "      <td>868</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a03c0c0-42f0-43aa-84b8-94338e24855d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a03c0c0-42f0-43aa-84b8-94338e24855d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a03c0c0-42f0-43aa-84b8-94338e24855d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b474e6ca-d1f0-4cea-a4ef-7d4332eae111 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "画像パスの抽出（RLともに揃っているもの）\n",
        "'''\n",
        "coordinates_filtered_df = coordinates_filtered_df.drop_duplicates(subset='id', keep='first')\n",
        "img_path_list = coordinates_filtered_df['img_path'].tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "画像の分割 train:valid = 8:2\n",
        "'''\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(img_path_list)\n",
        "\n",
        "split_index = int(0.8 * len(img_path_list))\n",
        "train_img_paths = img_path_list[:split_index]\n",
        "valid_img_paths = img_path_list[split_index:]\n",
        "\n",
        "if os.path.exists(out_dir):\n",
        "    shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir)\n",
        "\n",
        "'''\n",
        "フォルダの作成\n",
        "'''\n",
        "folders = ['train/images', 'train/labels', 'valid/images', 'valid/labels']\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(out_dir, folder))\n",
        "\n",
        "\n",
        "'''\n",
        "画像のコピー\n",
        "'''\n",
        "# Define paths for images directories\n",
        "train_images_dir = os.path.join(out_dir, 'train/images')\n",
        "valid_images_dir = os.path.join(out_dir, 'valid/images')\n",
        "\n",
        "# Copy training images\n",
        "for img_path in tqdm(train_img_paths, desc='Copying train images'):\n",
        "    shutil.copy(img_path, train_images_dir)\n",
        "\n",
        "# Copy validation images\n",
        "for img_path in tqdm(valid_img_paths, desc='Copying valid images'):\n",
        "    shutil.copy(img_path, valid_images_dir)"
      ],
      "metadata": {
        "id": "26f9JENWu9IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7d2f38-40d0-4dcd-db66-bbb1b6525673"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train images: 100%|██████████| 760/760 [00:07<00:00, 95.80it/s]\n",
            "Copying valid images: 100%|██████████| 190/190 [00:02<00:00, 72.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.width, img.height\n",
        "\n",
        "def convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height):\n",
        "    cx = (ex + (ew / 2)) / img_width\n",
        "    cy = (ey + (eh / 2)) / img_height\n",
        "    w = ew / img_width\n",
        "    h = eh / img_height\n",
        "    return cx, cy, w, h\n",
        "\n",
        "def create_label_files(image_dir, label_dir, df):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        matched_rows = df[df['id'] == int(base_name)]\n",
        "\n",
        "        if matched_rows.empty:\n",
        "            raise ValueError(f\"No matching id found for image {image_file}\")\n",
        "\n",
        "        label_file_path = os.path.join(label_dir, f\"{base_name}.txt\")\n",
        "        with open(label_file_path, 'w') as label_file:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                ex = row['ex']\n",
        "                ey = row['ey']\n",
        "                ew = row['ew']\n",
        "                eh = row['eh']\n",
        "                cx, cy, w, h = convert_to_yolo_format(ex, ey, ew, eh, img_width, img_height)\n",
        "                side = 0 if row['side R/L'] == 'R' else 1\n",
        "                label_file.write(f\"{cx} {cy} {w} {h} {side}\\n\")\n",
        "                #label_file.write(f\"{ex} {ey} {ew} {eh} {side}\\n\")\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "csv_integrated_df = pd.read_csv(csv_integrated_path)\n",
        "\n",
        "# trainとvalidのディレクトリパス\n",
        "train_images_dir = os.path.join(out_dir, \"train/images\")\n",
        "train_labels_dir = os.path.join(out_dir, \"train/labels\")\n",
        "valid_images_dir = os.path.join(out_dir, \"valid/images\")\n",
        "valid_labels_dir = os.path.join(out_dir, \"valid/labels\")\n",
        "\n",
        "# trainディレクトリでラベルファイルを生成\n",
        "create_label_files(train_images_dir, train_labels_dir, csv_integrated_df)\n",
        "\n",
        "# validディレクトリでラベルファイルを生成\n",
        "create_label_files(valid_images_dir, valid_labels_dir, csv_integrated_df)\n"
      ],
      "metadata": {
        "id": "FuakTNeruSKn"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## バウンディングボックスのサンプル描画 ##\n",
        "## (これは実行しなくて良い)            ##\n",
        "##############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# バウンディングボックスを描画する関数\n",
        "def draw_bounding_boxes(image_path, bboxes):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img)\n",
        "        for bbox in bboxes:\n",
        "            cx, cy, bw, bh, class_id = bbox\n",
        "            x = (cx - bw / 2) * img_width\n",
        "            y = (cy - bh / 2) * img_height\n",
        "            width = bw * img_width\n",
        "            height = bh * img_height\n",
        "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得する関数\n",
        "def get_bboxes_from_label_file(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as file:\n",
        "        for line in file:\n",
        "            cx, cy, bw, bh, class_id = map(float, line.split())\n",
        "            bboxes.append((cx, cy, bw, bh, class_id))\n",
        "    return bboxes\n",
        "\n",
        "# 画像パスとラベルファイルパス\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images/10.JPG\"\n",
        "label_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/labels/10.txt\"\n",
        "\n",
        "# 画像のサイズを取得\n",
        "img_width, img_height = get_image_dimensions(image_path)\n",
        "\n",
        "# ラベルファイルからバウンディングボックスのリストを取得\n",
        "bboxes = get_bboxes_from_label_file(label_path, img_width, img_height)\n",
        "\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "print(f\"img_width: {img_width}, img_height: {img_height}\")\n",
        "print(f\"bboxes: {bboxes}\")\n",
        "draw_bounding_boxes(image_path, bboxes)\n"
      ],
      "metadata": {
        "id": "c8VCF1LTp66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhSI3Wc_-ylJ",
        "outputId": "7ec7d3d9-6455-4b13-a408-35b5fe3085a6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "train: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/Olympia_dataset/dataset_uni_for_YOLOv5/dataset_uni/valid/images\n",
        "\n",
        "nc: 2\n",
        "names: ['right', 'left']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_q3B4rp68S",
        "outputId": "96a05d36-c0c4-499d-a9f4-e0f8940b9842"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5_iFish**"
      ],
      "metadata": {
        "id": "5E0QJ1W8_R3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir\n",
        "!git clone https://github.com/ykitaguchi77/yolov5-iFish.git #iFish augmentationを実装したバージョン\n",
        "%cd yolov5-iFish\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-33rbP1-iQ4",
        "outputId": "a8c62893-4e0a-4051-ce8a-e2685ead0fac"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 4d3e758 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 30.1/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "KFhpM_Xm_k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data $out_dir/dataset.yaml --weights yolov5n.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iRcXa8I-iSj",
        "outputId": "5c9a1487-807f-4a3a-cee8-3379385ff71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/299      1.94G    0.07129    0.02954    0.01525         23        640: 100% 48/48 [02:16<00:00,  2.84s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:07<00:00,  1.27s/it]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      1/299      1.94G    0.04749    0.02364   0.004452         19        640: 100% 48/48 [02:04<00:00,  2.60s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:04<00:00,  1.35it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      2/299      1.94G    0.04601    0.02137   0.001682         23        640: 100% 48/48 [01:54<00:00,  2.39s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:04<00:00,  1.20it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      3/299      1.94G    0.04263    0.01866    0.00157         16        640: 100% 48/48 [02:12<00:00,  2.76s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.65it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      4/299      1.94G    0.03525    0.01782   0.001095         20        640: 100% 48/48 [02:07<00:00,  2.65s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.28it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      5/299      1.94G    0.03188      0.017  0.0008958         22        640: 100% 48/48 [02:22<00:00,  2.96s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.19it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      6/299      1.94G    0.02991    0.01595  0.0007892         19        640: 100% 48/48 [02:16<00:00,  2.85s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.74it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      7/299      1.94G    0.03303    0.01611  0.0007334         23        640: 100% 48/48 [02:14<00:00,  2.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.64it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      8/299      1.94G     0.0331    0.01561  0.0006592         18        640: 100% 48/48 [02:23<00:00,  2.99s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.79it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      9/299      1.94G    0.02618     0.0157  0.0005512         22        640: 100% 48/48 [02:20<00:00,  2.93s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.25it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     10/299      1.94G    0.03018    0.01527  0.0005367         16        640: 100% 48/48 [02:17<00:00,  2.87s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.26it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     11/299      1.94G    0.02877     0.0154  0.0005695         27        640: 100% 48/48 [02:22<00:00,  2.97s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.04it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     12/299      1.94G    0.02849    0.01508  0.0005007         21        640: 100% 48/48 [02:19<00:00,  2.91s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.36it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     13/299      1.94G    0.02808    0.01547  0.0005568         21        640: 100% 48/48 [02:12<00:00,  2.77s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.27it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     14/299      1.94G    0.02673    0.01457  0.0004527         21        640: 100% 48/48 [02:00<00:00,  2.51s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.64it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     15/299      1.94G    0.02564    0.01481  0.0004468         19        640: 100% 48/48 [02:07<00:00,  2.66s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.33it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     16/299      1.94G    0.02596    0.01476  0.0004466         19        640: 100% 48/48 [02:06<00:00,  2.64s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:01<00:00,  3.30it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     17/299      1.94G    0.02605     0.0151  0.0004209         22        640: 100% 48/48 [02:14<00:00,  2.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.63it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     18/299      1.94G    0.02493    0.01477  0.0004439         23        640: 100% 48/48 [02:17<00:00,  2.86s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.88it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     19/299      1.94G    0.02659    0.01469  0.0004435         21        640: 100% 48/48 [02:07<00:00,  2.65s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 6/6 [00:02<00:00,  2.38it/s]\n",
            "                   all        190        380          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     20/299      1.94G    0.02591    0.01432    0.00055         43        640:  44% 21/48 [01:11<00:48,  1.81s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 途中から\n",
        "!python train.py --img 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/dataset.yaml --resume /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/last.pt"
      ],
      "metadata": {
        "id": "gfKHvpKk-iUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = f\"{out_dir}/yolov5-iFish/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = f\"{out_dir}/yolov5n-iFish_120epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "dDbAGdcH-iW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGnEppmy-iY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference original dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $out_dir/yolov5-iFish\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419d4427-d9f6-43da-ecde-8cbef11e4f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2269e2e Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 26.5/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = f\"{out_dir}/yolov5n_130epch.pt\"\n",
        "\n",
        "# もともとのデータセット\n",
        "orig_dir = orig_dir #元画像\n",
        "cropped_dir = cropped_dir #YOLOv5で切り抜いた画像用\n",
        "\n",
        "if os.path.exists(cropped_dir)\n",
        "    shutil.rmtree(cropped_dir)\n",
        "os.makedirs(cropped_dir)\n",
        "os.makedirs(f\"{cropped_dir}/cropped_images\")\n"
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def inference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "\n",
        "image_path = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "    pred = inference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    cv2_imshow(img_cv2)\n",
        "\n",
        "    # Crop and save the image\n",
        "    cropped_img = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    base_name = os.path.basename(img_path)\n",
        "    cropped_img_path = os.path.join(cropped_dir, f\"cropped_{base_name}\")\n",
        "    cv2.imwrite(cropped_img_path, cropped_img)\n"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "はみ出し防止、長辺を1辺としたletterbox化\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# ここでorig_dirとcropped_dirを適切に設定してください\n",
        "orig_dir = 'path_to_original_images'\n",
        "cropped_dir = 'path_to_cropped_images'\n",
        "\n",
        "# もしcropped_dirが存在しない場合は作成する\n",
        "if not os.path.exists(cropped_dir):\n",
        "    os.makedirs(cropped_dir)\n",
        "\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "# inference関数とweightを適切に定義してください\n",
        "\n",
        "weight = 'path_to_weights'  # 重みファイルのパス\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "    prob = pred[0][0][4].item()\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))\n",
        "\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    padding_x = (img_width - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_height - min(img_width, img_height)) / 2\n",
        "\n",
        "    x1 = max(x1 - padding_x, 0)\n",
        "    y1 = max(y1 - padding_y, 0)\n",
        "    x2 = min(x2 - padding_x, img_width)\n",
        "    y2 = min(y2 - padding_y, img_height)\n",
        "\n",
        "    # クロップされた画像の実際のサイズ\n",
        "    cropped_height = y2 - y1\n",
        "    cropped_width = x2 - x1\n",
        "\n",
        "    # 黒い背景画像を作成\n",
        "    if cropped_width > cropped_height:\n",
        "        size = cropped_width\n",
        "    else:\n",
        "        size = cropped_height\n",
        "    padded_img = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "\n",
        "    # クロップした画像を中央に配置\n",
        "    top = (size - cropped_height) // 2\n",
        "    bottom = size - cropped_height - top\n",
        "    left = (size - cropped_width) // 2\n",
        "    right = size - cropped_width - left\n",
        "\n",
        "    cropped_img = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "    padded_img[top:top + cropped_height, left:left + cropped_width] = cropped_img\n",
        "\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "    cv2_imshow(img_cv2)\n",
        "\n",
        "    base_name = os.path.basename(img_path)\n",
        "    cropped_img_path = os.path.join(cropped_dir, f\"cropped_{base_name}\")\n",
        "    cv2.imwrite(cropped_img_path, padded_img)\n"
      ],
      "metadata": {
        "id": "8Bhi-plnJQUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "640pxにリサイズしないバージョン\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# ここでorig_dirとcropped_dirを適切に設定してください\n",
        "orig_dir = 'path_to_original_images'\n",
        "cropped_dir = 'path_to_cropped_images'\n",
        "\n",
        "# もしcropped_dirが存在しない場合は作成する\n",
        "if not os.path.exists(cropped_dir):\n",
        "    os.makedirs(cropped_dir)\n",
        "\n",
        "image_paths = glob.glob(f\"{orig_dir}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"right\", 1: \"left\"}\n",
        "\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img_path = image_paths[i]\n",
        "    pred = inference(img_path, weight)\n",
        "    x1, y1, x2, y2, prob, class_num = pred[0][0]\n",
        "    class_name = class_names[class_num]\n",
        "\n",
        "    print(f\"診断は {class_name}、確率は{prob * 100:.1f}%です。\")\n",
        "\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # 画像から該当する領域を切り抜く\n",
        "    cropped_img = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 確認のために元画像に矩形を描画し、表示する\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "    cv2_imshow(img_cv2)\n",
        "\n",
        "    base_name = os.path.basename(img_path)\n",
        "    cropped_img_path = os.path.join(cropped_dir, f\"cropped_{base_name}\")\n",
        "    cv2.imwrite(cropped_img_path, cropped_img)\n"
      ],
      "metadata": {
        "id": "s6vaadJaJQbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kc9fruyiJQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGQIgqnPJQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNUyvI-CJQg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7Zp_wf_JQjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p5vrzYkJQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qa1NMrMQJQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5XdJerf7Zxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpdE1Mov7Zzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements.txt"
      ],
      "metadata": {
        "id": "yvNA4FiYxW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\""
      ],
      "metadata": {
        "id": "vwbKpW0n2ZOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python export.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt --nms --include coreml\n",
        "!python export.py --weights $weight_path --nms --include \"coreml\"\n"
      ],
      "metadata": {
        "id": "Nnxz6A9pyhvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxnOT6leGS9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-EigenCAM**\n",
        "\n",
        "https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
      ],
      "metadata": {
        "id": "YXt1zi1TGm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "DM7nTgGnjwYO",
        "outputId": "bc8c9e81-6dbc-4ec4-c0eb-c4f9edc3a822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'pytorch-grad-cam'...\n",
            "remote: Enumerating objects: 1122, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 1122 (delta 8), reused 9 (delta 3), pack-reused 1097\u001b[K\n",
            "Receiving objects: 100% (1122/1122), 110.21 MiB | 15.74 MiB/s, done.\n",
            "Resolving deltas: 100% (617/617), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "0xDMsB0GkMyg",
        "outputId": "239921df-d7d6-45a2-dfb8-89fda0fb434b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-grad-cam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "metadata": {
        "id": "FzxYfIoemXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "id": "DKEgZ-Laojx8",
        "outputId": "872a8e02-519e-4a1d-cf17-45e907ac1b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from PIL import Image\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "def parse_detections(results):\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    detections = detections.to_dict()\n",
        "    boxes, colors, names = [], [], []\n",
        "\n",
        "    for i in range(len(detections[\"xmin\"])):\n",
        "        confidence = detections[\"confidence\"][i]\n",
        "        if confidence < 0.2:\n",
        "            continue\n",
        "        xmin = int(detections[\"xmin\"][i])\n",
        "        ymin = int(detections[\"ymin\"][i])\n",
        "        xmax = int(detections[\"xmax\"][i])\n",
        "        ymax = int(detections[\"ymax\"][i])\n",
        "        name = detections[\"name\"][i]\n",
        "        category = int(detections[\"class\"][i])\n",
        "        color = COLORS[category]\n",
        "\n",
        "        boxes.append((xmin, ymin, xmax, ymax))\n",
        "        colors.append(color)\n",
        "        names.append(name)\n",
        "    return boxes, colors, names\n",
        "\n",
        "\n",
        "def draw_detections(boxes, colors, names, img):\n",
        "    for box, color, name in zip(boxes, colors, names):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (xmin, ymin),\n",
        "            (xmax, ymax),\n",
        "            color,\n",
        "            2)\n",
        "\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n",
        "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
        "img = cv2.resize(img, (640, 640))\n",
        "rgb_img = img.copy()\n",
        "img = np.float32(img) / 255\n",
        "transform = transforms.ToTensor()\n",
        "tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "target_layers = [model.model.model.model[-2]]\n",
        "\n",
        "results = model([rgb_img])\n",
        "boxes, colors, names = parse_detections(results)\n",
        "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
        "Image.fromarray(detections)"
      ],
      "metadata": {
        "id": "i_WLp1kGjvNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To do Next**"
      ],
      "metadata": {
        "id": "pStgcOTIFO62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "・外部のデータセット（Treated）を洗い出し\n",
        "\n",
        "・内部のデータセットをさらに水増し\n",
        "\n",
        "・内部および外部データセットより、test用各100枚（grav50枚、cont50枚）を抜き出しておき、合体する\n",
        "\n",
        "・既存のYOLOv5を用いてbounding boxを抜き出し、新たにトレーニングする"
      ],
      "metadata": {
        "id": "UjetedHEGU6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}